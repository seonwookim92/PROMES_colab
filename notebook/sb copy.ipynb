{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swkim/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Stable baselines3\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "# env\n",
    "import gym\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from kube_mm_scheduler.model.promes import Net5_\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FE_PROMES(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 80):\n",
    "        super(FE_PROMES, self).__init__(observation_space, features_dim)\n",
    "        self.net = Net5_().to(device)\n",
    "        self.net.load_state_dict(th.load(os.path.join(base_path,'kube_mm_scheduler/weight/net5.pt')))\n",
    "        self.net.eval()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        input1 = observations[:, :10].to(device)\n",
    "        input2 = observations[:, 10:].to(device)\n",
    "\n",
    "        return self.net(input1, input2)\n",
    "\n",
    "class FE_NAIVE(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 80):\n",
    "        super(FE_NAIVE, self).__init__(observation_space, features_dim)\n",
    "        self.net = nn.Linear(observation_space.shape[0], features_dim).to(device)\n",
    "        self.net.eval()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        return self.net(observations)\n",
    "\n",
    "policy_kwargs_promes = dict(\n",
    "    features_extractor_class=FE_PROMES,\n",
    "    features_extractor_kwargs=dict(features_dim=80),\n",
    ")\n",
    "\n",
    "policy_kwargs_naive = dict(\n",
    "    features_extractor_class=FE_NAIVE,\n",
    "    features_extractor_kwargs=dict(features_dim=80),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(scenario_file, model_fname, log_name, scenario_idx, policy_kwargs):\n",
    "    # ============================== Performance Test ===============================\n",
    "\n",
    "    # Previous model performance test (vs. defautl scheduler)\n",
    "    # Test scenario : scenario-5l-5m-1000p-10m.csv\n",
    "    test_env1 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "    test_env2 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "\n",
    "    # Default Scheduler\n",
    "    from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "    default_scheduler = SimHrScheduler(test_env2, 'default.py')\n",
    "\n",
    "    # RL Scheduler\n",
    "    from kube_rl_scheduler.scheduler.sim_rl_scheduler import SimRlScheduler\n",
    "    rl_scheduler = SimRlScheduler(test_env1, f'_{model_fname}.zip', policy_kwargs=policy_kwargs)\n",
    "\n",
    "\n",
    "    # Test the model\n",
    "    obs1 = test_env1.reset()\n",
    "    obs2 = test_env2.reset()\n",
    "    done1 = False\n",
    "    done2 = False\n",
    "    step1 = 0\n",
    "    step2 = 0\n",
    "    acc_rew1 = 0\n",
    "    acc_rew2 = 0\n",
    "\n",
    "    print(f\"Testing with {scenario_file} (my model vs. default)\")\n",
    "    while not done1 or not done2:\n",
    "        if not done1:\n",
    "            # action1, _ = model.predict(obs1)\n",
    "            action1 = rl_scheduler.decision(test_env1)\n",
    "            obs1, reward1, done1, _ = test_env1.step(action1)\n",
    "            step1 += 1\n",
    "            acc_rew1 += reward1\n",
    "        if not done2:\n",
    "            action2 = default_scheduler.decision(test_env2)\n",
    "            obs2, reward2, done2, _ = test_env2.step(action2)\n",
    "            step2 += 1\n",
    "            acc_rew2 += reward2\n",
    "\n",
    "    acc_rew1 = round(acc_rew1, 2)\n",
    "    acc_rew2 = round(acc_rew2, 2)\n",
    "\n",
    "    print(f\"Test result(reward): {acc_rew1} vs. {acc_rew2}\")\n",
    "    print(f\"Test result(step): {step1} vs. {step2}\")\n",
    "\n",
    "    return acc_rew1, acc_rew2, step1, step2\n",
    "\n",
    "    # ============================== ==================== ==============================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "def training(json_tracker_fname):\n",
    "\n",
    "    log_name = json_tracker_fname.split('.')[0]\n",
    "    log_path = 'training/log/' + log_name\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(f'training/log/{log_name}')\n",
    "\n",
    "    # Load the json tracker\n",
    "    import json\n",
    "    with open(f'training/{json_tracker_fname}', 'r') as f:\n",
    "        json_tracker = json.load(f)\n",
    "\n",
    "    reward_file = json_tracker['reward_file']\n",
    "    reward_key = os.path.splitext(reward_file)[0].split('_')[1]\n",
    "\n",
    "    # Environment\n",
    "    envs = []\n",
    "    for i in range(1, 50):\n",
    "        env = gym.make('SimKubeEnv-v0', reward_file=reward_file, scenario_file=f'trace2017_100_{i}.csv')\n",
    "        envs.append(env)\n",
    "\n",
    "    # Check if the last scenario is None\n",
    "    if json_tracker['last_scenario'] == 0:\n",
    "        # If it is None, then start from the first scenario\n",
    "        scenario_idx = 1\n",
    "    else:\n",
    "        # If it is not None, then continue from the last scenario\n",
    "        scenario_idx = int(json_tracker['last_scenario']) + 1\n",
    "\n",
    "    n_scenario = len(os.listdir(os.path.join(base_path, 'scenarios', 'trace2017')))\n",
    "\n",
    "    model_name = json_tracker['model_name']\n",
    "\n",
    "    feature_net = json_tracker['feature_net']\n",
    "    if feature_net == 'Promes':\n",
    "        policy_kwargs = policy_kwargs_promes\n",
    "    elif feature_net == 'Naive':\n",
    "        policy_kwargs = policy_kwargs_naive\n",
    "    else:\n",
    "        print(\"Wrong feature net name\")\n",
    "        return\n",
    "\n",
    "    if model_name == 'DQN':\n",
    "        model = sb3.DQN\n",
    "    elif model_name == 'PPO':\n",
    "        model = sb3.PPO\n",
    "\n",
    "    env = envs[scenario_idx-1]\n",
    "    timesteps = json_tracker['total_steps']\n",
    "\n",
    "    model_fname = f'{model_name}_{feature_net}_{reward_key}.zip'\n",
    "    model_fpath = f'training/model/{model_fname}'\n",
    "\n",
    "    # Check if the model is already trained\n",
    "    if os.path.exists(model_fpath):\n",
    "        print(f\"Model {model_name} is already trained.\")\n",
    "        model = model.load(model_fpath, env=env, verbose=1, policy_kwargs=policy_kwargs)\n",
    "    else:\n",
    "        print(f\"Training with scenario {env.scenario_file}\")\n",
    "        model = model('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)\n",
    "        model.save(model_fpath)\n",
    "\n",
    "    while scenario_idx < n_scenario:\n",
    "\n",
    "        a1, a2, a3, a4 = test_model('scenario-5l-5m-1000p-10m.csv', model_fname, log_name, scenario_idx, policy_kwargs=policy_kwargs)\n",
    "        b1, b2, b3, b4 = test_model('scenario-3l-10m-1000p-10m.csv', model_fname, log_name, scenario_idx, policy_kwargs=policy_kwargs)\n",
    "        c1, c2, c3, c4 = test_model('scenario-10l-3m-1000p-10m.csv', model_fname, log_name, scenario_idx, policy_kwargs=policy_kwargs)\n",
    "\n",
    "        with open(f'training/log/{log_name}/test_result.txt', 'a') as f:\n",
    "            f.write(f\"{scenario_idx - 1}, {a1}, {a2}, {a3}, {a4}, {b1}, {b2}, {b3}, {b4}, {c1}, {c2}, {c3}, {c4}\\n\")\n",
    "\n",
    "        env = envs[scenario_idx-1]\n",
    "        model.set_env(env)\n",
    "        model.learn(total_timesteps=timesteps)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(f'training/model/{model_name}.zip')\n",
    "\n",
    "        # Update the json tracker\n",
    "        json_tracker['last_scenario'] = scenario_idx\n",
    "\n",
    "        # Save the json tracker\n",
    "        with open(f'training/{json_tracker_fname}', 'w') as f:\n",
    "            json.dump(json_tracker, f)\n",
    "\n",
    "        scenario_idx += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO model, Naive feature net, Dynamic reward\n",
    "training('tracker_ppo_naive_dynamic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO model, Promes feature net, Dynamic reward\n",
    "training('tracker_ppo_promes_dynamic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DQN model, Naive feature net, Dynamic reward\n",
    "# training('tracker_dqn_naive_dynamic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # DQN model, Promes feature net, Dynamic reward\n",
    "# training('tracker_dqn_promes_dynamic.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
