{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='promes_static.py', scenario_file='scenario-5l-5m-1000p-10m.csv')\n",
    "model = PPO.load('training/model/PPO_Promes_Dynamic_New', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCriticPolicy(\n",
      "  (features_extractor): PromesPPO(\n",
      "    (net): Net5_(\n",
      "      (net3_): Net3_(\n",
      "        (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
      "        (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
      "        (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (fc4): None\n",
      "      )\n",
      "      (fc1_3_1): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_2): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_3): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_4): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_5): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc2_1): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_1): None\n",
      "      (fc2_2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_2): None\n",
      "      (fc2_3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_3): None\n",
      "      (fc2_4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_4): None\n",
      "      (fc2_5): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_5): None\n",
      "    )\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_policy = model.policy\n",
    "print(model_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from gym import spaces\n",
    "\n",
    "from kube_mm_scheduler.model.promes import Net5_\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class PromesPPO(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 80):\n",
    "        super(PromesPPO, self).__init__(observation_space, features_dim)\n",
    "        self.net = Net5_().to(device)\n",
    "        self.net.load_state_dict(th.load(os.path.join(base_path,'kube_mm_scheduler/weight/net5.pt')))\n",
    "        self.net.eval()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        input1 = observations[:, :10].to(device)\n",
    "        input2 = observations[:, 10:].to(device)\n",
    "\n",
    "        return self.net(input1, input2)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=PromesPPO,\n",
    "    features_extractor_kwargs=dict(features_dim=80),\n",
    ")\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.8209e-01, -2.0166e-03,  4.3477e-02, -4.1246e-01,  8.4649e-01,\n",
       "          1.2364e+00,  3.6112e-01, -7.0216e-02, -1.2488e+00, -8.4166e-01,\n",
       "         -6.3948e-01,  3.7009e-01,  9.2319e-01, -1.1981e+00,  2.1128e-01,\n",
       "         -6.8118e-01, -8.1349e-01,  6.4176e-01,  9.6026e-02, -9.1739e-01,\n",
       "          7.6262e-01, -3.5616e-01, -4.7152e-01, -3.4970e-01,  4.7748e-01,\n",
       "         -2.9206e-01, -6.0026e-01, -5.0589e-01,  1.6936e-01, -3.5726e-01,\n",
       "          2.2118e-01,  1.6680e-01,  1.0720e+00, -3.8339e-01,  2.0188e-01,\n",
       "          2.6974e-01,  1.0602e+00,  2.7954e-01, -3.0595e-01,  4.4118e-01,\n",
       "         -5.7934e-01, -1.1713e-01, -9.1546e-01, -3.5743e-01, -1.9049e-01,\n",
       "         -6.7302e-01, -1.3156e-01, -1.0659e+00,  5.0722e-01, -1.1593e+00,\n",
       "         -1.9281e+00,  9.0136e-01, -2.9678e-01, -1.1226e+00, -8.9587e-01,\n",
       "         -3.8955e-01,  2.4844e-01,  5.0076e-02, -6.8017e-01, -3.4872e-01,\n",
       "          5.4871e-01, -3.4983e-01,  8.3166e-02, -1.9030e-01,  9.8957e-01,\n",
       "         -5.7375e-02,  1.4242e-01,  4.3258e-01,  1.0652e-01, -4.4075e-01,\n",
       "          9.3889e-01,  9.4987e-03,  8.0831e-01,  5.2473e-02, -3.4707e-01,\n",
       "         -2.6684e-01, -8.0961e-01, -1.4363e-03,  6.2168e-01, -4.9394e-01]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = model.policy.extract_features(sample1)\n",
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3412, -0.9104, -0.5699, -0.8910,  0.5330,  0.9437,  0.7276, -0.2277,\n",
       "         -0.4597, -0.2187,  0.7416, -0.1159, -0.4737, -0.2007,  0.3745,  0.6154,\n",
       "          0.6743, -0.9535,  0.2167,  0.1879,  0.9270, -0.5601,  0.0476, -0.1895,\n",
       "         -0.7922,  0.7731,  0.5519, -0.4477,  0.4764, -0.2538, -0.5635,  0.6201,\n",
       "          0.6768,  0.7503, -0.8062, -0.6625,  0.3031,  0.9158, -0.8296,  0.6666,\n",
       "          0.7554, -0.7696, -0.1408,  0.6410,  0.1657,  0.8620, -0.3111, -0.9673,\n",
       "         -0.4229,  0.2294,  0.8908,  0.3504, -0.6294,  0.8077,  0.2003, -0.9478,\n",
       "          0.0798,  0.0733,  0.6702,  0.3428,  0.5562, -0.8597, -0.3336,  0.0386]],\n",
       "       grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent_pi, latent_vf = model.policy.mlp_extractor(features)\n",
    "latent_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0016, -0.0082,  0.0031, -0.0055, -0.0064, -0.0100]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = model.policy.action_net(latent_pi)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.001594634260982275,\n",
       " -0.008207956328988075,\n",
       " 0.0030819326639175415,\n",
       " -0.005500008352100849,\n",
       " -0.00635987613350153,\n",
       " -0.009973096661269665]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores.tolist()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), None)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy.predict(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.common.distributions.CategoricalDistribution at 0x7fcdc0272eb0>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cd = model.policy._get_action_dist_from_latent(latent_pi)\n",
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The value argument to log_prob must be a Tensor",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m cd\u001b[39m.\u001b[39;49mlog_prob([\u001b[39m1\u001b[39;49m,\u001b[39m2\u001b[39;49m,\u001b[39m3\u001b[39;49m,\u001b[39m4\u001b[39;49m,\u001b[39m5\u001b[39;49m,\u001b[39m6\u001b[39;49m])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/distributions.py:278\u001b[0m, in \u001b[0;36mCategoricalDistribution.log_prob\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, actions: th\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m th\u001b[39m.\u001b[39mTensor:\n\u001b[0;32m--> 278\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdistribution\u001b[39m.\u001b[39;49mlog_prob(actions)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/distributions/categorical.py:121\u001b[0m, in \u001b[0;36mCategorical.log_prob\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlog_prob\u001b[39m(\u001b[39mself\u001b[39m, value):\n\u001b[1;32m    120\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_args:\n\u001b[0;32m--> 121\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_sample(value)\n\u001b[1;32m    122\u001b[0m     value \u001b[39m=\u001b[39m value\u001b[39m.\u001b[39mlong()\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m    123\u001b[0m     value, log_pmf \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mbroadcast_tensors(value, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlogits)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/distributions/distribution.py:270\u001b[0m, in \u001b[0;36mDistribution._validate_sample\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39mArgument validation for distribution methods such as `log_prob`,\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39m`cdf` and `icdf`. The rightmost dimensions of a value to be\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[39m        distribution's batch and event shapes.\u001b[39;00m\n\u001b[1;32m    268\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    269\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(value, torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 270\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mThe value argument to log_prob must be a Tensor\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    272\u001b[0m event_dim_start \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(value\u001b[39m.\u001b[39msize()) \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_shape)\n\u001b[1;32m    273\u001b[0m \u001b[39mif\u001b[39;00m value\u001b[39m.\u001b[39msize()[event_dim_start:] \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_event_shape:\n",
      "\u001b[0;31mValueError\u001b[0m: The value argument to log_prob must be a Tensor"
     ]
    }
   ],
   "source": [
    "cd.log_prob([1,2,3,4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'observation_space', 'action_space', and 'lr_schedule'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# From stable_baselines3 model, exclude the last layer\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mstable_baselines3\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39msb3\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m sb3\u001b[39m.\u001b[39;49mcommon\u001b[39m.\u001b[39;49mpolicies\u001b[39m.\u001b[39;49mActorCriticPolicy()\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'observation_space', 'action_space', and 'lr_schedule'"
     ]
    }
   ],
   "source": [
    "# From stable_baselines3 model, exclude the last layer\n",
    "import stable_baselines3 as sb3\n",
    "\n",
    "observation_space = env.observation_space\n",
    "action_space = env.action_space\n",
    "\n",
    "\n",
    "sb3.common.policies.ActorCriticPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0747, -0.5190,  0.0521, -0.8373,  0.9420,  0.9265, -0.2214,  0.0048,\n",
       "         -0.3006, -0.9299, -0.6604, -0.0232,  0.2931,  0.0276,  0.0727,  0.5788,\n",
       "         -0.2546,  0.7006,  0.2385,  0.3027, -0.4275,  0.2718,  0.5836, -0.2001,\n",
       "          0.1188, -0.4523,  0.2804, -0.5034, -0.2554,  0.8030, -0.4536, -0.1466,\n",
       "          0.1496, -0.2085, -0.0995, -0.2559, -0.1075,  0.5839, -0.7997,  0.2909,\n",
       "         -0.6887, -0.4395, -0.1927,  0.2446, -0.6809, -0.0246,  0.2542, -0.3372,\n",
       "          0.5050, -0.0510, -1.2866, -0.4210, -0.4491, -0.1291, -0.5756,  0.1960,\n",
       "          0.2474, -0.6453,  0.2328, -0.1804,  0.0487,  0.1516, -0.1517, -0.1941,\n",
       "          0.0721,  0.1695,  0.3108,  0.0065, -0.1304, -0.2700, -0.0146, -0.0317,\n",
       "         -0.7572,  0.1292,  0.4665, -0.1370, -0.4350,  0.0142, -0.1797, -0.1488]],\n",
       "       grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.policy.features_extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCriticPolicy(\n",
      "  (features_extractor): PromesPPO(\n",
      "    (net): Net5_(\n",
      "      (net3_): Net3_(\n",
      "        (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
      "        (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
      "        (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (fc4): None\n",
      "      )\n",
      "      (fc1_3_1): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_2): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_3): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_4): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_5): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc2_1): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_1): None\n",
      "      (fc2_2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_2): None\n",
      "      (fc2_3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_3): None\n",
      "      (fc2_4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_4): None\n",
      "      (fc2_5): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_5): None\n",
      "    )\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_pytorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from kube_mm_scheduler.model.promes import Net5_\n",
    "from kube_mm_scheduler.model.net3 import Model as Net3\n",
    "\n",
    "from typing import Dict, List, Tuple, Type, Union\n",
    "\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Net3_(Net3):\n",
    "    def __init__(self):\n",
    "        super(Net3_, self).__init__()\n",
    "        self.fc4 = None\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1) \n",
    "        x = F.relu(self.fc2(x))  \n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PromesPPO(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 80):\n",
    "        super(PromesPPO, self).__init__(observation_space, features_dim)\n",
    "        self.net = Net5_().to(device)\n",
    "        self.net.load_state_dict(th.load(os.path.join(base_path,'kube_mm_scheduler/weight/net5.pt')))\n",
    "        self.net.eval()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        input1 = observations[:, :10].to(device)\n",
    "        input2 = observations[:, 10:].to(device)\n",
    "\n",
    "        return self.net(input1, input2)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=PromesPPO,\n",
    "    features_extractor_kwargs=dict(features_dim=80),\n",
    ")\n",
    "\n",
    "class MlpExtractor(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        net_arch: Union[List[int], Dict[str, List[int]]],\n",
    "        activation_fn: Type[nn.Module],\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        policy_net: List[nn.Module] = []\n",
    "        value_net: List[nn.Module] = []\n",
    "        last_layer_dim_pi = feature_dim\n",
    "        last_layer_dim_vf = feature_dim\n",
    "\n",
    "        # save dimensions of layers in policy and value nets\n",
    "        if isinstance(net_arch, dict):\n",
    "            # Note: if key is not specificed, assume linear network\n",
    "            pi_layers_dims = net_arch.get(\"pi\", [])  # Layer sizes of the policy network\n",
    "            vf_layers_dims = net_arch.get(\"vf\", [])  # Layer sizes of the value network\n",
    "        else:\n",
    "            pi_layers_dims = vf_layers_dims = net_arch\n",
    "        # Iterate through the policy layers and build the policy net\n",
    "        for curr_layer_dim in pi_layers_dims:\n",
    "            policy_net.append(nn.Linear(last_layer_dim_pi, curr_layer_dim))\n",
    "            policy_net.append(activation_fn())\n",
    "            last_layer_dim_pi = curr_layer_dim\n",
    "        # Iterate through the value layers and build the value net\n",
    "        for curr_layer_dim in vf_layers_dims:\n",
    "            value_net.append(nn.Linear(last_layer_dim_vf, curr_layer_dim))\n",
    "            value_net.append(activation_fn())\n",
    "            last_layer_dim_vf = curr_layer_dim\n",
    "\n",
    "        # Save dim, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Create networks\n",
    "        # If the list of layers is empty, the network will just act as an Identity module\n",
    "        self.policy_net = nn.Sequential(*policy_net).to(device)\n",
    "        self.value_net = nn.Sequential(*value_net).to(device)\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Features extractor\n",
    "        self.features_extractor = PromesPPO(observation_space=env.observation_space ,features_dim=80)\n",
    "\n",
    "        # MLP extractor\n",
    "        self.mlp_extractor = MlpExtractor(feature_dim=80, net_arch=[64, 64], activation_fn=nn.Tanh, device=device)\n",
    "\n",
    "        # Action net\n",
    "        self.action_net = nn.Linear(in_features=64, out_features=6, bias=True)\n",
    "\n",
    "        # Value net\n",
    "        self.value_net = nn.Linear(in_features=64, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Extract features\n",
    "        features = self.features_extractor(state)\n",
    "\n",
    "        # Extract policy and value\n",
    "        policy = self.mlp_extractor.policy_net(features)\n",
    "        value = self.mlp_extractor.value_net(features)\n",
    "\n",
    "        # Get action\n",
    "        action = self.action_net(policy)\n",
    "\n",
    "        # Get value\n",
    "        value = self.value_net(value)\n",
    "\n",
    "        return action, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a model with the same parameters\n",
    "pt_model = ActorCriticPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from model_policy\n",
    "weights = model_policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights to pt_model\n",
    "pt_model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.0138,  0.0029,  0.0015, -0.0032,  0.0009,  0.0082]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-0.2822]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.2938,  1.9472, -0.3218,  0.2534,  2.2748, -3.2760]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.9904, -0.0553, -0.9415, -1.1002, -0.3881,  4.7698]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-61.6606]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.6532, -0.6983,  1.0152,  0.3938,  1.4332, -0.1519]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[5.3250]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
