{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='promes_static.py', scenario_file='scenario-5l-5m-1000p-10m.csv')\n",
    "model = PPO.load('training/model/PPO_Promes_Combined', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCriticPolicy(\n",
      "  (features_extractor): PromesPPO(\n",
      "    (net): Net5_(\n",
      "      (net3_): Net3_(\n",
      "        (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
      "        (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
      "        (fc2): Linear(in_features=32, out_features=16, bias=True)\n",
      "        (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
      "        (fc4): None\n",
      "      )\n",
      "      (fc1_3_1): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_2): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_3): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_4): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc1_3_5): Linear(in_features=2, out_features=16, bias=True)\n",
      "      (fc2_1): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_1): None\n",
      "      (fc2_2): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_2): None\n",
      "      (fc2_3): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_3): None\n",
      "      (fc2_4): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_4): None\n",
      "      (fc2_5): Linear(in_features=32, out_features=16, bias=True)\n",
      "      (fc3_5): None\n",
      "    )\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=80, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_policy = model.policy\n",
    "print(model_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from kube_mm_scheduler.model.promes import Net5_\n",
    "from kube_mm_scheduler.model.net3 import Model as Net3\n",
    "\n",
    "from typing import Dict, List, Tuple, Type, Union\n",
    "\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Net3_(Net3):\n",
    "    def __init__(self):\n",
    "        super(Net3_, self).__init__()\n",
    "        self.fc4 = None\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1) \n",
    "        x = F.relu(self.fc2(x))  \n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PromesPPO(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 80):\n",
    "        super(PromesPPO, self).__init__(observation_space, features_dim)\n",
    "        self.net = Net5_().to(device)\n",
    "        self.net.load_state_dict(th.load(os.path.join(base_path,'kube_mm_scheduler/weight/net5.pt')))\n",
    "        self.net.eval()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        input1 = observations[:, :10].to(device)\n",
    "        input2 = observations[:, 10:].to(device)\n",
    "\n",
    "        return self.net(input1, input2)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=PromesPPO,\n",
    "    features_extractor_kwargs=dict(features_dim=80),\n",
    ")\n",
    "\n",
    "class MlpExtractor(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        net_arch: Union[List[int], Dict[str, List[int]]],\n",
    "        activation_fn: Type[nn.Module],\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        policy_net: List[nn.Module] = []\n",
    "        value_net: List[nn.Module] = []\n",
    "        last_layer_dim_pi = feature_dim\n",
    "        last_layer_dim_vf = feature_dim\n",
    "\n",
    "        # save dimensions of layers in policy and value nets\n",
    "        if isinstance(net_arch, dict):\n",
    "            # Note: if key is not specificed, assume linear network\n",
    "            pi_layers_dims = net_arch.get(\"pi\", [])  # Layer sizes of the policy network\n",
    "            vf_layers_dims = net_arch.get(\"vf\", [])  # Layer sizes of the value network\n",
    "        else:\n",
    "            pi_layers_dims = vf_layers_dims = net_arch\n",
    "        # Iterate through the policy layers and build the policy net\n",
    "        for curr_layer_dim in pi_layers_dims:\n",
    "            policy_net.append(nn.Linear(last_layer_dim_pi, curr_layer_dim))\n",
    "            policy_net.append(activation_fn())\n",
    "            last_layer_dim_pi = curr_layer_dim\n",
    "        # Iterate through the value layers and build the value net\n",
    "        for curr_layer_dim in vf_layers_dims:\n",
    "            value_net.append(nn.Linear(last_layer_dim_vf, curr_layer_dim))\n",
    "            value_net.append(activation_fn())\n",
    "            last_layer_dim_vf = curr_layer_dim\n",
    "\n",
    "        # Save dim, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Create networks\n",
    "        # If the list of layers is empty, the network will just act as an Identity module\n",
    "        self.policy_net = nn.Sequential(*policy_net).to(device)\n",
    "        self.value_net = nn.Sequential(*value_net).to(device)\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Features extractor\n",
    "        self.features_extractor = PromesPPO(observation_space=env.observation_space ,features_dim=80)\n",
    "\n",
    "        # MLP extractor\n",
    "        self.mlp_extractor = MlpExtractor(feature_dim=80, net_arch=[64, 64], activation_fn=nn.Tanh, device=device)\n",
    "\n",
    "        # Action net\n",
    "        self.action_net = nn.Linear(in_features=64, out_features=6, bias=True)\n",
    "\n",
    "        # Value net\n",
    "        self.value_net = nn.Linear(in_features=64, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Extract features\n",
    "        features = self.features_extractor(state)\n",
    "\n",
    "        # Extract policy and value\n",
    "        policy = self.mlp_extractor.policy_net(features)\n",
    "        value = self.mlp_extractor.value_net(features)\n",
    "\n",
    "        # Get action\n",
    "        action = self.action_net(policy)\n",
    "\n",
    "        # Get value\n",
    "        value = self.value_net(value)\n",
    "\n",
    "        return action, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a model with the same parameters\n",
    "pt_model = ActorCriticPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from model_policy\n",
    "weights = model_policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load weights to pt_model\n",
    "pt_model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-4.6821,  1.2452, -1.6478, -1.4470, -0.2967,  4.0329]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[11.3650]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.9904, -0.0553, -0.9415, -1.1002, -0.3881,  4.7698]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-61.6606]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.6532, -0.6983,  1.0152,  0.3938,  1.4332, -0.1519]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[5.3250]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
