{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv\n",
    "\n",
    "from stable_baselines3 import DQN, PPO\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='promes_static.py', scenario_file='scenario-5l-5m-1000p-10m.csv')\n",
    "model = PPO.load('training/model/PPO_Promes_Combined', env=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCriticPolicy(\n",
      "  (features_extractor): FlattenExtractor(\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  )\n",
      "  (mlp_extractor): MlpExtractor(\n",
      "    (shared_net): Sequential()\n",
      "    (policy_net): Sequential(\n",
      "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "    (value_net): Sequential(\n",
      "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
      "      (1): Tanh()\n",
      "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "      (3): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (action_net): Linear(in_features=64, out_features=6, bias=True)\n",
      "  (value_net): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model_policy = model.policy\n",
    "print(model_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import spaces\n",
    "\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from kube_mm_scheduler.model.promes import Net5_\n",
    "from kube_mm_scheduler.model.net3 import Model as Net3\n",
    "\n",
    "from typing import Dict, List, Tuple, Type, Union\n",
    "\n",
    "\n",
    "device = th.device(\"cuda\" if th.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Net3_(Net3):\n",
    "    def __init__(self):\n",
    "        super(Net3_, self).__init__()\n",
    "        self.fc4 = None\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1) \n",
    "        x = F.relu(self.fc2(x))  \n",
    "        x = F.relu(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class PromesPPO(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 80):\n",
    "        super(PromesPPO, self).__init__(observation_space, features_dim)\n",
    "        self.net = Net5_().to(device)\n",
    "        self.net.load_state_dict(th.load(os.path.join(base_path,'kube_mm_scheduler/weight/net5.pt')))\n",
    "        self.net.eval()\n",
    "\n",
    "    def forward(self, observations: th.Tensor) -> th.Tensor:\n",
    "        input1 = observations[:, :10].to(device)\n",
    "        input2 = observations[:, 10:].to(device)\n",
    "\n",
    "        return self.net(input1, input2)\n",
    "\n",
    "policy_kwargs = dict(\n",
    "    features_extractor_class=PromesPPO,\n",
    "    features_extractor_kwargs=dict(features_dim=80),\n",
    ")\n",
    "\n",
    "class MlpExtractor(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        feature_dim: int,\n",
    "        net_arch: Union[List[int], Dict[str, List[int]]],\n",
    "        activation_fn: Type[nn.Module],\n",
    "        device: Union[th.device, str] = \"auto\",\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        policy_net: List[nn.Module] = []\n",
    "        value_net: List[nn.Module] = []\n",
    "        last_layer_dim_pi = feature_dim\n",
    "        last_layer_dim_vf = feature_dim\n",
    "\n",
    "        # save dimensions of layers in policy and value nets\n",
    "        if isinstance(net_arch, dict):\n",
    "            # Note: if key is not specificed, assume linear network\n",
    "            pi_layers_dims = net_arch.get(\"pi\", [])  # Layer sizes of the policy network\n",
    "            vf_layers_dims = net_arch.get(\"vf\", [])  # Layer sizes of the value network\n",
    "        else:\n",
    "            pi_layers_dims = vf_layers_dims = net_arch\n",
    "        # Iterate through the policy layers and build the policy net\n",
    "        for curr_layer_dim in pi_layers_dims:\n",
    "            policy_net.append(nn.Linear(last_layer_dim_pi, curr_layer_dim))\n",
    "            policy_net.append(activation_fn())\n",
    "            last_layer_dim_pi = curr_layer_dim\n",
    "        # Iterate through the value layers and build the value net\n",
    "        for curr_layer_dim in vf_layers_dims:\n",
    "            value_net.append(nn.Linear(last_layer_dim_vf, curr_layer_dim))\n",
    "            value_net.append(activation_fn())\n",
    "            last_layer_dim_vf = curr_layer_dim\n",
    "\n",
    "        # Save dim, used to create the distributions\n",
    "        self.latent_dim_pi = last_layer_dim_pi\n",
    "        self.latent_dim_vf = last_layer_dim_vf\n",
    "\n",
    "        # Create networks\n",
    "        # If the list of layers is empty, the network will just act as an Identity module\n",
    "        self.policy_net = nn.Sequential(*policy_net).to(device)\n",
    "        self.value_net = nn.Sequential(*value_net).to(device)\n",
    "\n",
    "    def forward(self, features: th.Tensor) -> Tuple[th.Tensor, th.Tensor]:\n",
    "        \"\"\"\n",
    "        :return: latent_policy, latent_value of the specified network.\n",
    "            If all layers are shared, then ``latent_policy == latent_value``\n",
    "        \"\"\"\n",
    "        return self.forward_actor(features), self.forward_critic(features)\n",
    "\n",
    "    def forward_actor(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.policy_net(features)\n",
    "\n",
    "    def forward_critic(self, features: th.Tensor) -> th.Tensor:\n",
    "        return self.value_net(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ActorCriticPolicy(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Features extractor\n",
    "        self.features_extractor = PromesPPO(observation_space=env.observation_space ,features_dim=80)\n",
    "\n",
    "        # MLP extractor\n",
    "        self.mlp_extractor = MlpExtractor(feature_dim=80, net_arch=[64, 64], activation_fn=nn.Tanh, device=device)\n",
    "\n",
    "        # Action net\n",
    "        self.action_net = nn.Linear(in_features=64, out_features=6, bias=True)\n",
    "\n",
    "        # Value net\n",
    "        self.value_net = nn.Linear(in_features=64, out_features=1, bias=True)\n",
    "\n",
    "    def forward(self, state):\n",
    "        # Extract features\n",
    "        features = self.features_extractor(state)\n",
    "\n",
    "        # Extract policy and value\n",
    "        policy = self.mlp_extractor.policy_net(features)\n",
    "        value = self.mlp_extractor.value_net(features)\n",
    "\n",
    "        # Get action\n",
    "        action = self.action_net(policy)\n",
    "\n",
    "        # Get value\n",
    "        value = self.value_net(value)\n",
    "\n",
    "        return action, value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a model with the same parameters\n",
    "pt_model = ActorCriticPolicy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract weights from model_policy\n",
    "weights = model_policy.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for ActorCriticPolicy:\n\tMissing key(s) in state_dict: \"features_extractor.net.net3_.fc1_1.weight\", \"features_extractor.net.net3_.fc1_1.bias\", \"features_extractor.net.net3_.fc1_2.weight\", \"features_extractor.net.net3_.fc1_2.bias\", \"features_extractor.net.net3_.fc2.weight\", \"features_extractor.net.net3_.fc2.bias\", \"features_extractor.net.net3_.fc3.weight\", \"features_extractor.net.net3_.fc3.bias\", \"features_extractor.net.fc1_3_1.weight\", \"features_extractor.net.fc1_3_1.bias\", \"features_extractor.net.fc1_3_2.weight\", \"features_extractor.net.fc1_3_2.bias\", \"features_extractor.net.fc1_3_3.weight\", \"features_extractor.net.fc1_3_3.bias\", \"features_extractor.net.fc1_3_4.weight\", \"features_extractor.net.fc1_3_4.bias\", \"features_extractor.net.fc1_3_5.weight\", \"features_extractor.net.fc1_3_5.bias\", \"features_extractor.net.fc2_1.weight\", \"features_extractor.net.fc2_1.bias\", \"features_extractor.net.fc2_2.weight\", \"features_extractor.net.fc2_2.bias\", \"features_extractor.net.fc2_3.weight\", \"features_extractor.net.fc2_3.bias\", \"features_extractor.net.fc2_4.weight\", \"features_extractor.net.fc2_4.bias\", \"features_extractor.net.fc2_5.weight\", \"features_extractor.net.fc2_5.bias\". \n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([64, 12]) from checkpoint, the shape in current model is torch.Size([64, 80]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([64, 12]) from checkpoint, the shape in current model is torch.Size([64, 80]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Load weights to pt_model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m pt_model\u001b[39m.\u001b[39;49mload_state_dict(weights)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for ActorCriticPolicy:\n\tMissing key(s) in state_dict: \"features_extractor.net.net3_.fc1_1.weight\", \"features_extractor.net.net3_.fc1_1.bias\", \"features_extractor.net.net3_.fc1_2.weight\", \"features_extractor.net.net3_.fc1_2.bias\", \"features_extractor.net.net3_.fc2.weight\", \"features_extractor.net.net3_.fc2.bias\", \"features_extractor.net.net3_.fc3.weight\", \"features_extractor.net.net3_.fc3.bias\", \"features_extractor.net.fc1_3_1.weight\", \"features_extractor.net.fc1_3_1.bias\", \"features_extractor.net.fc1_3_2.weight\", \"features_extractor.net.fc1_3_2.bias\", \"features_extractor.net.fc1_3_3.weight\", \"features_extractor.net.fc1_3_3.bias\", \"features_extractor.net.fc1_3_4.weight\", \"features_extractor.net.fc1_3_4.bias\", \"features_extractor.net.fc1_3_5.weight\", \"features_extractor.net.fc1_3_5.bias\", \"features_extractor.net.fc2_1.weight\", \"features_extractor.net.fc2_1.bias\", \"features_extractor.net.fc2_2.weight\", \"features_extractor.net.fc2_2.bias\", \"features_extractor.net.fc2_3.weight\", \"features_extractor.net.fc2_3.bias\", \"features_extractor.net.fc2_4.weight\", \"features_extractor.net.fc2_4.bias\", \"features_extractor.net.fc2_5.weight\", \"features_extractor.net.fc2_5.bias\". \n\tsize mismatch for mlp_extractor.policy_net.0.weight: copying a param with shape torch.Size([64, 12]) from checkpoint, the shape in current model is torch.Size([64, 80]).\n\tsize mismatch for mlp_extractor.value_net.0.weight: copying a param with shape torch.Size([64, 12]) from checkpoint, the shape in current model is torch.Size([64, 80])."
     ]
    }
   ],
   "source": [
    "# Load weights to pt_model\n",
    "pt_model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.2938,  1.9472, -0.3218,  0.2534,  2.2748, -3.2760]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[25.3541]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-6.2938,  1.9472, -0.3218,  0.2534,  2.2748, -3.2760]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-5.9904, -0.0553, -0.9415, -1.1002, -0.3881,  4.7698]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[-61.6606]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-6.6532, -0.6983,  1.0152,  0.3938,  1.4332, -0.1519]],\n",
       "        grad_fn=<AddmmBackward0>),\n",
       " tensor([[5.3250]], grad_fn=<AddmmBackward0>))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pt_model(sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
