{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN, HER, DDPG\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from kube_sim_gym.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pr_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, portion=0.5 ,train=True):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        # Resize the data to the portion\n",
    "        self.data = self.data.iloc[:int(len(self.data) * portion), :]\n",
    "\n",
    "        # Drop the row which has 0 for the last -2, -3 columns\n",
    "        # self.data = self.data.drop(self.data[(self.data.iloc[:, -2] == 0) & (self.data.iloc[:, -3] == 0)].index)\n",
    "\n",
    "        if train:\n",
    "            self.data = self.data.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            self.data = self.data.drop(self.data.sample(frac=0.8, random_state=42).index)\n",
    "\n",
    "        self.data = self.transform(self.data)\n",
    "        self.input = self.data[:, :12]\n",
    "        self.label = self.data[:, 12:13].long()\n",
    "\n",
    "    def transform(self, data):\n",
    "        return torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_expert.csv\")\n",
    "train_static_dataset = Pr_Dataset(data_path, 0.5, train=True)\n",
    "test_static_dataset = Pr_Dataset(data_path, 0.5, train=False)\n",
    "train_static_dataloader = DataLoader(train_static_dataset, batch_size=64, shuffle=False)\n",
    "test_static_dataloader = DataLoader(test_static_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 1])\n",
      "input1: tensor([[0.0800, 0.0000, 0.0900, 0.0600, 0.2000, 0.1400, 0.0300, 0.0000, 0.2300,\n",
      "         0.0300, 0.0400, 0.0500],\n",
      "        [0.0200, 0.0500, 0.1900, 0.0300, 0.2100, 0.0300, 0.2300, 0.0300, 0.1500,\n",
      "         0.0400, 0.0800, 0.1300],\n",
      "        [0.0000, 0.0400, 0.0200, 0.0000, 0.0200, 0.0500, 0.1000, 0.4500, 0.0300,\n",
      "         0.1600, 0.1500, 0.0800],\n",
      "        [0.0800, 0.1000, 0.0900, 0.1800, 0.2800, 0.1600, 0.1200, 0.1100, 0.1400,\n",
      "         0.0800, 0.0300, 0.0500],\n",
      "        [0.0600, 0.0000, 0.0400, 0.2300, 0.0300, 0.1100, 0.0700, 0.1100, 0.1100,\n",
      "         0.1000, 0.1100, 0.1200]])\n",
      "labels: tensor([[3],\n",
      "        [0],\n",
      "        [0],\n",
      "        [3],\n",
      "        [0]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_static_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input[:5]}\\nlabels: {labels[:5]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_step_3.py', scenario_file='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model_untrained = sb3.DQN('MlpPolicy', env, verbose=1)\n",
    "model_pretrained = sb3.DQN('MlpPolicy', env, verbose=1, exploration_initial_eps=0.05, exploration_final_eps=0.03)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_eval_env():\n",
    "    # Prepare Eval ENV & Callback\n",
    "    eval_env0 = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_3.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-10000p-10m_unbalanced.csv')\n",
    "    eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env4 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_ct.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "    return [eval_env0, eval_env1, eval_env2, eval_env3, eval_env4]\n",
    "\n",
    "def eval_model(model, eval_envs, verbose=False):\n",
    "    ret = []\n",
    "    print('Evaluation : train_step_3') if verbose else None\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[0], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\") if verbose else None\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Evaluation : eval_rur')  if verbose else None\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[1], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\") if verbose else None\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Evaluation : eval_rbd1') if verbose else None\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[2], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\") if verbose else None\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Evaluation : eval_rbd2') if verbose else None\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[3], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\") if verbose else None\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Episode length :') if verbose else None\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[4], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"Episode length:{mean_reward:.2f} +/- {std_reward:.2f}\") if verbose else None\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_agent(\n",
    "    student,\n",
    "    batch_size=64,\n",
    "    epochs=1000,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=1000,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    test_batch_size=64,\n",
    "):\n",
    "    use_cuda = not no_cuda and th.cuda.is_available()\n",
    "    th.manual_seed(seed)\n",
    "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
    "\n",
    "    if isinstance(env.action_space, gym.spaces.Box):\n",
    "        criterion = nn.MSELoss()\n",
    "    else:\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = nn.MSELoss()\n",
    "\n",
    "    # Extract initial policy\n",
    "    model = student.policy.to(device)\n",
    "\n",
    "    def train(model, device, train_loader, optimizer):\n",
    "        model.train()\n",
    "\n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            target = target.squeeze(-1)\n",
    "\n",
    "            if isinstance(env.action_space, gym.spaces.Box):\n",
    "                # A2C/PPO policy outputs actions, values, log_prob\n",
    "                # SAC/TD3 policy outputs actions only\n",
    "                if isinstance(student, (A2C, PPO)):\n",
    "                    action, _, _ = model(data)\n",
    "                else:\n",
    "                    # SAC/TD3:\n",
    "                    action = model(data)\n",
    "                action_prediction = action.double()\n",
    "            else:\n",
    "                action_prediction = model.q_net(data)\n",
    "                # target = model.q_net_target(data)\n",
    "                target = target.long()\n",
    "                # if isinstance(student, (DQN)):\n",
    "                #     action_prediction = model.q_net(data)\n",
    "                #     # target = model.q_net_target(data)\n",
    "                #     target = target.long()\n",
    "                # else:\n",
    "                #   # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                #   dist = model.get_distribution(data)\n",
    "                #   action_prediction = dist.distribution.logits\n",
    "                #   target = target.long()\n",
    "\n",
    "            loss = criterion(action_prediction, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            if batch_idx % log_interval == 0:\n",
    "                print(\n",
    "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
    "                        epoch,\n",
    "                        batch_idx * len(data),\n",
    "                        len(train_loader.dataset),\n",
    "                        100.0 * batch_idx / len(train_loader),\n",
    "                        loss.item(),\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    def test(model, device, test_loader):\n",
    "        model.eval()\n",
    "        test_loss = 0\n",
    "        with th.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "\n",
    "                if isinstance(env.action_space, gym.spaces.Box):\n",
    "                    # A2C/PPO policy outputs actions, values, log_prob\n",
    "                    # SAC/TD3 policy outputs actions only\n",
    "                    if isinstance(student, (A2C, PPO)):\n",
    "                        action, _, _ = model(data)\n",
    "                    else:\n",
    "                        # SAC/TD3:\n",
    "                        action = model(data)\n",
    "                    action_prediction = action.double()\n",
    "                else:\n",
    "                    if isinstance(student, (DQN)):\n",
    "                        action_prediction = model.q_net(data)\n",
    "                        target = model.q_net_target(data)\n",
    "                    else:\n",
    "                        # Retrieve the logits for A2C/PPO when using discrete actions\n",
    "                        dist = model.get_distribution(data)\n",
    "                        action_prediction = dist.distribution.logits\n",
    "                        target = target.long()\n",
    "\n",
    "                test_loss = criterion(action_prediction, target)\n",
    "        test_loss /= len(test_loader.dataset)\n",
    "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
    "\n",
    "    # # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
    "    # # and testing\n",
    "    # train_loader = th.utils.data.DataLoader(\n",
    "    #     dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
    "    # )\n",
    "    # test_loader = th.utils.data.DataLoader(\n",
    "    #     dataset=test_expert_dataset,\n",
    "    #     batch_size=test_batch_size,\n",
    "    #     shuffle=True,\n",
    "    #     **kwargs,\n",
    "    # )\n",
    "\n",
    "    # Define an Optimizer and a learning rate schedule.\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
    "\n",
    "    # Now we are finally ready to train the policy model.\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train(model, device, train_static_dataloader, optimizer)\n",
    "        test(model, device, test_static_dataloader)\n",
    "        scheduler.step()\n",
    "\n",
    "    # Implant the trained policy network back into the RL student agent\n",
    "    model_pretrained.policy = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_envs = init_eval_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swkim/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained result: [-2820.3200046904385, 0.0, 733.8700128365308, 0.0, 6226.279976069927, 0.0, 3360.8299952447414, 0.0, 4002.0, 0.0]\n",
      "Pretrained result: [-2811.660005379468, 0.0, 719.4800149668008, 0.0, 6240.909976303577, 0.0, 3373.640003889799, 0.0, 4002.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "untrained_result = eval_model(model_untrained, eval_envs)\n",
    "pretrained_result = eval_model(model_pretrained, eval_envs)\n",
    "\n",
    "print('Untrained result:', untrained_result)\n",
    "print('Pretrained result:', pretrained_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_env0 = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_3.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-10000p-10m_unbalanced.csv')\n",
    "eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "eval_env4 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_ct.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Train Epoch: 1 [0/1529199 (0%)]\tLoss: 1.773570\n",
      "Train Epoch: 1 [640000/1529199 (42%)]\tLoss: 0.305127\n",
      "Train Epoch: 1 [1280000/1529199 (84%)]\tLoss: 0.279394\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 2 [0/1529199 (0%)]\tLoss: 0.196469\n",
      "Train Epoch: 2 [640000/1529199 (42%)]\tLoss: 0.142052\n",
      "Train Epoch: 2 [1280000/1529199 (84%)]\tLoss: 0.188623\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 3 [0/1529199 (0%)]\tLoss: 0.108388\n",
      "Train Epoch: 3 [640000/1529199 (42%)]\tLoss: 0.071101\n",
      "Train Epoch: 3 [1280000/1529199 (84%)]\tLoss: 0.148466\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 4 [0/1529199 (0%)]\tLoss: 0.069776\n",
      "Train Epoch: 4 [640000/1529199 (42%)]\tLoss: 0.051751\n",
      "Train Epoch: 4 [1280000/1529199 (84%)]\tLoss: 0.122438\n",
      "Test set: Average loss: 0.0000\n",
      "Train Epoch: 5 [0/1529199 (0%)]\tLoss: 0.070997\n",
      "Train Epoch: 5 [640000/1529199 (42%)]\tLoss: 0.050344\n",
      "Train Epoch: 5 [1280000/1529199 (84%)]\tLoss: 0.100933\n",
      "Test set: Average loss: 0.0000\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.13e+03 |\n",
      "|    exploration_rate | 0.848     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 3366      |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 16008     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.12e+03 |\n",
      "|    exploration_rate | 0.696     |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 3391      |\n",
      "|    time_elapsed     | 9         |\n",
      "|    total_timesteps  | 32016     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.12e+03 |\n",
      "|    exploration_rate | 0.544     |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 3381      |\n",
      "|    time_elapsed     | 14        |\n",
      "|    total_timesteps  | 48024     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swkim/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=50000, episode_reward=11771.09 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.18e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.06e+03 |\n",
      "|    exploration_rate | 0.392     |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 323       |\n",
      "|    time_elapsed     | 197       |\n",
      "|    total_timesteps  | 64032     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 6.19      |\n",
      "|    n_updates        | 3507      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.04e+03 |\n",
      "|    exploration_rate | 0.24      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 366       |\n",
      "|    time_elapsed     | 218       |\n",
      "|    total_timesteps  | 80040     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.53      |\n",
      "|    n_updates        | 7509      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.02e+03 |\n",
      "|    exploration_rate | 0.0875    |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 399       |\n",
      "|    time_elapsed     | 240       |\n",
      "|    total_timesteps  | 96048     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.93      |\n",
      "|    n_updates        | 11511     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=10302.97 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.03e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 3.97     |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -2e+03   |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 28       |\n",
      "|    fps              | 259      |\n",
      "|    time_elapsed     | 431      |\n",
      "|    total_timesteps  | 112056   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.89     |\n",
      "|    n_updates        | 15513    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.97e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 283       |\n",
      "|    time_elapsed     | 451       |\n",
      "|    total_timesteps  | 128064    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.19      |\n",
      "|    n_updates        | 19515     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.93e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 304       |\n",
      "|    time_elapsed     | 472       |\n",
      "|    total_timesteps  | 144072    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.07      |\n",
      "|    n_updates        | 23517     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=9894.82 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 9.89e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.43     |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.9e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 40       |\n",
      "|    fps              | 242      |\n",
      "|    time_elapsed     | 659      |\n",
      "|    total_timesteps  | 160080   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.4      |\n",
      "|    n_updates        | 27519    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.86e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 258       |\n",
      "|    time_elapsed     | 681       |\n",
      "|    total_timesteps  | 176088    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.86      |\n",
      "|    n_updates        | 31521     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.82e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 272       |\n",
      "|    time_elapsed     | 704       |\n",
      "|    total_timesteps  | 192096    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.27      |\n",
      "|    n_updates        | 35523     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=10538.30 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.05e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 2.62     |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.8e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 52       |\n",
      "|    fps              | 231      |\n",
      "|    time_elapsed     | 897      |\n",
      "|    total_timesteps  | 208104   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.77     |\n",
      "|    n_updates        | 39525    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.78e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 243       |\n",
      "|    time_elapsed     | 918       |\n",
      "|    total_timesteps  | 224112    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.46      |\n",
      "|    n_updates        | 43527     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.75e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 255       |\n",
      "|    time_elapsed     | 939       |\n",
      "|    total_timesteps  | 240120    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.5       |\n",
      "|    n_updates        | 47529     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=7314.78 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 7.31e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.22     |\n",
      "|    n_updates        | 49999    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.73e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 226       |\n",
      "|    time_elapsed     | 1131      |\n",
      "|    total_timesteps  | 256128    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.906     |\n",
      "|    n_updates        | 51531     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.71e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 235       |\n",
      "|    time_elapsed     | 1155      |\n",
      "|    total_timesteps  | 272136    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.54      |\n",
      "|    n_updates        | 55533     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.69e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 72        |\n",
      "|    fps              | 245       |\n",
      "|    time_elapsed     | 1176      |\n",
      "|    total_timesteps  | 288144    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.25      |\n",
      "|    n_updates        | 59535     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=7077.00 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 7.08e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.21     |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.67e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 76        |\n",
      "|    fps              | 221       |\n",
      "|    time_elapsed     | 1371      |\n",
      "|    total_timesteps  | 304152    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.23      |\n",
      "|    n_updates        | 63537     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.65e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 80        |\n",
      "|    fps              | 230       |\n",
      "|    time_elapsed     | 1391      |\n",
      "|    total_timesteps  | 320160    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.909     |\n",
      "|    n_updates        | 67539     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.64e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 84        |\n",
      "|    fps              | 238       |\n",
      "|    time_elapsed     | 1410      |\n",
      "|    total_timesteps  | 336168    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.906     |\n",
      "|    n_updates        | 71541     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=6983.36 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 6.98e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 350000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.886    |\n",
      "|    n_updates        | 74999    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.63e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 88        |\n",
      "|    fps              | 221       |\n",
      "|    time_elapsed     | 1587      |\n",
      "|    total_timesteps  | 352176    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.1       |\n",
      "|    n_updates        | 75543     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.62e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 92        |\n",
      "|    fps              | 228       |\n",
      "|    time_elapsed     | 1611      |\n",
      "|    total_timesteps  | 368184    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 1.12      |\n",
      "|    n_updates        | 79545     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.61e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 96        |\n",
      "|    fps              | 235       |\n",
      "|    time_elapsed     | 1631      |\n",
      "|    total_timesteps  | 384192    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.554     |\n",
      "|    n_updates        | 83547     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=7049.42 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 7.05e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.996    |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.6e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 221      |\n",
      "|    time_elapsed     | 1808     |\n",
      "|    total_timesteps  | 400200   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.02     |\n",
      "|    n_updates        | 87549    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.57e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 104       |\n",
      "|    fps              | 227       |\n",
      "|    time_elapsed     | 1827      |\n",
      "|    total_timesteps  | 416208    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.294     |\n",
      "|    n_updates        | 91551     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.54e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 108       |\n",
      "|    fps              | 234       |\n",
      "|    time_elapsed     | 1846      |\n",
      "|    total_timesteps  | 432216    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.298     |\n",
      "|    n_updates        | 95553     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.51e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 112       |\n",
      "|    fps              | 240       |\n",
      "|    time_elapsed     | 1866      |\n",
      "|    total_timesteps  | 448224    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.57      |\n",
      "|    n_updates        | 99555     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=6471.55 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 6.47e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 450000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.569    |\n",
      "|    n_updates        | 99999    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.49e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 116       |\n",
      "|    fps              | 227       |\n",
      "|    time_elapsed     | 2041      |\n",
      "|    total_timesteps  | 464232    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.425     |\n",
      "|    n_updates        | 103557    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.47e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 120       |\n",
      "|    fps              | 233       |\n",
      "|    time_elapsed     | 2060      |\n",
      "|    total_timesteps  | 480240    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.895     |\n",
      "|    n_updates        | 107559    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.45e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 124       |\n",
      "|    fps              | 238       |\n",
      "|    time_elapsed     | 2079      |\n",
      "|    total_timesteps  | 496248    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.627     |\n",
      "|    n_updates        | 111561    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=6654.05 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 6.65e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.455    |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.43e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 128       |\n",
      "|    fps              | 227       |\n",
      "|    time_elapsed     | 2254      |\n",
      "|    total_timesteps  | 512256    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.54      |\n",
      "|    n_updates        | 115563    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.41e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 132       |\n",
      "|    fps              | 232       |\n",
      "|    time_elapsed     | 2274      |\n",
      "|    total_timesteps  | 528264    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.653     |\n",
      "|    n_updates        | 119565    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.41e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 136       |\n",
      "|    fps              | 237       |\n",
      "|    time_elapsed     | 2293      |\n",
      "|    total_timesteps  | 544272    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.389     |\n",
      "|    n_updates        | 123567    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=6484.67 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 6.48e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 550000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.542    |\n",
      "|    n_updates        | 124999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.4e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 140      |\n",
      "|    fps              | 226      |\n",
      "|    time_elapsed     | 2468     |\n",
      "|    total_timesteps  | 560280   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.5      |\n",
      "|    n_updates        | 127569   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.39e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 144       |\n",
      "|    fps              | 231       |\n",
      "|    time_elapsed     | 2487      |\n",
      "|    total_timesteps  | 576288    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.139     |\n",
      "|    n_updates        | 131571    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.39e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 148       |\n",
      "|    fps              | 236       |\n",
      "|    time_elapsed     | 2506      |\n",
      "|    total_timesteps  | 592296    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.546     |\n",
      "|    n_updates        | 135573    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=6409.67 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 6.41e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.632    |\n",
      "|    n_updates        | 137499   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.38e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 152       |\n",
      "|    fps              | 226       |\n",
      "|    time_elapsed     | 2685      |\n",
      "|    total_timesteps  | 608304    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.572     |\n",
      "|    n_updates        | 139575    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.37e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 156       |\n",
      "|    fps              | 230       |\n",
      "|    time_elapsed     | 2704      |\n",
      "|    total_timesteps  | 624312    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.191     |\n",
      "|    n_updates        | 143577    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.37e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 160       |\n",
      "|    fps              | 235       |\n",
      "|    time_elapsed     | 2723      |\n",
      "|    total_timesteps  | 640320    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.205     |\n",
      "|    n_updates        | 147579    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=6475.26 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 6.48e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 650000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.3      |\n",
      "|    n_updates        | 149999   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.36e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 164       |\n",
      "|    fps              | 226       |\n",
      "|    time_elapsed     | 2901      |\n",
      "|    total_timesteps  | 656328    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.29      |\n",
      "|    n_updates        | 151581    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.36e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 168       |\n",
      "|    fps              | 230       |\n",
      "|    time_elapsed     | 2920      |\n",
      "|    total_timesteps  | 672336    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.293     |\n",
      "|    n_updates        | 155583    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.36e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 172       |\n",
      "|    fps              | 234       |\n",
      "|    time_elapsed     | 2939      |\n",
      "|    total_timesteps  | 688344    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.236     |\n",
      "|    n_updates        | 159585    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=8838.02 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 8.84e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 162499   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.35e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 176       |\n",
      "|    fps              | 225       |\n",
      "|    time_elapsed     | 3122      |\n",
      "|    total_timesteps  | 704352    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.199     |\n",
      "|    n_updates        | 163587    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.35e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 180       |\n",
      "|    fps              | 229       |\n",
      "|    time_elapsed     | 3141      |\n",
      "|    total_timesteps  | 720360    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.27      |\n",
      "|    n_updates        | 167589    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.35e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 184       |\n",
      "|    fps              | 232       |\n",
      "|    time_elapsed     | 3161      |\n",
      "|    total_timesteps  | 736368    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.199     |\n",
      "|    n_updates        | 171591    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=10825.73 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.08e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 750000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 1.64     |\n",
      "|    n_updates        | 174999   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.34e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 188       |\n",
      "|    fps              | 224       |\n",
      "|    time_elapsed     | 3345      |\n",
      "|    total_timesteps  | 752376    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.355     |\n",
      "|    n_updates        | 175593    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.33e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 192       |\n",
      "|    fps              | 228       |\n",
      "|    time_elapsed     | 3364      |\n",
      "|    total_timesteps  | 768384    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.181     |\n",
      "|    n_updates        | 179595    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.33e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 196       |\n",
      "|    fps              | 231       |\n",
      "|    time_elapsed     | 3384      |\n",
      "|    total_timesteps  | 784392    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.352     |\n",
      "|    n_updates        | 183597    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=11547.33 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.15e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.111    |\n",
      "|    n_updates        | 187499   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.32e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 200       |\n",
      "|    fps              | 224       |\n",
      "|    time_elapsed     | 3571      |\n",
      "|    total_timesteps  | 800400    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.368     |\n",
      "|    n_updates        | 187599    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.31e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 204       |\n",
      "|    fps              | 227       |\n",
      "|    time_elapsed     | 3590      |\n",
      "|    total_timesteps  | 816408    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.3       |\n",
      "|    n_updates        | 191601    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.3e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 208      |\n",
      "|    fps              | 230      |\n",
      "|    time_elapsed     | 3610     |\n",
      "|    total_timesteps  | 832416   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.242    |\n",
      "|    n_updates        | 195603   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.29e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 212       |\n",
      "|    fps              | 233       |\n",
      "|    time_elapsed     | 3629      |\n",
      "|    total_timesteps  | 848424    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.299     |\n",
      "|    n_updates        | 199605    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=11497.64 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.15e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 850000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.281    |\n",
      "|    n_updates        | 199999   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.28e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 216       |\n",
      "|    fps              | 226       |\n",
      "|    time_elapsed     | 3818      |\n",
      "|    total_timesteps  | 864432    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.184     |\n",
      "|    n_updates        | 203607    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.27e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 220       |\n",
      "|    fps              | 229       |\n",
      "|    time_elapsed     | 3837      |\n",
      "|    total_timesteps  | 880440    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.105     |\n",
      "|    n_updates        | 207609    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.26e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 224       |\n",
      "|    fps              | 232       |\n",
      "|    time_elapsed     | 3856      |\n",
      "|    total_timesteps  | 896448    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.248     |\n",
      "|    n_updates        | 211611    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=11704.14 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.17e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.238    |\n",
      "|    n_updates        | 212499   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.25e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 228       |\n",
      "|    fps              | 225       |\n",
      "|    time_elapsed     | 4047      |\n",
      "|    total_timesteps  | 912456    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.298     |\n",
      "|    n_updates        | 215613    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.24e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 232       |\n",
      "|    fps              | 228       |\n",
      "|    time_elapsed     | 4067      |\n",
      "|    total_timesteps  | 928464    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.243     |\n",
      "|    n_updates        | 219615    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.23e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 236       |\n",
      "|    fps              | 231       |\n",
      "|    time_elapsed     | 4087      |\n",
      "|    total_timesteps  | 944472    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.18      |\n",
      "|    n_updates        | 223617    |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=11739.62 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.17e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 950000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0592   |\n",
      "|    n_updates        | 224999   |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.22e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 240       |\n",
      "|    fps              | 224       |\n",
      "|    time_elapsed     | 4280      |\n",
      "|    total_timesteps  | 960480    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.161     |\n",
      "|    n_updates        | 227619    |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.21e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 244       |\n",
      "|    fps              | 226       |\n",
      "|    time_elapsed     | 4302      |\n",
      "|    total_timesteps  | 976488    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.158     |\n",
      "|    n_updates        | 231621    |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.2e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 248      |\n",
      "|    fps              | 229      |\n",
      "|    time_elapsed     | 4322     |\n",
      "|    total_timesteps  | 992496   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.101    |\n",
      "|    n_updates        | 235623   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=11708.30 +/- 0.00\n",
      "Episode length: 13002.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.3e+04  |\n",
      "|    mean_reward      | 1.17e+04 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.232    |\n",
      "|    n_updates        | 237499   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fcf652f45b0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_callback = EvalCallback(eval_env2, eval_freq=50000, deterministic=True, render=False, verbose=1) # Change eval_env!\n",
    "\n",
    "model_test = sb3.DQN(\"MlpPolicy\", env, verbose=1) \n",
    "\n",
    "pretrain_agent(\n",
    "    model_test,\n",
    "    epochs=5,\n",
    "    scheduler_gamma=0.7,\n",
    "    learning_rate=1.0,\n",
    "    log_interval=10000,\n",
    "    no_cuda=True,\n",
    "    seed=1,\n",
    "    batch_size=64,\n",
    "    test_batch_size=1000,\n",
    ")\n",
    "# model_pretrained.save(\"a2c_student\")\n",
    "\n",
    "model_test.learn(total_timesteps=1000000, callback=eval_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation : train_step_3\n",
      "mean_reward:-188.44 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1425.31 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11845.16 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1509.48 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1606.00 +/- 0.00\n",
      "Pretrained result: [-188.4399991016835, 0.0, 1425.3099972177297, 0.0, 11845.16000956297, 0.0, 1509.4800036549568, 0.0, 1606.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "pretrained_result = eval_model(model_pretrained, eval_envs, True)\n",
    "\n",
    "print('Pretrained result:', pretrained_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.11e+03 |\n",
      "|    exploration_rate | 0.0436    |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 2254      |\n",
      "|    time_elapsed     | 7         |\n",
      "|    total_timesteps  | 16008     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -2.1e+03 |\n",
      "|    exploration_rate | 0.0372   |\n",
      "| time/               |          |\n",
      "|    episodes         | 8        |\n",
      "|    fps              | 2233     |\n",
      "|    time_elapsed     | 14       |\n",
      "|    total_timesteps  | 32016    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -2.1e+03 |\n",
      "|    exploration_rate | 0.0308   |\n",
      "| time/               |          |\n",
      "|    episodes         | 12       |\n",
      "|    fps              | 2213     |\n",
      "|    time_elapsed     | 21       |\n",
      "|    total_timesteps  | 48024    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.05e+03 |\n",
      "|    exploration_rate | 0.03      |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 1412      |\n",
      "|    time_elapsed     | 45        |\n",
      "|    total_timesteps  | 64032     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 6.11      |\n",
      "|    n_updates        | 3507      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.02e+03 |\n",
      "|    exploration_rate | 0.03      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 1133      |\n",
      "|    time_elapsed     | 70        |\n",
      "|    total_timesteps  | 80040     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.03      |\n",
      "|    n_updates        | 7509      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.99e+03 |\n",
      "|    exploration_rate | 0.03      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 1016      |\n",
      "|    time_elapsed     | 94        |\n",
      "|    total_timesteps  | 96048     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.86      |\n",
      "|    n_updates        | 11511     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.96e+03 |\n",
      "|    exploration_rate | 0.03      |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 938       |\n",
      "|    time_elapsed     | 119       |\n",
      "|    total_timesteps  | 112056    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.14      |\n",
      "|    n_updates        | 15513     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.92e+03 |\n",
      "|    exploration_rate | 0.03      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 884       |\n",
      "|    time_elapsed     | 144       |\n",
      "|    total_timesteps  | 128064    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 3.59      |\n",
      "|    n_updates        | 19515     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.88e+03 |\n",
      "|    exploration_rate | 0.03      |\n",
      "| time/               |           |\n",
      "|    episodes         | 36        |\n",
      "|    fps              | 828       |\n",
      "|    time_elapsed     | 173       |\n",
      "|    total_timesteps  | 144072    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 2.92      |\n",
      "|    n_updates        | 23517     |\n",
      "-----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m model_pretrained\u001b[39m.\u001b[39mexploration_initial_eps \u001b[39m=\u001b[39m \u001b[39m0.05\u001b[39m\n\u001b[1;32m      2\u001b[0m model_pretrained\u001b[39m.\u001b[39mexploration_final_eps \u001b[39m=\u001b[39m \u001b[39m0.02\u001b[39m\n\u001b[0;32m----> 4\u001b[0m model_pretrained\u001b[39m.\u001b[39;49mlearn(\u001b[39m500000\u001b[39;49m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py:269\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[1;32m    262\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    268\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    270\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    271\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    272\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    273\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    274\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    275\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:330\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 330\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    332\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    334\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py:219\u001b[0m, in \u001b[0;36mDQN.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[39m# Optimize the policy\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 219\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m    220\u001b[0m \u001b[39m# Clip gradient norm\u001b[39;00m\n\u001b[1;32m    221\u001b[0m th\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mclip_grad_norm_(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mparameters(), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmax_grad_norm)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    175\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_pretrained.exploration_initial_eps = 0.05\n",
    "model_pretrained.exploration_final_eps = 0.02\n",
    "\n",
    "model_pretrained.learn(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
