{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)\n",
    "\n",
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from kube_sim_gym.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pr_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, train=True):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        # Drop the row which has 0 for the last -2, -3 columns\n",
    "        # self.data = self.data.drop(self.data[(self.data.iloc[:, -2] == 0) & (self.data.iloc[:, -3] == 0)].index)\n",
    "\n",
    "        if train:\n",
    "            self.data = self.data.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            self.data = self.data.drop(self.data.sample(frac=0.8, random_state=42).index)\n",
    "\n",
    "        self.data = self.transform(self.data)\n",
    "        self.input = self.data[:, :-6]\n",
    "        self.label = self.data[:, -6:]\n",
    "\n",
    "    def transform(self, data):\n",
    "        return torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_step3.csv\")\n",
    "train_static_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_static_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_static_dataloader = DataLoader(train_static_dataset, batch_size=64, shuffle=False)\n",
    "test_static_dataloader = DataLoader(test_static_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.8400, 0.9500, 0.7600, 1.0000, 0.9500, 0.8600, 0.9200, 0.7200, 0.7100,\n",
      "         0.7000, 0.1100, 0.1400],\n",
      "        [0.6200, 0.6500, 0.9200, 0.1200, 0.4500, 0.1900, 0.7600, 0.5400, 0.7400,\n",
      "         0.8400, 0.2500, 0.2400],\n",
      "        [0.0900, 0.1100, 0.3800, 0.2100, 0.5300, 0.8700, 0.4300, 0.1100, 0.5600,\n",
      "         0.4900, 0.1100, 0.0000],\n",
      "        [0.9600, 0.7100, 0.9500, 0.8200, 0.8000, 1.0000, 0.8800, 0.8600, 0.8600,\n",
      "         0.9700, 0.1200, 0.2400],\n",
      "        [0.6800, 0.6400, 0.3700, 0.3900, 0.5700, 0.9800, 0.3400, 0.0400, 0.8700,\n",
      "         0.7200, 0.0100, 0.0400]])\n",
      "labels: tensor([[-0.0800, -0.5200, -0.6500, -0.5000, -0.6100, -0.0800],\n",
      "        [-0.1600, -0.2000, -1.2900, -0.4000, -0.7100, -0.5900],\n",
      "        [-0.1700, -0.2500, -0.4500, -0.4000, -0.6000, -0.3600],\n",
      "        [-0.0600, -0.6400, -0.5200, -0.5900, -0.4100, -0.5000],\n",
      "        [-0.1900, -0.2000, -0.2400, -0.9300, -0.4600, -0.3100]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_static_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input[:5]}\\nlabels: {labels[:5]}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_step_3.py', scenario_file='random')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sb3.DQN('MlpPolicy', env)\n",
    "model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlattenExtractor(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int):\n",
    "        super(FlattenExtractor, self).__init__(observation_space, features_dim)\n",
    "        # Flatten the observation space\n",
    "        self.flatten = nn.Flatten()\n",
    "        # self.fe = nn.Linear(observation_space.shape[0], features_dim)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FlattenExtractor(\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=12, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=64, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FlattenExtractor,\n",
    "    features_extractor_kwargs=dict(features_dim=12),\n",
    "    net_arch=[64, 64]\n",
    ")\n",
    "\n",
    "rl_model_untrained = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)\n",
    "\n",
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_net = DQN_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_net.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0121, Train Acc: 93.49%, Test Loss: 0.0072, Test Acc: 95.97%\n",
      "Epoch 2: Train Loss: 0.0064, Train Acc: 94.66%, Test Loss: 0.0060, Test Acc: 95.98%\n",
      "Epoch 3: Train Loss: 0.0056, Train Acc: 94.38%, Test Loss: 0.0054, Test Acc: 95.84%\n",
      "Epoch 4: Train Loss: 0.0051, Train Acc: 94.09%, Test Loss: 0.0049, Test Acc: 95.87%\n",
      "Epoch 5: Train Loss: 0.0044, Train Acc: 93.91%, Test Loss: 0.0040, Test Acc: 95.98%\n",
      "Epoch 6: Train Loss: 0.0036, Train Acc: 94.07%, Test Loss: 0.0035, Test Acc: 95.98%\n",
      "Epoch 7: Train Loss: 0.0031, Train Acc: 94.28%, Test Loss: 0.0030, Test Acc: 95.24%\n",
      "Epoch 8: Train Loss: 0.0026, Train Acc: 94.37%, Test Loss: 0.0025, Test Acc: 95.78%\n",
      "Epoch 9: Train Loss: 0.0023, Train Acc: 94.44%, Test Loss: 0.0021, Test Acc: 96.19%\n",
      "Epoch 10: Train Loss: 0.0020, Train Acc: 94.60%, Test Loss: 0.0018, Test Acc: 95.77%\n",
      "Epoch 11: Train Loss: 0.0018, Train Acc: 94.74%, Test Loss: 0.0016, Test Acc: 95.35%\n",
      "Epoch 12: Train Loss: 0.0016, Train Acc: 94.88%, Test Loss: 0.0014, Test Acc: 95.77%\n",
      "Epoch 13: Train Loss: 0.0015, Train Acc: 94.96%, Test Loss: 0.0013, Test Acc: 95.88%\n",
      "Epoch 14: Train Loss: 0.0014, Train Acc: 95.03%, Test Loss: 0.0014, Test Acc: 95.83%\n",
      "Epoch 15: Train Loss: 0.0013, Train Acc: 95.08%, Test Loss: 0.0013, Test Acc: 95.66%\n",
      "Epoch 16: Train Loss: 0.0012, Train Acc: 95.12%, Test Loss: 0.0011, Test Acc: 95.64%\n",
      "Epoch 17: Train Loss: 0.0012, Train Acc: 95.17%, Test Loss: 0.0011, Test Acc: 95.43%\n",
      "Epoch 18: Train Loss: 0.0011, Train Acc: 95.24%, Test Loss: 0.0013, Test Acc: 95.22%\n",
      "Epoch 19: Train Loss: 0.0011, Train Acc: 95.28%, Test Loss: 0.0010, Test Acc: 94.91%\n",
      "Epoch 20: Train Loss: 0.0010, Train Acc: 95.42%, Test Loss: 0.0012, Test Acc: 95.09%\n",
      "Epoch 21: Train Loss: 0.0010, Train Acc: 95.47%, Test Loss: 0.0015, Test Acc: 94.86%\n",
      "Epoch 22: Train Loss: 0.0009, Train Acc: 95.47%, Test Loss: 0.0011, Test Acc: 95.26%\n",
      "Epoch 23: Train Loss: 0.0009, Train Acc: 95.57%, Test Loss: 0.0013, Test Acc: 95.10%\n",
      "Epoch 24: Train Loss: 0.0009, Train Acc: 95.58%, Test Loss: 0.0010, Test Acc: 95.79%\n",
      "Epoch 25: Train Loss: 0.0009, Train Acc: 95.60%, Test Loss: 0.0011, Test Acc: 95.01%\n",
      "Epoch 26: Train Loss: 0.0008, Train Acc: 95.70%, Test Loss: 0.0010, Test Acc: 95.82%\n",
      "Epoch 27: Train Loss: 0.0008, Train Acc: 95.71%, Test Loss: 0.0009, Test Acc: 96.40%\n",
      "Epoch 28: Train Loss: 0.0008, Train Acc: 95.80%, Test Loss: 0.0010, Test Acc: 96.42%\n",
      "Epoch 29: Train Loss: 0.0008, Train Acc: 95.81%, Test Loss: 0.0009, Test Acc: 96.72%\n",
      "Epoch 30: Train Loss: 0.0008, Train Acc: 95.82%, Test Loss: 0.0009, Test Acc: 96.02%\n"
     ]
    }
   ],
   "source": [
    "epochs = 30\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_net, train_static_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_net, test_static_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 98:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.00921137,  0.00387826,  0.00025387, ..., -0.04905361,\n",
       "        -0.13919872,  0.01619057], dtype=float32),\n",
       " array([-0.00921137,  0.00387826,  0.00025387, ..., -0.04905361,\n",
       "        -0.13919872,  0.01619057], dtype=float32))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqn_net.q_net.parameters_to_vector(), rl_model.q_net.parameters_to_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_model.policy.q_net.load_state_dict(dqn_net.q_net.state_dict())\n",
    "# rl_model.policy.q_net_target.load_state_dict(dqn_net.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_eval_env():\n",
    "    # Prepare Eval ENV & Callback\n",
    "    eval_env0 = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_3.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-10000p-10m_unbalanced.csv')\n",
    "    eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env4 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_ct.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "    return [eval_env0, eval_env1, eval_env2, eval_env3, eval_env4]\n",
    "\n",
    "def eval_model(model, eval_envs):\n",
    "    ret = []\n",
    "    print('Evaluation : train_step_3')\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[0], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Evaluation : eval_rur')\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[1], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Evaluation : eval_rbd1')\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[2], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Evaluation : eval_rbd2')\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[3], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    print('Episode length :')\n",
    "    mean_reward, std_reward = evaluate_policy(model, eval_envs[4], n_eval_episodes=1, deterministic=True)\n",
    "    print(f\"Episode length:{mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "    ret += [mean_reward, std_reward]\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_model.learn(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rl_model_untrained.learn(500000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_envs = init_eval_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_log = eval_model(rl_model, eval_envs)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Training {i}th iteration...\")\n",
    "\n",
    "    rl_model.learn(total_timesteps=100000, log_interval=10000)\n",
    "    print(f\"{i}th training done\")\n",
    "    eval_log = eval_model(rl_model, eval_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation : train_step_3\n",
      "mean_reward:-2811.66 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:719.48 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:6240.91 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:3373.64 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:4002.00 +/- 0.00\n",
      "Training 0th iteration...\n",
      "0th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-544.64 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1450.18 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11346.44 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1805.85 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1927.00 +/- 0.00\n",
      "Training 1th iteration...\n",
      "1th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-407.11 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1437.37 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11729.48 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1638.33 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1725.00 +/- 0.00\n",
      "Training 2th iteration...\n",
      "2th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-391.28 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1427.06 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11851.51 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1561.62 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1666.00 +/- 0.00\n",
      "Training 3th iteration...\n",
      "3th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-406.50 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1425.46 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11724.76 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1542.77 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1656.00 +/- 0.00\n",
      "Training 4th iteration...\n",
      "4th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-382.27 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1431.77 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11842.67 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1551.39 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1666.00 +/- 0.00\n",
      "Training 5th iteration...\n",
      "5th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-382.72 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1425.54 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11882.39 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1529.17 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1639.00 +/- 0.00\n",
      "Training 6th iteration...\n",
      "6th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-1328.59 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:7.25 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:12990.50 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:4001.78 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:4002.00 +/- 0.00\n",
      "Training 7th iteration...\n",
      "7th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-375.01 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1424.50 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:11998.00 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1517.93 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1627.00 +/- 0.00\n",
      "Training 8th iteration...\n",
      "8th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-389.20 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1425.63 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:12056.97 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1507.04 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1625.00 +/- 0.00\n",
      "Training 9th iteration...\n",
      "9th training done\n",
      "Evaluation : train_step_3\n",
      "mean_reward:-387.73 +/- 0.00\n",
      "Evaluation : eval_rur\n",
      "mean_reward:1424.30 +/- 0.00\n",
      "Evaluation : eval_rbd1\n",
      "mean_reward:12037.26 +/- 0.00\n",
      "Evaluation : eval_rbd2\n",
      "mean_reward:1511.88 +/- 0.00\n",
      "Episode length :\n",
      "Episode length:1625.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "eval_log = eval_model(rl_model_untrained, eval_envs)\n",
    "\n",
    "for i in range(10):\n",
    "    print(f\"Training {i}th iteration...\")\n",
    "\n",
    "    rl_model_untrained.learn(total_timesteps=100000, log_interval=10000)\n",
    "    print(f\"{i}th training done\")\n",
    "    eval_log = eval_model(rl_model_untrained, eval_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.08e+03 |\n",
      "|    exploration_rate | 0.493     |\n",
      "| time/               |           |\n",
      "|    episodes         | 4         |\n",
      "|    fps              | 3210      |\n",
      "|    time_elapsed     | 4         |\n",
      "|    total_timesteps  | 16008     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.09e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 8         |\n",
      "|    fps              | 3303      |\n",
      "|    time_elapsed     | 9         |\n",
      "|    total_timesteps  | 32016     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.09e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 12        |\n",
      "|    fps              | 3387      |\n",
      "|    time_elapsed     | 14        |\n",
      "|    total_timesteps  | 48024     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -2.01e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 16        |\n",
      "|    fps              | 1559      |\n",
      "|    time_elapsed     | 41        |\n",
      "|    total_timesteps  | 64032     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0148    |\n",
      "|    n_updates        | 3507      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.97e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 20        |\n",
      "|    fps              | 1063      |\n",
      "|    time_elapsed     | 75        |\n",
      "|    total_timesteps  | 80040     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0199    |\n",
      "|    n_updates        | 7509      |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.92e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 24        |\n",
      "|    fps              | 847       |\n",
      "|    time_elapsed     | 113       |\n",
      "|    total_timesteps  | 96048     |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0155    |\n",
      "|    n_updates        | 11511     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.88e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 28        |\n",
      "|    fps              | 772       |\n",
      "|    time_elapsed     | 145       |\n",
      "|    total_timesteps  | 112056    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0256    |\n",
      "|    n_updates        | 15513     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.83e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 32        |\n",
      "|    fps              | 719       |\n",
      "|    time_elapsed     | 178       |\n",
      "|    total_timesteps  | 128064    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0288    |\n",
      "|    n_updates        | 19515     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.8e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 36       |\n",
      "|    fps              | 702      |\n",
      "|    time_elapsed     | 204      |\n",
      "|    total_timesteps  | 144072   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 23517    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.75e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 40        |\n",
      "|    fps              | 662       |\n",
      "|    time_elapsed     | 241       |\n",
      "|    total_timesteps  | 160080    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0203    |\n",
      "|    n_updates        | 27519     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.71e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 44        |\n",
      "|    fps              | 642       |\n",
      "|    time_elapsed     | 273       |\n",
      "|    total_timesteps  | 176088    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0185    |\n",
      "|    n_updates        | 31521     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.68e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 48        |\n",
      "|    fps              | 615       |\n",
      "|    time_elapsed     | 311       |\n",
      "|    total_timesteps  | 192096    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0247    |\n",
      "|    n_updates        | 35523     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.64e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 52        |\n",
      "|    fps              | 603       |\n",
      "|    time_elapsed     | 345       |\n",
      "|    total_timesteps  | 208104    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0459    |\n",
      "|    n_updates        | 39525     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.61e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 56        |\n",
      "|    fps              | 595       |\n",
      "|    time_elapsed     | 376       |\n",
      "|    total_timesteps  | 224112    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0315    |\n",
      "|    n_updates        | 43527     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.58e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 60        |\n",
      "|    fps              | 589       |\n",
      "|    time_elapsed     | 407       |\n",
      "|    total_timesteps  | 240120    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0505    |\n",
      "|    n_updates        | 47529     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.55e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 64        |\n",
      "|    fps              | 582       |\n",
      "|    time_elapsed     | 439       |\n",
      "|    total_timesteps  | 256128    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.074     |\n",
      "|    n_updates        | 51531     |\n",
      "-----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4e+03     |\n",
      "|    ep_rew_mean      | -1.52e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 68        |\n",
      "|    fps              | 582       |\n",
      "|    time_elapsed     | 466       |\n",
      "|    total_timesteps  | 272136    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0176    |\n",
      "|    n_updates        | 55533     |\n",
      "-----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4e+03    |\n",
      "|    ep_rew_mean      | -1.5e+03 |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 72       |\n",
      "|    fps              | 586      |\n",
      "|    time_elapsed     | 491      |\n",
      "|    total_timesteps  | 288144   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0194   |\n",
      "|    n_updates        | 59535    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7f9a9915f8e0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = sb3.DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "model.learn(300000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_model(model, eval_envs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
