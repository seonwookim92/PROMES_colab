{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.data.types import Transitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test version\n",
    "data = np.genfromtxt('../dataset/data_expert.csv', delimiter=',', max_rows=1000000) \n",
    "\n",
    "# Final version\n",
    "# data = np.genfromtxt('../dataset/data_expert.csv', delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into obs, acts, infos, next_obs, and dones\n",
    "obs = data[:, :12]\n",
    "acts = data[:, 12]\n",
    "infos = np.empty(len(data), dtype=dict)\n",
    "next_obs = data[:, 13:25]\n",
    "dones = data[:, 25].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = Transitions(obs, acts, infos, next_obs, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs': array([0.91, 0.85, 0.73, 0.76, 0.83, 0.75, 0.82, 0.94, 0.76, 0.84, 0.05,\n",
       "        0.14]),\n",
       " 'acts': 1.0,\n",
       " 'infos': None,\n",
       " 'next_obs': array([0.86, 0.71, 0.73, 0.76, 0.83, 0.75, 0.82, 0.94, 0.76, 0.84, 0.04,\n",
       "        0.06]),\n",
       " 'dones': False}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions[13]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert from newly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_expert():\n",
    "#     print(\"Training a expert.\")\n",
    "\n",
    "#     expert = DQN(\n",
    "#         policy='MlpPolicy',\n",
    "#         env=env\n",
    "#     )\n",
    "\n",
    "#     expert.learn(10000)  # Note: change this to 100000 to train a decent expert.\n",
    "#     return expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_expert_transitions():\n",
    "#     expert = train_expert()\n",
    "\n",
    "#     print(\"Sampling expert transitions.\")\n",
    "#     rollouts = rollout.rollout(\n",
    "#         expert,\n",
    "#         DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n",
    "#         rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "#         rng=rng,\n",
    "#     )\n",
    "#     return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# def eval_unit(model, env, num_eval_episodes=2):\n",
    "#     rews, lens = evaluate_policy(model, env, n_eval_episodes=num_eval_episodes, return_episode_rewards=True)\n",
    "#     # Takes the mean of rews elements divided by lens elements\n",
    "#     mean_rew = np.mean([rew / length for rew, length in zip(rews, lens)]).round(2)\n",
    "\n",
    "#     # Takes the difference between the maximum and minimum of reward elements\n",
    "#     std_rew = (np.max(rews) - np.min(rews)) / 2\n",
    "\n",
    "#     mean_rew = round(mean_rew, 2)\n",
    "#     std_rew = round(std_rew, 2)\n",
    "\n",
    "#     return (mean_rew, std_rew)\n",
    "\n",
    "\n",
    "# def eval_set(model):\n",
    "#     eval_rur_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rur_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rur_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "#     eval_rbd1_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd1_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd1_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "#     eval_rbd2_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd2_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd2_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "#     rur1 = eval_unit(model, eval_rur_env1, num_eval_episodes=2)\n",
    "#     rur2 = eval_unit(model, eval_rur_env2, num_eval_episodes=2)\n",
    "#     rur3 = eval_unit(model, eval_rur_env3, num_eval_episodes=2)\n",
    "\n",
    "#     print(rur1, rur2, rur3)\n",
    "\n",
    "#     rbd11 = eval_unit(model, eval_rbd1_env1, num_eval_episodes=2)\n",
    "#     rbd12 = eval_unit(model, eval_rbd1_env2, num_eval_episodes=2)\n",
    "#     rbd13 = eval_unit(model, eval_rbd1_env3, num_eval_episodes=2)\n",
    "\n",
    "#     print(rbd11, rbd12, rbd13)\n",
    "\n",
    "#     rbd21 = eval_unit(model, eval_rbd2_env1, num_eval_episodes=2)\n",
    "#     rbd22 = eval_unit(model, eval_rbd2_env2, num_eval_episodes=2)\n",
    "#     rbd23 = eval_unit(model, eval_rbd2_env3, num_eval_episodes=2)\n",
    "\n",
    "#     print(rbd21, rbd22, rbd23)\n",
    "\n",
    "def evalnoma(eval_model):\n",
    "\n",
    "    eval_env0 = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_3.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "    step3 = evaluate_policy(eval_model, eval_env0, n_eval_episodes=1, return_episode_rewards=True)\n",
    "    rur = evaluate_policy(eval_model, eval_env1, n_eval_episodes=1, return_episode_rewards=True)\n",
    "    rbd1 = evaluate_policy(eval_model, eval_env2, n_eval_episodes=1, return_episode_rewards=True)\n",
    "    rbd2 = evaluate_policy(eval_model, eval_env3, n_eval_episodes=1, return_episode_rewards=True)\n",
    "\n",
    "    print(step3, rur, rbd1, rbd2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize environment without rendering\n",
    "env = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_3.py')\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "([-5877.09001268819], [8369]) ([1517.7100288439542], [8369]) ([8144.810059189796], [8369]) ([5934.309975683689], [8369])\n"
     ]
    }
   ],
   "source": [
    "evalnoma(bc_trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00179 |\n",
      "|    entropy        | 1.79     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 88.5     |\n",
      "|    loss           | 1.79     |\n",
      "|    neglogp        | 1.79     |\n",
      "|    prob_true_act  | 0.167    |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99991batch [07:02, 245.13batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 100000    |\n",
      "|    ent_loss       | -0.000158 |\n",
      "|    entropy        | 0.158     |\n",
      "|    epoch          | 3         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 1.48e+03  |\n",
      "|    loss           | 0.212     |\n",
      "|    neglogp        | 0.212     |\n",
      "|    prob_true_act  | 0.909     |\n",
      "|    samples_so_far | 3200032   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "156250batch [11:01, 236.22batch/s]\n"
     ]
    }
   ],
   "source": [
    "# bc_trainer.train(log_interval=100000, n_batches=100000) # Test version \n",
    "bc_trainer.train(n_epochs=5, log_interval=100000) # Fianl version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-10000p-60m_unbalanced.csv')\n",
    "eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-10000p-60m_unbalanced.csv')\n",
    "eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-10000p-60m_unbalanced.csv')\n",
    "\n",
    "# Test version\n",
    "# eval_callback1 = EvalCallback(eval_env1, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "# eval_callback2 = EvalCallback(eval_env2, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "# eval_callback3 = EvalCallback(eval_env3, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "\n",
    "# Final verison\n",
    "eval_callback1 = EvalCallback(eval_env1, eval_freq=10000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rur\")\n",
    "eval_callback2 = EvalCallback(eval_env2, eval_freq=10000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rbd1\")\n",
    "eval_callback3 = EvalCallback(eval_env3, eval_freq=10000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rbd2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model1 = DQN('MlpPolicy', env, verbose=1)\n",
    "model2 = DQN('MlpPolicy', env, verbose=1)\n",
    "model3 = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "model1.policy = bc_trainer.policy\n",
    "model2.policy = bc_trainer.policy\n",
    "model3.policy = bc_trainer.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalnoma(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0456   |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.053    |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0528   |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0264   |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.045    |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0242   |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.22e+03 |\n",
      "|    ep_rew_mean      | -625     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 50       |\n",
      "|    fps              | 674      |\n",
      "|    time_elapsed     | 164      |\n",
      "|    total_timesteps  | 110944   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 15235    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0235   |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0414   |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0157   |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0305   |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.029    |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0364   |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0317   |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0225   |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0245   |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 210000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0217   |\n",
      "|    n_updates        | 39999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+03 |\n",
      "|    ep_rew_mean      | -461     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 582      |\n",
      "|    time_elapsed     | 374      |\n",
      "|    total_timesteps  | 218447   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0394   |\n",
      "|    n_updates        | 42111    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0274   |\n",
      "|    n_updates        | 42499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0341   |\n",
      "|    n_updates        | 44999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0219   |\n",
      "|    n_updates        | 47499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0322   |\n",
      "|    n_updates        | 49999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0244   |\n",
      "|    n_updates        | 52499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 270000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 54999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 280000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0197   |\n",
      "|    n_updates        | 57499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 290000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.034    |\n",
      "|    n_updates        | 59999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0278   |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 310000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.02     |\n",
      "|    n_updates        | 64999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 320000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 67499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.15e+03 |\n",
      "|    ep_rew_mean      | -295     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 150      |\n",
      "|    fps              | 545      |\n",
      "|    time_elapsed     | 596      |\n",
      "|    total_timesteps  | 325778   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0376   |\n",
      "|    n_updates        | 68944    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 330000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0117   |\n",
      "|    n_updates        | 69999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 340000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0214   |\n",
      "|    n_updates        | 72499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 350000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0262   |\n",
      "|    n_updates        | 74999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 360000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0166   |\n",
      "|    n_updates        | 77499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 370000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0214   |\n",
      "|    n_updates        | 79999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 380000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 82499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 390000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0129   |\n",
      "|    n_updates        | 84999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0316   |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 410000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0196   |\n",
      "|    n_updates        | 89999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 420000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.027    |\n",
      "|    n_updates        | 92499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=1426.30 +/- 0.00\n",
      "Episode length: 1750.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.75e+03 |\n",
      "|    mean_reward      | 1.43e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 430000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00831  |\n",
      "|    n_updates        | 94999    |\n",
      "----------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test version\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# model1.learn(total_timesteps=5e5, \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#             log_interval=50, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m# Final version\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model1\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e6\u001b[39;49m, \n\u001b[1;32m     10\u001b[0m             log_interval\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[1;32m     11\u001b[0m             \u001b[39m# progress_bar=True, \u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m             callback\u001b[39m=\u001b[39;49meval_callback1\n\u001b[1;32m     13\u001b[0m            )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py:269\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[1;32m    262\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    268\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    270\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    271\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    272\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    273\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    274\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    275\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:330\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    328\u001b[0m         \u001b[39m# Special case when the user passes `gradient_steps=0`\u001b[39;00m\n\u001b[1;32m    329\u001b[0m         \u001b[39mif\u001b[39;00m gradient_steps \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m--> 330\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain(batch_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatch_size, gradient_steps\u001b[39m=\u001b[39;49mgradient_steps)\n\u001b[1;32m    332\u001b[0m callback\u001b[39m.\u001b[39mon_training_end()\n\u001b[1;32m    334\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py:208\u001b[0m, in \u001b[0;36mDQN.train\u001b[0;34m(self, gradient_steps, batch_size)\u001b[0m\n\u001b[1;32m    205\u001b[0m     target_q_values \u001b[39m=\u001b[39m replay_data\u001b[39m.\u001b[39mrewards \u001b[39m+\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m replay_data\u001b[39m.\u001b[39mdones) \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgamma \u001b[39m*\u001b[39m next_q_values\n\u001b[1;32m    207\u001b[0m \u001b[39m# Get current Q-values estimates\u001b[39;00m\n\u001b[0;32m--> 208\u001b[0m current_q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_net(replay_data\u001b[39m.\u001b[39;49mobservations)\n\u001b[1;32m    210\u001b[0m \u001b[39m# Retrieve the q-values for the actions from the replay buffer\u001b[39;00m\n\u001b[1;32m    211\u001b[0m current_q_values \u001b[39m=\u001b[39m th\u001b[39m.\u001b[39mgather(current_q_values, dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, index\u001b[39m=\u001b[39mreplay_data\u001b[39m.\u001b[39mactions\u001b[39m.\u001b[39mlong())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/dqn/policies.py:66\u001b[0m, in \u001b[0;36mQNetwork.forward\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[39m# For type checker:\u001b[39;00m\n\u001b[1;32m     65\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeatures_extractor, BaseFeaturesExtractor)\n\u001b[0;32m---> 66\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mq_net(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mextract_features(obs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeatures_extractor))\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test version\n",
    "# model1.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model1.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback1\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0118   |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.22e+03 |\n",
      "|    ep_rew_mean      | -626     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 50       |\n",
      "|    fps              | 979      |\n",
      "|    time_elapsed     | 113      |\n",
      "|    total_timesteps  | 110940   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.046    |\n",
      "|    n_updates        | 15234    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+03 |\n",
      "|    ep_rew_mean      | -456     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 796      |\n",
      "|    time_elapsed     | 273      |\n",
      "|    total_timesteps  | 217568   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.027    |\n",
      "|    n_updates        | 41891    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0251   |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.13e+03 |\n",
      "|    ep_rew_mean      | -287     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 150      |\n",
      "|    fps              | 755      |\n",
      "|    time_elapsed     | 428      |\n",
      "|    total_timesteps  | 324171   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 68542    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0324   |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -285     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 762      |\n",
      "|    time_elapsed     | 565      |\n",
      "|    total_timesteps  | 431281   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 95320    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -283     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 250      |\n",
      "|    fps              | 772      |\n",
      "|    time_elapsed     | 696      |\n",
      "|    total_timesteps  | 538225   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00415  |\n",
      "|    n_updates        | 122056   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 137499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -287     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 773      |\n",
      "|    time_elapsed     | 833      |\n",
      "|    total_timesteps  | 644945   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00859  |\n",
      "|    n_updates        | 148736   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0213   |\n",
      "|    n_updates        | 162499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -288     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 350      |\n",
      "|    fps              | 765      |\n",
      "|    time_elapsed     | 983      |\n",
      "|    total_timesteps  | 752078   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 175519   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 187499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -287     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 749      |\n",
      "|    time_elapsed     | 1146     |\n",
      "|    total_timesteps  | 859308   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0196   |\n",
      "|    n_updates        | 202326   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00713  |\n",
      "|    n_updates        | 212499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -287     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 450      |\n",
      "|    fps              | 757      |\n",
      "|    time_elapsed     | 1276     |\n",
      "|    total_timesteps  | 966426   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00807  |\n",
      "|    n_updates        | 229106   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.01     |\n",
      "|    n_updates        | 237499   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fbcfb2e6cd0>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test version\n",
    "# model2.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model2.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0443   |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.21e+03 |\n",
      "|    ep_rew_mean      | -618     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 50       |\n",
      "|    fps              | 1342     |\n",
      "|    time_elapsed     | 82       |\n",
      "|    total_timesteps  | 110474   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0357   |\n",
      "|    n_updates        | 15118    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0358   |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.17e+03 |\n",
      "|    ep_rew_mean      | -452     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 1009     |\n",
      "|    time_elapsed     | 215      |\n",
      "|    total_timesteps  | 217261   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0287   |\n",
      "|    n_updates        | 41815    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0246   |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.13e+03 |\n",
      "|    ep_rew_mean      | -286     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 150      |\n",
      "|    fps              | 836      |\n",
      "|    time_elapsed     | 387      |\n",
      "|    total_timesteps  | 323775   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0136   |\n",
      "|    n_updates        | 68443    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00866  |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -288     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 819      |\n",
      "|    time_elapsed     | 526      |\n",
      "|    total_timesteps  | 431227   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0287   |\n",
      "|    n_updates        | 95306    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0124   |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.15e+03 |\n",
      "|    ep_rew_mean      | -289     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 250      |\n",
      "|    fps              | 809      |\n",
      "|    time_elapsed     | 664      |\n",
      "|    total_timesteps  | 538470   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0145   |\n",
      "|    n_updates        | 122117   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0127   |\n",
      "|    n_updates        | 137499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -288     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 797      |\n",
      "|    time_elapsed     | 808      |\n",
      "|    total_timesteps  | 645530   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0167   |\n",
      "|    n_updates        | 148882   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0229   |\n",
      "|    n_updates        | 162499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -288     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 350      |\n",
      "|    fps              | 802      |\n",
      "|    time_elapsed     | 938      |\n",
      "|    total_timesteps  | 752902   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0182   |\n",
      "|    n_updates        | 175725   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0178   |\n",
      "|    n_updates        | 187499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -288     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 808      |\n",
      "|    time_elapsed     | 1063     |\n",
      "|    total_timesteps  | 859895   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00942  |\n",
      "|    n_updates        | 202473   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.015    |\n",
      "|    n_updates        | 212499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -287     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 450      |\n",
      "|    fps              | 816      |\n",
      "|    time_elapsed     | 1184     |\n",
      "|    total_timesteps  | 966975   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0067   |\n",
      "|    n_updates        | 229243   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0138   |\n",
      "|    n_updates        | 237499   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fbcfb2e9bb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test version\n",
    "# model3.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model3.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback3\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
