{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.data.types import Transitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test version\n",
    "# data = np.genfromtxt('../dataset/data_expert.csv', delimiter=',', max_rows=1000000) \n",
    "\n",
    "# Final version\n",
    "data = np.genfromtxt('../dataset/data_expert.csv', delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into obs, acts, infos, next_obs, and dones\n",
    "obs = data[:, :12]\n",
    "acts = data[:, 12]\n",
    "infos = np.empty(len(data), dtype=dict)\n",
    "next_obs = data[:, 13:25]\n",
    "dones = data[:, 25].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = Transitions(obs, acts, infos, next_obs, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs': array([0.91, 0.85, 0.73, 0.76, 0.83, 0.75, 0.82, 0.94, 0.76, 0.84, 0.05,\n",
       "        0.14]),\n",
       " 'acts': 1.0,\n",
       " 'infos': None,\n",
       " 'next_obs': array([0.86, 0.71, 0.73, 0.76, 0.83, 0.75, 0.82, 0.94, 0.76, 0.84, 0.04,\n",
       "        0.06]),\n",
       " 'dones': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions[13]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert from newly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_expert():\n",
    "#     print(\"Training a expert.\")\n",
    "\n",
    "#     expert = DQN(\n",
    "#         policy='MlpPolicy',\n",
    "#         env=env\n",
    "#     )\n",
    "\n",
    "#     expert.learn(10000)  # Note: change this to 100000 to train a decent expert.\n",
    "#     return expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_expert_transitions():\n",
    "#     expert = train_expert()\n",
    "\n",
    "#     print(\"Sampling expert transitions.\")\n",
    "#     rollouts = rollout.rollout(\n",
    "#         expert,\n",
    "#         DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n",
    "#         rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "#         rng=rng,\n",
    "#     )\n",
    "#     return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "def eval_unit(model, env, num_eval_episodes=2):\n",
    "    rews, lens = evaluate_policy(model, env, n_eval_episodes=num_eval_episodes, return_episode_rewards=True)\n",
    "    # Takes the mean of rews elements divided by lens elements\n",
    "    mean_rew = np.mean([rew / length for rew, length in zip(rews, lens)]).round(2)\n",
    "\n",
    "    # Takes the difference between the maximum and minimum of reward elements\n",
    "    std_rew = (np.max(rews) - np.min(rews)) / 2\n",
    "\n",
    "    mean_rew = round(mean_rew, 2)\n",
    "    std_rew = round(std_rew, 2)\n",
    "\n",
    "    return (mean_rew, std_rew)\n",
    "\n",
    "\n",
    "def eval_set(model):\n",
    "    eval_rur_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_rur_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "    eval_rur_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "    eval_rbd1_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_rbd1_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "    eval_rbd1_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "    eval_rbd2_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_rbd2_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "    eval_rbd2_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "    rur1 = eval_unit(model, eval_rur_env1, num_eval_episodes=2)\n",
    "    rur2 = eval_unit(model, eval_rur_env2, num_eval_episodes=2)\n",
    "    rur3 = eval_unit(model, eval_rur_env3, num_eval_episodes=2)\n",
    "\n",
    "    print(rur1, rur2, rur3)\n",
    "\n",
    "    rbd11 = eval_unit(model, eval_rbd1_env1, num_eval_episodes=2)\n",
    "    rbd12 = eval_unit(model, eval_rbd1_env2, num_eval_episodes=2)\n",
    "    rbd13 = eval_unit(model, eval_rbd1_env3, num_eval_episodes=2)\n",
    "\n",
    "    print(rbd11, rbd12, rbd13)\n",
    "\n",
    "    rbd21 = eval_unit(model, eval_rbd2_env1, num_eval_episodes=2)\n",
    "    rbd22 = eval_unit(model, eval_rbd2_env2, num_eval_episodes=2)\n",
    "    rbd23 = eval_unit(model, eval_rbd2_env3, num_eval_episodes=2)\n",
    "\n",
    "    print(rbd21, rbd22, rbd23)\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment without rendering\n",
    "env = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_3.py')\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'eval_set' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m eval_set(bc_trainer\u001b[39m.\u001b[39mpolicy)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'eval_set' is not defined"
     ]
    }
   ],
   "source": [
    "eval_set(bc_trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00179 |\n",
      "|    entropy        | 1.79     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 88.5     |\n",
      "|    loss           | 1.79     |\n",
      "|    neglogp        | 1.79     |\n",
      "|    prob_true_act  | 0.167    |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99976batch [06:43, 243.57batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 100000    |\n",
      "|    ent_loss       | -0.000157 |\n",
      "|    entropy        | 0.157     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 1.43e+03  |\n",
      "|    loss           | 0.058     |\n",
      "|    neglogp        | 0.0581    |\n",
      "|    prob_true_act  | 0.948     |\n",
      "|    samples_so_far | 3200032   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199987batch [13:12, 197.03batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 200000    |\n",
      "|    ent_loss       | -4.35e-05 |\n",
      "|    entropy        | 0.0435    |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 2.19e+03  |\n",
      "|    loss           | 0.0352    |\n",
      "|    neglogp        | 0.0353    |\n",
      "|    prob_true_act  | 0.976     |\n",
      "|    samples_so_far | 6400032   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299984batch [19:44, 286.94batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 300000    |\n",
      "|    ent_loss       | -0.000223 |\n",
      "|    entropy        | 0.223     |\n",
      "|    epoch          | 2         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 2.87e+03  |\n",
      "|    loss           | 0.144     |\n",
      "|    neglogp        | 0.144     |\n",
      "|    prob_true_act  | 0.892     |\n",
      "|    samples_so_far | 9600032   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399996batch [26:22, 268.73batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 400000    |\n",
      "|    ent_loss       | -0.000119 |\n",
      "|    entropy        | 0.119     |\n",
      "|    epoch          | 3         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 3.52e+03  |\n",
      "|    loss           | 0.104     |\n",
      "|    neglogp        | 0.104     |\n",
      "|    prob_true_act  | 0.936     |\n",
      "|    samples_so_far | 12800032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "499992batch [34:07, 101.02batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500000    |\n",
      "|    ent_loss       | -4.93e-05 |\n",
      "|    entropy        | 0.0493    |\n",
      "|    epoch          | 4         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 4.14e+03  |\n",
      "|    loss           | 0.0154    |\n",
      "|    neglogp        | 0.0155    |\n",
      "|    prob_true_act  | 0.986     |\n",
      "|    samples_so_far | 16000032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599977batch [42:58, 199.69batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 600000   |\n",
      "|    ent_loss       | -0.0001  |\n",
      "|    entropy        | 0.1      |\n",
      "|    epoch          | 5        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 4.72e+03 |\n",
      "|    loss           | 0.0939   |\n",
      "|    neglogp        | 0.094    |\n",
      "|    prob_true_act  | 0.943    |\n",
      "|    samples_so_far | 19200032 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "699993batch [53:16, 133.26batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 700000    |\n",
      "|    ent_loss       | -3.47e-05 |\n",
      "|    entropy        | 0.0347    |\n",
      "|    epoch          | 6         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 5.26e+03  |\n",
      "|    loss           | 0.0427    |\n",
      "|    neglogp        | 0.0427    |\n",
      "|    prob_true_act  | 0.974     |\n",
      "|    samples_so_far | 22400032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "799996batch [1:02:50, 221.54batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 800000    |\n",
      "|    ent_loss       | -4.55e-05 |\n",
      "|    entropy        | 0.0455    |\n",
      "|    epoch          | 7         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 5.77e+03  |\n",
      "|    loss           | 0.0163    |\n",
      "|    neglogp        | 0.0164    |\n",
      "|    prob_true_act  | 0.985     |\n",
      "|    samples_so_far | 25600032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "899994batch [1:12:35, 175.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 900000    |\n",
      "|    ent_loss       | -0.000142 |\n",
      "|    entropy        | 0.142     |\n",
      "|    epoch          | 8         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 6.23e+03  |\n",
      "|    loss           | 0.0886    |\n",
      "|    neglogp        | 0.0888    |\n",
      "|    prob_true_act  | 0.935     |\n",
      "|    samples_so_far | 28800032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999996batch [1:23:22, 136.59batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000000   |\n",
      "|    ent_loss       | -4.38e-05 |\n",
      "|    entropy        | 0.0438    |\n",
      "|    epoch          | 9         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 6.66e+03  |\n",
      "|    loss           | 0.0473    |\n",
      "|    neglogp        | 0.0474    |\n",
      "|    prob_true_act  | 0.969     |\n",
      "|    samples_so_far | 32000032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1071450batch [1:28:45, 201.18batch/s]\n"
     ]
    }
   ],
   "source": [
    "# bc_trainer.train(log_interval=100000, n_batches=100000) # Test version \n",
    "bc_trainer.train(n_epochs=10, log_interval=100000) # Fianl version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.88, 0.0) (0.86, 0.0) (0.91, 0.0)\n",
      "(0.9, 0.0) (0.87, 0.0) (0.93, 0.0)\n",
      "(0.94, 0.0) (0.93, 0.0) (0.96, 0.0)\n"
     ]
    }
   ],
   "source": [
    "eval_set(bc_trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "# Test version\n",
    "# eval_callback1 = EvalCallback(eval_env1, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "# eval_callback2 = EvalCallback(eval_env2, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "# eval_callback3 = EvalCallback(eval_env3, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "\n",
    "# Final verison\n",
    "eval_callback1 = EvalCallback(eval_env1, eval_freq=10000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rur\")\n",
    "eval_callback2 = EvalCallback(eval_env2, eval_freq=10000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rbd1\")\n",
    "eval_callback3 = EvalCallback(eval_env3, eval_freq=10000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rbd2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model1 = DQN('MlpPolicy', env, verbose=1)\n",
    "model2 = DQN('MlpPolicy', env, verbose=1)\n",
    "model3 = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "model1.policy = bc_trainer.policy\n",
    "model2.policy = bc_trainer.policy\n",
    "model3.policy = bc_trainer.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=100000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0417   |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.22e+03 |\n",
      "|    ep_rew_mean      | -620     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 50       |\n",
      "|    fps              | 1201     |\n",
      "|    time_elapsed     | 92       |\n",
      "|    total_timesteps  | 111039   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0336   |\n",
      "|    n_updates        | 15259    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0194   |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.18e+03 |\n",
      "|    ep_rew_mean      | -454     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 888      |\n",
      "|    time_elapsed     | 245      |\n",
      "|    total_timesteps  | 218435   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0304   |\n",
      "|    n_updates        | 42108    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0209   |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -287     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 150      |\n",
      "|    fps              | 843      |\n",
      "|    time_elapsed     | 385      |\n",
      "|    total_timesteps  | 324993   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0215   |\n",
      "|    n_updates        | 68748    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0159   |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.13e+03 |\n",
      "|    ep_rew_mean      | -286     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 789      |\n",
      "|    time_elapsed     | 546      |\n",
      "|    total_timesteps  | 431841   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0145   |\n",
      "|    n_updates        | 95460    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0045   |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -286     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 250      |\n",
      "|    fps              | 794      |\n",
      "|    time_elapsed     | 678      |\n",
      "|    total_timesteps  | 539033   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0123   |\n",
      "|    n_updates        | 122258   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0184   |\n",
      "|    n_updates        | 137499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -286     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 300      |\n",
      "|    fps              | 791      |\n",
      "|    time_elapsed     | 815      |\n",
      "|    total_timesteps  | 645736   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0169   |\n",
      "|    n_updates        | 148933   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0152   |\n",
      "|    n_updates        | 162499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -286     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 350      |\n",
      "|    fps              | 803      |\n",
      "|    time_elapsed     | 936      |\n",
      "|    total_timesteps  | 752853   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 175713   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0156   |\n",
      "|    n_updates        | 187499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -284     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 400      |\n",
      "|    fps              | 803      |\n",
      "|    time_elapsed     | 1069     |\n",
      "|    total_timesteps  | 859451   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00576  |\n",
      "|    n_updates        | 202362   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0141   |\n",
      "|    n_updates        | 212499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 2.14e+03 |\n",
      "|    ep_rew_mean      | -285     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 450      |\n",
      "|    fps              | 792      |\n",
      "|    time_elapsed     | 1220     |\n",
      "|    total_timesteps  | 966564   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0134   |\n",
      "|    n_updates        | 229140   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=1423.02 +/- 0.00\n",
      "Episode length: 1609.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.42e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0117   |\n",
      "|    n_updates        | 237499   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fbcfb2e66d0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test version\n",
    "# model1.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model1.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback1\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swkim/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.128    |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0935   |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.063    |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.065    |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.108    |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0826   |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0722   |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0755   |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0549   |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0682   |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0384   |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0606   |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0308   |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0758   |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0514   |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 210000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0401   |\n",
      "|    n_updates        | 39999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0478   |\n",
      "|    n_updates        | 42499    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.54e+03  |\n",
      "|    ep_rew_mean      | -1.17e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 50        |\n",
      "|    fps              | 543       |\n",
      "|    time_elapsed     | 417       |\n",
      "|    total_timesteps  | 226908    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0385    |\n",
      "|    n_updates        | 44226     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0596   |\n",
      "|    n_updates        | 44999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0446   |\n",
      "|    n_updates        | 47499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0688   |\n",
      "|    n_updates        | 49999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0511   |\n",
      "|    n_updates        | 52499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 270000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0265   |\n",
      "|    n_updates        | 54999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 280000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0393   |\n",
      "|    n_updates        | 57499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 290000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0817   |\n",
      "|    n_updates        | 59999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0397   |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 310000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0533   |\n",
      "|    n_updates        | 64999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 320000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0165   |\n",
      "|    n_updates        | 67499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 330000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0508   |\n",
      "|    n_updates        | 69999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 340000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0359   |\n",
      "|    n_updates        | 72499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 350000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0246   |\n",
      "|    n_updates        | 74999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 360000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0438   |\n",
      "|    n_updates        | 77499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 370000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 79999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 380000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0305   |\n",
      "|    n_updates        | 82499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 390000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0239   |\n",
      "|    n_updates        | 84999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00862  |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 410000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0453   |\n",
      "|    n_updates        | 89999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 420000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0411   |\n",
      "|    n_updates        | 92499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 430000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0152   |\n",
      "|    n_updates        | 94999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 440000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0128   |\n",
      "|    n_updates        | 97499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 450000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0488   |\n",
      "|    n_updates        | 99999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.52e+03 |\n",
      "|    ep_rew_mean      | -937     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 491      |\n",
      "|    time_elapsed     | 918      |\n",
      "|    total_timesteps  | 452067   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0345   |\n",
      "|    n_updates        | 100516   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 460000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0567   |\n",
      "|    n_updates        | 102499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 470000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0282   |\n",
      "|    n_updates        | 104999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 480000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0255   |\n",
      "|    n_updates        | 107499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 490000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0105   |\n",
      "|    n_updates        | 109999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0455   |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 510000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0292   |\n",
      "|    n_updates        | 114999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 520000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.03     |\n",
      "|    n_updates        | 117499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 530000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.028    |\n",
      "|    n_updates        | 119999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 540000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0302   |\n",
      "|    n_updates        | 122499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 550000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0252   |\n",
      "|    n_updates        | 124999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 560000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0125   |\n",
      "|    n_updates        | 127499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 570000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0426   |\n",
      "|    n_updates        | 129999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 580000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0345   |\n",
      "|    n_updates        | 132499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 590000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.012    |\n",
      "|    n_updates        | 134999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0144   |\n",
      "|    n_updates        | 137499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 610000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0315   |\n",
      "|    n_updates        | 139999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 620000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0163   |\n",
      "|    n_updates        | 142499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 630000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0191   |\n",
      "|    n_updates        | 144999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 640000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0274   |\n",
      "|    n_updates        | 147499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 650000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0291   |\n",
      "|    n_updates        | 149999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 660000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0247   |\n",
      "|    n_updates        | 152499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 670000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00817  |\n",
      "|    n_updates        | 154999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5e+03  |\n",
      "|    ep_rew_mean      | -703     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 150      |\n",
      "|    fps              | 484      |\n",
      "|    time_elapsed     | 1397     |\n",
      "|    total_timesteps  | 676592   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0209   |\n",
      "|    n_updates        | 156647   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 680000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0231   |\n",
      "|    n_updates        | 157499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 690000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0501   |\n",
      "|    n_updates        | 159999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0326   |\n",
      "|    n_updates        | 162499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 710000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0389   |\n",
      "|    n_updates        | 164999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 720000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0311   |\n",
      "|    n_updates        | 167499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 730000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0291   |\n",
      "|    n_updates        | 169999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 740000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0501   |\n",
      "|    n_updates        | 172499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 750000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0189   |\n",
      "|    n_updates        | 174999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 760000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0178   |\n",
      "|    n_updates        | 177499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 770000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0161   |\n",
      "|    n_updates        | 179999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 780000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0219   |\n",
      "|    n_updates        | 182499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 790000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0151   |\n",
      "|    n_updates        | 184999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00761  |\n",
      "|    n_updates        | 187499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 810000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0306   |\n",
      "|    n_updates        | 189999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 820000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0126   |\n",
      "|    n_updates        | 192499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 830000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0146   |\n",
      "|    n_updates        | 194999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 840000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00738  |\n",
      "|    n_updates        | 197499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 850000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0164   |\n",
      "|    n_updates        | 199999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 860000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.019    |\n",
      "|    n_updates        | 202499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 870000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00806  |\n",
      "|    n_updates        | 204999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 880000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0196   |\n",
      "|    n_updates        | 207499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 890000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00778  |\n",
      "|    n_updates        | 209999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0297   |\n",
      "|    n_updates        | 212499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.48e+03 |\n",
      "|    ep_rew_mean      | -708     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 498      |\n",
      "|    time_elapsed     | 1806     |\n",
      "|    total_timesteps  | 900535   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0232   |\n",
      "|    n_updates        | 212633   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 910000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0183   |\n",
      "|    n_updates        | 214999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 920000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0286   |\n",
      "|    n_updates        | 217499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 930000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0295   |\n",
      "|    n_updates        | 219999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 940000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00491  |\n",
      "|    n_updates        | 222499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 950000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00971  |\n",
      "|    n_updates        | 224999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 960000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0306   |\n",
      "|    n_updates        | 227499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 970000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0212   |\n",
      "|    n_updates        | 229999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 980000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0473   |\n",
      "|    n_updates        | 232499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 990000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0156   |\n",
      "|    n_updates        | 234999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=1458.67 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.46e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 237499   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fbe6512ff40>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test version\n",
    "# model2.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model2.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval num_timesteps=10000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.905    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 10000    |\n",
      "----------------------------------\n",
      "New best mean reward!\n",
      "Eval num_timesteps=20000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.81     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 20000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=30000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.715    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 30000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=40000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.62     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 40000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=50000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.525    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 50000    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=60000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.43     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 60000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.124    |\n",
      "|    n_updates        | 2499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=70000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.335    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 70000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.121    |\n",
      "|    n_updates        | 4999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=80000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.24     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 80000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0807   |\n",
      "|    n_updates        | 7499     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=90000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.145    |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 90000    |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0824   |\n",
      "|    n_updates        | 9999     |\n",
      "----------------------------------\n",
      "Eval num_timesteps=100000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 100000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0783   |\n",
      "|    n_updates        | 12499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=110000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 110000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0917   |\n",
      "|    n_updates        | 14999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=120000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 120000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0782   |\n",
      "|    n_updates        | 17499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=130000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 130000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0452   |\n",
      "|    n_updates        | 19999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=140000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 140000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0354   |\n",
      "|    n_updates        | 22499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=150000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 150000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0683   |\n",
      "|    n_updates        | 24999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=160000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 160000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0721   |\n",
      "|    n_updates        | 27499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=170000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 170000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0258   |\n",
      "|    n_updates        | 29999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=180000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 180000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0505   |\n",
      "|    n_updates        | 32499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=190000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 190000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.057    |\n",
      "|    n_updates        | 34999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=200000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 200000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0555   |\n",
      "|    n_updates        | 37499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=210000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 210000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0315   |\n",
      "|    n_updates        | 39999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=220000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 220000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.058    |\n",
      "|    n_updates        | 42499    |\n",
      "----------------------------------\n",
      "-----------------------------------\n",
      "| rollout/            |           |\n",
      "|    ep_len_mean      | 4.57e+03  |\n",
      "|    ep_rew_mean      | -1.19e+03 |\n",
      "|    exploration_rate | 0.05      |\n",
      "| time/               |           |\n",
      "|    episodes         | 50        |\n",
      "|    fps              | 664       |\n",
      "|    time_elapsed     | 344       |\n",
      "|    total_timesteps  | 228718    |\n",
      "| train/              |           |\n",
      "|    learning_rate    | 0.0001    |\n",
      "|    loss             | 0.0401    |\n",
      "|    n_updates        | 44679     |\n",
      "-----------------------------------\n",
      "Eval num_timesteps=230000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 230000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.032    |\n",
      "|    n_updates        | 44999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=240000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 240000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0368   |\n",
      "|    n_updates        | 47499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 250000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0378   |\n",
      "|    n_updates        | 49999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=260000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 260000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0274   |\n",
      "|    n_updates        | 52499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=270000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 270000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0305   |\n",
      "|    n_updates        | 54999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=280000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 280000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0287   |\n",
      "|    n_updates        | 57499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=290000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 290000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0258   |\n",
      "|    n_updates        | 59999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=300000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 300000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0207   |\n",
      "|    n_updates        | 62499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=310000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 310000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 64999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=320000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 320000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0358   |\n",
      "|    n_updates        | 67499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=330000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 330000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0591   |\n",
      "|    n_updates        | 69999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=340000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 340000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.021    |\n",
      "|    n_updates        | 72499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=350000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 350000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.034    |\n",
      "|    n_updates        | 74999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=360000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 360000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0305   |\n",
      "|    n_updates        | 77499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=370000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 370000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.03     |\n",
      "|    n_updates        | 79999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=380000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 380000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0236   |\n",
      "|    n_updates        | 82499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=390000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 390000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0357   |\n",
      "|    n_updates        | 84999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=400000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 400000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0205   |\n",
      "|    n_updates        | 87499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=410000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 410000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0171   |\n",
      "|    n_updates        | 89999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=420000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 420000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0157   |\n",
      "|    n_updates        | 92499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=430000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 430000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0427   |\n",
      "|    n_updates        | 94999    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=440000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 440000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0186   |\n",
      "|    n_updates        | 97499    |\n",
      "----------------------------------\n",
      "Eval num_timesteps=450000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 450000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00973  |\n",
      "|    n_updates        | 99999    |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.53e+03 |\n",
      "|    ep_rew_mean      | -949     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 100      |\n",
      "|    fps              | 605      |\n",
      "|    time_elapsed     | 748      |\n",
      "|    total_timesteps  | 453389   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0255   |\n",
      "|    n_updates        | 100847   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=460000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 460000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0149   |\n",
      "|    n_updates        | 102499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=470000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 470000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0473   |\n",
      "|    n_updates        | 104999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=480000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 480000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0209   |\n",
      "|    n_updates        | 107499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=490000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 490000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0296   |\n",
      "|    n_updates        | 109999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 500000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0369   |\n",
      "|    n_updates        | 112499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=510000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 510000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00895  |\n",
      "|    n_updates        | 114999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=520000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 520000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0229   |\n",
      "|    n_updates        | 117499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=530000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 530000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0293   |\n",
      "|    n_updates        | 119999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=540000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 540000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0398   |\n",
      "|    n_updates        | 122499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=550000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 550000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0291   |\n",
      "|    n_updates        | 124999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=560000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 560000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00825  |\n",
      "|    n_updates        | 127499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=570000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 570000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0528   |\n",
      "|    n_updates        | 129999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=580000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 580000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0157   |\n",
      "|    n_updates        | 132499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=590000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 590000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00939  |\n",
      "|    n_updates        | 134999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=600000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 600000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0392   |\n",
      "|    n_updates        | 137499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=610000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 610000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0296   |\n",
      "|    n_updates        | 139999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=620000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 620000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0207   |\n",
      "|    n_updates        | 142499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=630000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 630000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0187   |\n",
      "|    n_updates        | 144999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=640000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 640000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0372   |\n",
      "|    n_updates        | 147499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=650000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 650000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0302   |\n",
      "|    n_updates        | 149999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=660000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 660000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.026    |\n",
      "|    n_updates        | 152499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=670000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 670000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0158   |\n",
      "|    n_updates        | 154999   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5e+03  |\n",
      "|    ep_rew_mean      | -706     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 150      |\n",
      "|    fps              | 588      |\n",
      "|    time_elapsed     | 1153     |\n",
      "|    total_timesteps  | 678612   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00682  |\n",
      "|    n_updates        | 157152   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=680000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 680000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0263   |\n",
      "|    n_updates        | 157499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=690000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 690000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0288   |\n",
      "|    n_updates        | 159999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=700000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 700000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0488   |\n",
      "|    n_updates        | 162499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=710000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 710000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0305   |\n",
      "|    n_updates        | 164999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=720000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 720000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.026    |\n",
      "|    n_updates        | 167499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=730000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 730000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0298   |\n",
      "|    n_updates        | 169999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=740000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 740000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0514   |\n",
      "|    n_updates        | 172499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 750000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00625  |\n",
      "|    n_updates        | 174999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=760000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 760000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0225   |\n",
      "|    n_updates        | 177499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=770000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 770000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00668  |\n",
      "|    n_updates        | 179999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=780000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 780000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0292   |\n",
      "|    n_updates        | 182499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=790000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 790000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0316   |\n",
      "|    n_updates        | 184999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=800000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 800000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0137   |\n",
      "|    n_updates        | 187499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=810000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 810000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0145   |\n",
      "|    n_updates        | 189999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=820000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 820000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0404   |\n",
      "|    n_updates        | 192499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=830000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 830000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0356   |\n",
      "|    n_updates        | 194999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=840000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 840000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00941  |\n",
      "|    n_updates        | 197499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=850000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 850000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0446   |\n",
      "|    n_updates        | 199999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=860000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 860000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0201   |\n",
      "|    n_updates        | 202499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=870000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 870000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0145   |\n",
      "|    n_updates        | 204999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=880000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 880000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0122   |\n",
      "|    n_updates        | 207499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=890000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 890000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0257   |\n",
      "|    n_updates        | 209999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=900000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 900000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0119   |\n",
      "|    n_updates        | 212499   |\n",
      "----------------------------------\n",
      "----------------------------------\n",
      "| rollout/            |          |\n",
      "|    ep_len_mean      | 4.5e+03  |\n",
      "|    ep_rew_mean      | -706     |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    episodes         | 200      |\n",
      "|    fps              | 580      |\n",
      "|    time_elapsed     | 1556     |\n",
      "|    total_timesteps  | 903866   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0249   |\n",
      "|    n_updates        | 213466   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=910000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 910000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0153   |\n",
      "|    n_updates        | 214999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=920000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 920000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0398   |\n",
      "|    n_updates        | 217499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=930000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 930000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.018    |\n",
      "|    n_updates        | 219999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=940000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 940000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.00425  |\n",
      "|    n_updates        | 222499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=950000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 950000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0463   |\n",
      "|    n_updates        | 224999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=960000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 960000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0253   |\n",
      "|    n_updates        | 227499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=970000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 970000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0211   |\n",
      "|    n_updates        | 229999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=980000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 980000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0309   |\n",
      "|    n_updates        | 232499   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=990000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 990000   |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0194   |\n",
      "|    n_updates        | 234999   |\n",
      "----------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=1527.46 +/- 0.00\n",
      "Episode length: 1606.00 +/- 0.00\n",
      "----------------------------------\n",
      "| eval/               |          |\n",
      "|    mean_ep_length   | 1.61e+03 |\n",
      "|    mean_reward      | 1.53e+03 |\n",
      "| rollout/            |          |\n",
      "|    exploration_rate | 0.05     |\n",
      "| time/               |          |\n",
      "|    total_timesteps  | 1000000  |\n",
      "| train/              |          |\n",
      "|    learning_rate    | 0.0001   |\n",
      "|    loss             | 0.0293   |\n",
      "|    n_updates        | 237499   |\n",
      "----------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.dqn.dqn.DQN at 0x7fbe65142640>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test version\n",
    "# model3.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model3.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback3\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rur_default = [0.88, 0.86, 0.87, 0.87, 0.89, 0.88, 0.88, 0.86, 0.86, 0.87]\n",
    "rur_random = [0.81, 0.82, 0.84, 0.79, 0.81, 0.83, 0.78, 0.83, 0.82, 0.84]\n",
    "rur_antcolony = [0.86, 0.85, 0.89, 0.88, 0.89, 0.90, 0.85, 0.86, 0.88, 0.90]\n",
    "\n",
    "rur_st_ut = [0.87, 0.88, 0.89, 0.89, 0.88, 0.88, 0.88, 0.89, 0.88, 0.89]\n",
    "rur_st_pt = [0.91, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.91,0.91, 0.92]\n",
    "rur_dy_ut = [0.92, 0.91, 0.91, 0.91, 0.9, 0.91, 0.9, 0.9, 0.9, 0.9]\n",
    "rur_dy_pt = [0.91, 0.92, 0.91, 0.93, 0.93, 0.93, 0.93, 0.93, 0.93, 0.92]\n",
    "\n",
    "data = [rur_default, rur_random, rur_antcolony, rur_st_ut, rur_st_pt, rur_dy_ut, rur_dy_pt]\n",
    "\n",
    "plt.style.use('_mpl-gallery')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4, 2))\n",
    "\n",
    "\n",
    "VP = ax.boxplot(data, positions=[2, 4, 6, 8, 10, 12, 14], widths=1.5, patch_artist=True,\n",
    "                showmeans=False, showfliers=False,\n",
    "                medianprops={\"color\": \"#8C8C8C\", \"linewidth\": 0.5},\n",
    "                boxprops={\"facecolor\": \"#BFBFBF\", \"edgecolor\": \"white\",\n",
    "                          \"linewidth\": 0.5},\n",
    "                whiskerprops={\"color\": \"#8C8C8C\", \"linewidth\": 1.5},\n",
    "                capprops={\"color\": \"#8C8C8C\", \"linewidth\": 1.5})\n",
    "\n",
    "# Draw horizontal line at 0.9\n",
    "ax.axhline(y=0.9, color='#8C8C8C', linestyle='--', linewidth=1)\n",
    "\n",
    "# Change x axis labels tilt\n",
    "for tick in ax.get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# Change x axis labels\n",
    "ax.set_xticklabels(['RUR', 'RBD1', 'RBD2', 'ST-UT', 'ST-PT', 'DY-UT', 'DY-PT'])\n",
    "# Set x axos fontsize smaller\n",
    "ax.tick_params(axis='x', labelsize=8)\n",
    "\n",
    "\n",
    "# Move y axis closer to plot\n",
    "ax.tick_params(axis='y', pad=0)\n",
    "\n",
    "# Make plot starts from 0 in x axis\n",
    "ax.set_xlim(left=0)\n",
    "\n",
    "# y axis round to 2 decimal places\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))\n",
    "\n",
    "# Show *100 of y axis but not show % symbol\n",
    "vals = ax.get_yticks()\n",
    "ax.set_yticklabels(['{:,.0%}'.format(x) for x in vals])\n",
    "\n",
    "# ylabel fontsize\n",
    "ax.set_ylabel('Average Reward', fontsize=8)\n",
    "\n",
    "\n",
    "# Highlight DY-PT\n",
    "VP['boxes'][6].set_facecolor('#FFC000')\n",
    "\n",
    "# Set labels\n",
    "ax.set_xlabel('Algorithm')\n",
    "ax.set_ylabel('Average Reward')\n",
    "\n",
    "ax.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = ['1', '2', '3']\n",
    "# Change all to elements in a to integer\n",
    "a = list(map(int, a))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
