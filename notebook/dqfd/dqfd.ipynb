{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/dqfd/../..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"../..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from kube_sim_gym.envs import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q Learning from Demonstrations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "\n",
    "class ExpertData:\n",
    "    def __init__(self, expert_data_path='expert_data/ed1.csv'):\n",
    "        self.expert_data_path = expert_data_path\n",
    "        self.expert_data = self.load_expert_data()\n",
    "        \n",
    "    def load_expert_data(self): # csv file\n",
    "        with open(self.expert_data_path, 'r') as f:\n",
    "            self.expert_data = np.loadtxt(f, delimiter=',')\n",
    "        return self.expert_data\n",
    "    \n",
    "    def generate_expert_data(self, reward_path, scenario_path, file_path, expert_fname):\n",
    "        env = gym.make('SimKubeEnv-v0', reward_file=reward_path, scenario_file=scenario_path)\n",
    "        state = env.reset()\n",
    "\n",
    "        from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "        expert = SimHrScheduler(env, expert_fname)\n",
    "\n",
    "        with open(file_path, 'a') as f:\n",
    "            done = False\n",
    "            while not done:\n",
    "                state = env.get_state()\n",
    "                action = expert.decision(env)\n",
    "\n",
    "                _, reward, done, info = env.step(action)\n",
    "\n",
    "                next_state = env.get_state()\n",
    "\n",
    "                expert_data = np.concatenate((state, next_state, [action,reward, done]))\n",
    "                # Round to 2 decimal places\n",
    "                expert_data = np.round(expert_data, 2)\n",
    "                f.write(','.join([str(x) for x in expert_data]) + '\\n')\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "        return self.load_expert_data()\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if self.expert_data is None:\n",
    "            self.load_expert_data()\n",
    "        idx = np.random.randint(0, len(self.expert_data), batch_size)\n",
    "        return self.expert_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_data = ExpertData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.5 ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  0.  ,  0.5 ,  0.  ],\n",
       "       [ 0.  ,  0.  ,  0.  , ...,  4.  ,  0.5 ,  0.  ],\n",
       "       ...,\n",
       "       [ 0.9 ,  0.96,  0.95, ...,  0.  ,  0.5 ,  0.  ],\n",
       "       [ 0.75,  0.82,  0.95, ...,  1.  , -0.01,  0.  ],\n",
       "       [ 0.84,  0.93,  0.95, ...,  5.  ,  0.5 ,  1.  ]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expert_data.generate_expert_data('train_dynamic.py', 'random', 'expert_data/ed1.csv', 'default.py')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Critic & Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class Critic:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        self.critic = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.state_dim + 1, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, 1),\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        q_value = self.critic(torch.cat([state, action], dim=1))\n",
    "        return q_value\n",
    "\n",
    "    def update(self, batch_state, batch_action, batch_reward, batch_done):\n",
    "\n",
    "        q_values = self.critic(batch_state, batch_action)\n",
    "        target_q_values = batch_reward + self.gamma * q_values.clone() * (1 - batch_done)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = torch.nn.MSELoss()(q_values, target_q_values)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "class Policy:\n",
    "\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "\n",
    "        self.policy = torch.nn.Sequential(\n",
    "            torch.nn.Linear(self.state_dim, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, 64),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(64, self.action_dim),\n",
    "            torch.nn.Softmax(dim=1),\n",
    "        )\n",
    "\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters())\n",
    "\n",
    "    def forward(self, state):\n",
    "        probs = self.policy(state)\n",
    "        return probs\n",
    "\n",
    "    def update(self, batch_state, batch_reward, batch_done):\n",
    "\n",
    "        probs = self.policy(batch_state)\n",
    "        log_probs = torch.log(probs)\n",
    "        entropy = -torch.sum(probs * log_probs, dim=1)\n",
    "\n",
    "        advantage = batch_reward + self.gamma * entropy * (1 - batch_done) - log_probs\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss = -torch.sum(advantage * probs, dim=1)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DQfD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class DQfD:\n",
    "\n",
    "    def __init__(self, \n",
    "                #  env,\n",
    "                 policy, \n",
    "                 critic, \n",
    "                 expert_data, \n",
    "                 batch_size, \n",
    "                 gamma, \n",
    "                 tau):\n",
    "        # self.env = env\n",
    "        self.policy = policy\n",
    "        self.critic = critic\n",
    "        self.expert_data = expert_data\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.num_iterations = 100\n",
    "\n",
    "    def learn(self):\n",
    "        for i in range(self.num_iterations):\n",
    "            # Sample a batch of transitions from the expert data.\n",
    "            # batch = self.sample_batch()\n",
    "            # print(batch)\n",
    "\n",
    "            batch_state, batch_next_state, batch_action, batch_reward, batch_done = self.sample_batch()\n",
    "\n",
    "            # batch_state = torch.tensor(batch[0], dtype=torch.float32)\n",
    "            # batch_next_state = torch.tensor(batch[1][:, 12:24], dtype=torch.float32)\n",
    "            # batch_action = torch.tensor(batch[:, 24:25], dtype=torch.int64)\n",
    "            # batch_reward = torch.tensor(batch[:, 25:26], dtype=torch.float32)\n",
    "            # batch_done = torch.tensor(batch[:, 26:27], dtype=torch.float32)\n",
    "            \n",
    "\n",
    "            # Calculate the Q-values for the expert actions.\n",
    "            q_values = self.critic(torch.cat((batch_state, batch_action), dim=1))\n",
    "\n",
    "            # Update the critic.\n",
    "            self.critic.update(batch_state, batch_action, batch_reward, batch_done)\n",
    "\n",
    "            # Update the policy.\n",
    "            self.policy.update(batch_state, batch_reward, batch_done)\n",
    "\n",
    "    def sample_batch(self):\n",
    "        batch = []\n",
    "        for _ in range(self.batch_size):\n",
    "            data = self.expert_data.sample(self.batch_size)\n",
    "            data = torch.tensor(data, dtype=torch.float32)\n",
    "            state = data[:, :12]\n",
    "            next_state = data[:, 12:24]\n",
    "            action = data[:, 24:25]\n",
    "            reward = data[:, 25:26]\n",
    "            done = data[:, 26:27]\n",
    "        return state, next_state, action, reward, done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='random')\n",
    "policy_net = Policy(env)\n",
    "critic_net = Critic(env)\n",
    "expert_data = ExpertData()\n",
    "batch_size = 32\n",
    "gamma = 0.99\n",
    "tau = 0.001\n",
    "\n",
    "dqfd = DQfD(policy_net, critic_net, expert_data, batch_size, gamma, tau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Critic' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[107], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dqfd\u001b[39m.\u001b[39;49mlearn()\n",
      "Cell \u001b[0;32mIn[105], line 38\u001b[0m, in \u001b[0;36mDQfD.learn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m batch_state, batch_next_state, batch_action, batch_reward, batch_done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample_batch()\n\u001b[1;32m     30\u001b[0m \u001b[39m# batch_state = torch.tensor(batch[0], dtype=torch.float32)\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m# batch_next_state = torch.tensor(batch[1][:, 12:24], dtype=torch.float32)\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39m# batch_action = torch.tensor(batch[:, 24:25], dtype=torch.int64)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \n\u001b[1;32m     37\u001b[0m \u001b[39m# Calculate the Q-values for the expert actions.\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcritic(torch\u001b[39m.\u001b[39;49mcat((batch_state, batch_action), dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m))\n\u001b[1;32m     40\u001b[0m \u001b[39m# Update the critic.\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic\u001b[39m.\u001b[39mupdate(batch_state, batch_action, batch_reward, batch_done)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'Critic' object is not callable"
     ]
    }
   ],
   "source": [
    "dqfd.learn()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.9600, 0.9600, 0.8400, 0.9200, 0.8600, 0.9500, 0.9800, 0.7800, 0.6800,\n",
       "          0.8900, 0.0700, 0.1300],\n",
       "         [0.9900, 0.8900, 0.9400, 0.8100, 0.7700, 0.9900, 0.7600, 0.9600, 0.9800,\n",
       "          0.8400, 0.1000, 0.0600],\n",
       "         [1.0000, 1.0000, 0.9300, 0.8400, 0.9600, 0.7900, 0.9000, 0.9600, 0.9600,\n",
       "          0.9900, 0.1100, 0.1300],\n",
       "         [0.9400, 0.9600, 0.9600, 0.9300, 0.7500, 0.8600, 0.9500, 0.8600, 0.9800,\n",
       "          0.9200, 0.0700, 0.0100],\n",
       "         [0.7600, 0.9100, 0.8400, 0.9200, 0.8600, 0.9500, 1.0000, 0.8400, 0.6800,\n",
       "          0.8900, 0.1500, 0.1300],\n",
       "         [0.9400, 0.7100, 0.9600, 0.9700, 0.6800, 0.9200, 0.7400, 0.8900, 0.9900,\n",
       "          0.9100, 0.1400, 0.1500],\n",
       "         [0.8900, 0.9800, 0.9200, 0.9400, 0.9100, 0.9100, 0.7100, 1.0000, 0.8700,\n",
       "          0.8600, 0.1100, 0.1200],\n",
       "         [0.8600, 0.8700, 0.9000, 0.7000, 0.8600, 0.8200, 1.0000, 0.7700, 0.9700,\n",
       "          0.9600, 0.1500, 0.1000],\n",
       "         [0.7700, 0.9200, 0.8100, 0.9700, 0.8800, 0.9800, 0.8200, 0.9900, 0.7900,\n",
       "          0.9900, 0.1500, 0.1100],\n",
       "         [0.8500, 0.9000, 0.9400, 0.9200, 0.8400, 0.9000, 0.9800, 0.7100, 0.9600,\n",
       "          0.8600, 0.1500, 0.1100],\n",
       "         [0.7200, 0.9000, 0.7100, 0.9000, 0.8800, 0.8000, 0.9400, 0.8400, 0.7200,\n",
       "          0.9400, 0.0500, 0.1400],\n",
       "         [0.8900, 0.9800, 0.8800, 0.8300, 0.8700, 1.0000, 0.9000, 0.9800, 0.6800,\n",
       "          0.7900, 0.1300, 0.1100],\n",
       "         [0.9700, 0.8200, 0.9100, 0.7000, 0.8800, 0.8700, 0.5100, 0.8700, 0.9600,\n",
       "          0.7900, 0.0400, 0.1400],\n",
       "         [1.0000, 0.9100, 0.9900, 0.9500, 1.0000, 0.9600, 1.0000, 0.9500, 0.9900,\n",
       "          0.6400, 0.0900, 0.0200],\n",
       "         [0.7800, 0.8800, 0.8900, 0.9000, 0.7800, 0.9800, 0.8200, 0.9900, 0.6800,\n",
       "          0.9200, 0.0900, 0.1300],\n",
       "         [0.7600, 0.8200, 0.9500, 0.9300, 0.6600, 0.9200, 0.9000, 0.9100, 0.9200,\n",
       "          0.9100, 0.1000, 0.0300],\n",
       "         [0.8900, 0.9800, 0.9200, 0.9400, 0.9100, 0.9100, 0.7800, 0.9000, 0.8100,\n",
       "          0.9200, 0.1300, 0.1400],\n",
       "         [0.5800, 0.7600, 0.9000, 0.9900, 0.7600, 0.9000, 1.0000, 0.8300, 0.7400,\n",
       "          0.9300, 0.1500, 0.0100],\n",
       "         [0.8600, 0.9500, 0.9800, 0.9300, 0.9900, 0.9300, 0.7000, 0.9900, 0.9800,\n",
       "          0.9800, 0.0200, 0.0300],\n",
       "         [0.9400, 0.9600, 0.9600, 0.9300, 0.7500, 0.8600, 0.9500, 0.8600, 0.8600,\n",
       "          0.8800, 0.1200, 0.0400],\n",
       "         [0.9400, 0.7800, 0.9500, 0.7100, 0.9700, 0.8300, 1.0000, 0.9600, 0.9500,\n",
       "          0.6300, 0.0600, 0.1100],\n",
       "         [0.9400, 0.8100, 0.9200, 0.8100, 0.8600, 0.6900, 0.8400, 0.8900, 0.9600,\n",
       "          0.9900, 0.0400, 0.1500],\n",
       "         [0.6300, 0.8700, 0.9600, 0.9300, 0.7900, 0.9400, 1.0000, 0.8400, 0.7100,\n",
       "          0.9000, 0.0400, 0.0100],\n",
       "         [0.7800, 0.8800, 0.8900, 0.9000, 0.7800, 0.9800, 0.8200, 0.9900, 0.6800,\n",
       "          0.9200, 0.0900, 0.1300],\n",
       "         [0.9600, 0.9300, 0.9600, 0.9500, 0.9600, 0.7800, 0.9400, 0.7400, 0.7100,\n",
       "          0.9400, 0.0400, 0.0700],\n",
       "         [1.0000, 0.9600, 1.0000, 0.5900, 0.9900, 0.9400, 0.9900, 1.0000, 0.8100,\n",
       "          0.9000, 0.0800, 0.1400],\n",
       "         [0.9400, 0.8300, 0.9000, 0.8100, 0.8900, 0.9800, 1.0000, 0.8900, 0.9500,\n",
       "          0.8500, 0.1400, 0.0600],\n",
       "         [1.0000, 1.0000, 0.9300, 0.8400, 0.9600, 0.7900, 0.9000, 0.9600, 0.9600,\n",
       "          0.9900, 0.1100, 0.1300],\n",
       "         [0.9500, 0.7900, 0.8500, 0.7100, 0.8700, 0.9700, 0.4300, 0.8700, 0.9000,\n",
       "          0.9500, 0.1000, 0.0200],\n",
       "         [0.9000, 0.9600, 0.9500, 0.9700, 0.9400, 0.8200, 0.9800, 0.9600, 0.9200,\n",
       "          0.8900, 0.0900, 0.1100],\n",
       "         [0.8300, 0.8500, 0.6700, 0.9500, 0.7900, 0.9700, 0.9600, 0.9700, 0.9600,\n",
       "          0.9500, 0.0700, 0.1200],\n",
       "         [0.9200, 0.6800, 1.0000, 0.8400, 0.7400, 0.8900, 0.5100, 0.8700, 0.9700,\n",
       "          0.9700, 0.1100, 0.1500]]),\n",
       " tensor([[0.9600, 0.9600, 0.8400, 0.9200, 0.8600, 0.9500, 0.9800, 0.7800, 0.6800,\n",
       "          0.8900, 0.0700, 0.1300],\n",
       "         [0.9900, 0.8900, 0.9400, 0.8100, 0.7700, 0.9900, 0.7600, 0.9600, 0.9800,\n",
       "          0.8400, 0.1000, 0.0600],\n",
       "         [1.0000, 1.0000, 0.9300, 0.8400, 0.9600, 0.7900, 0.9000, 0.9600, 0.9600,\n",
       "          0.9900, 0.1100, 0.1300],\n",
       "         [0.9400, 0.9600, 0.9600, 0.9300, 0.8200, 0.8700, 0.9500, 0.8600, 0.9800,\n",
       "          0.9200, 0.0300, 0.1000],\n",
       "         [0.6300, 0.7900, 0.8400, 0.9200, 0.8600, 0.9500, 1.0000, 0.8400, 0.6800,\n",
       "          0.8900, 0.1500, 0.1300],\n",
       "         [0.9400, 0.7100, 0.8100, 0.8500, 0.6800, 0.9200, 0.7400, 0.8900, 0.9900,\n",
       "          0.9100, 0.1400, 0.1500],\n",
       "         [0.8900, 0.9800, 0.9200, 0.9400, 0.9100, 0.9100, 0.7100, 1.0000, 0.9800,\n",
       "          0.9800, 0.0400, 0.0800],\n",
       "         [0.8600, 0.8700, 0.9000, 0.7000, 0.8600, 0.8200, 0.8900, 0.6300, 0.9700,\n",
       "          0.9600, 0.1500, 0.1000],\n",
       "         [0.7700, 0.9200, 0.8100, 0.9700, 0.8800, 0.9800, 0.8200, 0.9900, 0.7900,\n",
       "          0.9900, 0.1500, 0.1100],\n",
       "         [0.8500, 0.9000, 0.9400, 0.9200, 0.8400, 0.9000, 0.9800, 0.7100, 0.9600,\n",
       "          0.8600, 0.1500, 0.1100],\n",
       "         [0.7200, 0.9000, 0.7100, 0.9000, 0.9300, 0.9400, 0.9400, 0.8400, 0.7200,\n",
       "          0.9400, 0.1400, 0.0400],\n",
       "         [0.8900, 0.9800, 0.8800, 0.8300, 0.8700, 1.0000, 0.8600, 0.8500, 0.8100,\n",
       "          0.9000, 0.0500, 0.1300],\n",
       "         [0.9700, 0.8200, 0.9500, 0.8400, 0.8800, 0.8700, 0.5100, 0.8700, 0.9600,\n",
       "          0.7900, 0.0600, 0.1100],\n",
       "         [1.0000, 0.9100, 0.9900, 0.9500, 1.0000, 0.9600, 1.0000, 0.9500, 0.9900,\n",
       "          0.6400, 0.0900, 0.0200],\n",
       "         [0.7800, 0.8800, 0.8900, 0.9000, 0.7800, 0.9800, 0.8200, 0.9900, 0.6800,\n",
       "          0.9200, 0.0900, 0.1300],\n",
       "         [0.8600, 0.8500, 0.9500, 0.9300, 0.6600, 0.9200, 0.9000, 0.9100, 0.9200,\n",
       "          0.9100, 0.1500, 0.0300],\n",
       "         [0.8900, 0.9800, 0.9200, 0.9400, 0.9100, 0.9100, 0.7800, 0.9000, 0.8100,\n",
       "          0.9200, 0.1300, 0.1400],\n",
       "         [0.7300, 0.7700, 0.9000, 0.9900, 0.6700, 0.8100, 1.0000, 0.8300, 0.7400,\n",
       "          0.9300, 0.0200, 0.0600],\n",
       "         [0.8600, 0.9500, 1.0000, 0.9600, 0.9900, 0.9300, 0.7000, 0.9900, 0.9800,\n",
       "          0.9800, 0.1000, 0.0600],\n",
       "         [0.9400, 0.9600, 0.9600, 0.9300, 0.7500, 0.8600, 0.9500, 0.8600, 0.9800,\n",
       "          0.9200, 0.0700, 0.0100],\n",
       "         [1.0000, 0.8900, 0.9500, 0.7100, 0.9700, 0.8300, 1.0000, 0.9600, 0.8400,\n",
       "          0.5900, 0.1500, 0.1000],\n",
       "         [0.9400, 0.8100, 0.9200, 0.8100, 0.9000, 0.8400, 0.8400, 0.8900, 0.9600,\n",
       "          0.9900, 0.0300, 0.0200],\n",
       "         [0.6300, 0.8700, 1.0000, 0.9400, 0.7900, 0.9400, 1.0000, 0.8400, 0.7100,\n",
       "          0.9000, 0.0400, 0.0200],\n",
       "         [0.7800, 0.8800, 0.8900, 0.9000, 0.7800, 0.9800, 0.8200, 0.9900, 0.6800,\n",
       "          0.9200, 0.0900, 0.1300],\n",
       "         [1.0000, 1.0000, 0.9600, 0.9500, 0.9600, 0.7800, 0.9400, 0.7400, 0.7100,\n",
       "          0.9400, 0.1500, 0.0900],\n",
       "         [1.0000, 0.9600, 1.0000, 0.5900, 0.9900, 0.9400, 0.9900, 1.0000, 0.8100,\n",
       "          0.9000, 0.0800, 0.1400],\n",
       "         [0.9400, 0.8300, 0.9000, 0.8100, 0.8900, 0.9800, 1.0000, 0.8900, 0.9500,\n",
       "          0.8500, 0.1400, 0.0600],\n",
       "         [1.0000, 1.0000, 0.9300, 0.8400, 0.9600, 0.7900, 0.9000, 0.9600, 0.9600,\n",
       "          0.9900, 0.1100, 0.1300],\n",
       "         [0.9500, 0.7900, 0.9500, 0.7300, 0.8700, 0.9700, 0.4300, 0.8700, 0.9000,\n",
       "          0.9500, 0.0900, 0.0300],\n",
       "         [0.9000, 0.9600, 0.9500, 0.9700, 0.9400, 0.8200, 0.9800, 0.9600, 0.9200,\n",
       "          0.8900, 0.0900, 0.1100],\n",
       "         [0.9000, 0.9700, 0.6300, 0.8700, 0.7900, 0.9700, 0.9600, 0.9700, 0.9600,\n",
       "          0.9500, 0.0600, 0.0700],\n",
       "         [0.9200, 0.6800, 1.0000, 0.8400, 0.7400, 0.8900, 0.5100, 0.8700, 0.9700,\n",
       "          0.9700, 0.1100, 0.1500]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [3.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [5.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [3.],\n",
       "         [5.],\n",
       "         [2.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [2.],\n",
       "         [5.],\n",
       "         [1.],\n",
       "         [3.],\n",
       "         [2.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [2.],\n",
       "         [0.],\n",
       "         [1.],\n",
       "         [0.]]),\n",
       " tensor([[ 0.0500],\n",
       "         [-0.0100],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 1.0000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [-0.0800],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 1.0000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000],\n",
       "         [ 0.5000]]),\n",
       " tensor([[0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.],\n",
       "         [0.]]))"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dqfd.sample_batch()[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
