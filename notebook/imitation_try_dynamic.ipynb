{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import DQN, PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from stable_baselines3.ppo import MlpPolicy\n",
    "\n",
    "from imitation.algorithms import bc\n",
    "from imitation.data import rollout\n",
    "from imitation.data.wrappers import RolloutInfoWrapper\n",
    "from imitation.data.types import Transitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test version\n",
    "# data = np.genfromtxt('../dataset/data_expert.csv', delimiter=',', max_rows=1000000) \n",
    "\n",
    "# Final version\n",
    "data = np.genfromtxt('../dataset/data_expert.csv', delimiter=',') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into obs, acts, infos, next_obs, and dones\n",
    "obs = data[:, :12]\n",
    "acts = data[:, 12]\n",
    "infos = np.empty(len(data), dtype=dict)\n",
    "next_obs = data[:, 13:25]\n",
    "dones = data[:, 25].astype(bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "transitions = Transitions(obs, acts, infos, next_obs, dones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'obs': array([0.91, 0.85, 0.73, 0.76, 0.83, 0.75, 0.82, 0.94, 0.76, 0.84, 0.05,\n",
       "        0.14]),\n",
       " 'acts': 1.0,\n",
       " 'infos': None,\n",
       " 'next_obs': array([0.86, 0.71, 0.73, 0.76, 0.83, 0.75, 0.82, 0.94, 0.76, 0.84, 0.04,\n",
       "        0.06]),\n",
       " 'dones': False}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transitions[13]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expert from newly trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def train_expert():\n",
    "#     print(\"Training a expert.\")\n",
    "\n",
    "#     expert = DQN(\n",
    "#         policy='MlpPolicy',\n",
    "#         env=env\n",
    "#     )\n",
    "\n",
    "#     expert.learn(10000)  # Note: change this to 100000 to train a decent expert.\n",
    "#     return expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sample_expert_transitions():\n",
    "#     expert = train_expert()\n",
    "\n",
    "#     print(\"Sampling expert transitions.\")\n",
    "#     rollouts = rollout.rollout(\n",
    "#         expert,\n",
    "#         DummyVecEnv([lambda: RolloutInfoWrapper(env)]),\n",
    "#         rollout.make_sample_until(min_timesteps=None, min_episodes=50),\n",
    "#         rng=rng,\n",
    "#     )\n",
    "#     return rollout.flatten_trajectories(rollouts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "# def eval_unit(model, env, num_eval_episodes=2):\n",
    "#     rews, lens = evaluate_policy(model, env, n_eval_episodes=num_eval_episodes, return_episode_rewards=True)\n",
    "#     # Takes the mean of rews elements divided by lens elements\n",
    "#     mean_rew = np.mean([rew / length for rew, length in zip(rews, lens)]).round(2)\n",
    "\n",
    "#     # Takes the difference between the maximum and minimum of reward elements\n",
    "#     std_rew = (np.max(rews) - np.min(rews)) / 2\n",
    "\n",
    "#     mean_rew = round(mean_rew, 2)\n",
    "#     std_rew = round(std_rew, 2)\n",
    "\n",
    "#     return (mean_rew, std_rew)\n",
    "\n",
    "\n",
    "# def eval_set(model):\n",
    "#     eval_rur_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rur_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rur_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rur.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "#     eval_rbd1_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd1_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd1_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd1.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "#     eval_rbd2_env1 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd2_env2 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-10l-3m-1000p-10m_unbalanced.csv')\n",
    "#     eval_rbd2_env3 = gym.make('SimKubeEnv-v0', reward_file='eval_rbd2.py', scenario_file='scenario-3l-10m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "#     rur1 = eval_unit(model, eval_rur_env1, num_eval_episodes=2)\n",
    "#     rur2 = eval_unit(model, eval_rur_env2, num_eval_episodes=2)\n",
    "#     rur3 = eval_unit(model, eval_rur_env3, num_eval_episodes=2)\n",
    "\n",
    "#     print(rur1, rur2, rur3)\n",
    "\n",
    "#     rbd11 = eval_unit(model, eval_rbd1_env1, num_eval_episodes=2)\n",
    "#     rbd12 = eval_unit(model, eval_rbd1_env2, num_eval_episodes=2)\n",
    "#     rbd13 = eval_unit(model, eval_rbd1_env3, num_eval_episodes=2)\n",
    "\n",
    "#     print(rbd11, rbd12, rbd13)\n",
    "\n",
    "#     rbd21 = eval_unit(model, eval_rbd2_env1, num_eval_episodes=2)\n",
    "#     rbd22 = eval_unit(model, eval_rbd2_env2, num_eval_episodes=2)\n",
    "#     rbd23 = eval_unit(model, eval_rbd2_env3, num_eval_episodes=2)\n",
    "\n",
    "#     print(rbd21, rbd22, rbd23)\n",
    "    \n",
    "\n",
    "def evalnoma(eval_model):\n",
    "\n",
    "    eval_env0 = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_3.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "    eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "    step3 = evaluate_policy(eval_model, eval_env0, n_eval_episodes=1, return_episode_rewards=True)\n",
    "    rur = evaluate_policy(eval_model, eval_env1, n_eval_episodes=1, return_episode_rewards=True)\n",
    "    rbd1 = evaluate_policy(eval_model, eval_env2, n_eval_episodes=1, return_episode_rewards=True)\n",
    "    rbd2 = evaluate_policy(eval_model, eval_env3, n_eval_episodes=1, return_episode_rewards=True)\n",
    "\n",
    "    print(step3, rur, rbd1, rbd2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "# Initialize environment without rendering\n",
    "env = gym.make(\"SimKubeEnv-v0\", reward_file='train_step_dynamic.py')\n",
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "bc_trainer = bc.BC(\n",
    "    observation_space=env.observation_space,\n",
    "    action_space=env.action_space,\n",
    "    demonstrations=transitions,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_set(bc_trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0batch [00:00, ?batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 0        |\n",
      "|    ent_loss       | -0.00179 |\n",
      "|    entropy        | 1.79     |\n",
      "|    epoch          | 0        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 88.5     |\n",
      "|    loss           | 1.79     |\n",
      "|    neglogp        | 1.79     |\n",
      "|    prob_true_act  | 0.167    |\n",
      "|    samples_so_far | 32       |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "99994batch [03:56, 451.85batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 100000    |\n",
      "|    ent_loss       | -0.000229 |\n",
      "|    entropy        | 0.229     |\n",
      "|    epoch          | 0         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 1.3e+03   |\n",
      "|    loss           | 0.128     |\n",
      "|    neglogp        | 0.128     |\n",
      "|    prob_true_act  | 0.906     |\n",
      "|    samples_so_far | 3200032   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "199971batch [08:34, 283.36batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 200000    |\n",
      "|    ent_loss       | -9.89e-05 |\n",
      "|    entropy        | 0.0989    |\n",
      "|    epoch          | 1         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 2.08e+03  |\n",
      "|    loss           | 0.0696    |\n",
      "|    neglogp        | 0.0697    |\n",
      "|    prob_true_act  | 0.951     |\n",
      "|    samples_so_far | 6400032   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "299988batch [12:58, 345.93batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 300000    |\n",
      "|    ent_loss       | -0.000118 |\n",
      "|    entropy        | 0.118     |\n",
      "|    epoch          | 2         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 2.73e+03  |\n",
      "|    loss           | 0.0924    |\n",
      "|    neglogp        | 0.0925    |\n",
      "|    prob_true_act  | 0.945     |\n",
      "|    samples_so_far | 9600032   |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "399965batch [17:12, 400.75batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 400000    |\n",
      "|    ent_loss       | -9.88e-05 |\n",
      "|    entropy        | 0.0988    |\n",
      "|    epoch          | 3         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 3.35e+03  |\n",
      "|    loss           | 0.104     |\n",
      "|    neglogp        | 0.104     |\n",
      "|    prob_true_act  | 0.94      |\n",
      "|    samples_so_far | 12800032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "499977batch [21:55, 415.18batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 500000    |\n",
      "|    ent_loss       | -8.91e-05 |\n",
      "|    entropy        | 0.0891    |\n",
      "|    epoch          | 4         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 3.95e+03  |\n",
      "|    loss           | 0.207     |\n",
      "|    neglogp        | 0.207     |\n",
      "|    prob_true_act  | 0.927     |\n",
      "|    samples_so_far | 16000032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "599999batch [26:04, 396.55batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "| batch_size        | 32       |\n",
      "| bc/               |          |\n",
      "|    batch          | 600000   |\n",
      "|    ent_loss       | -8.9e-05 |\n",
      "|    entropy        | 0.089    |\n",
      "|    epoch          | 5        |\n",
      "|    l2_loss        | 0        |\n",
      "|    l2_norm        | 4.54e+03 |\n",
      "|    loss           | 0.0651   |\n",
      "|    neglogp        | 0.0652   |\n",
      "|    prob_true_act  | 0.954    |\n",
      "|    samples_so_far | 19200032 |\n",
      "--------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "699978batch [30:22, 305.07batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 700000    |\n",
      "|    ent_loss       | -0.000146 |\n",
      "|    entropy        | 0.146     |\n",
      "|    epoch          | 6         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 5.12e+03  |\n",
      "|    loss           | 0.294     |\n",
      "|    neglogp        | 0.295     |\n",
      "|    prob_true_act  | 0.86      |\n",
      "|    samples_so_far | 22400032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "799977batch [35:22, 373.12batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 800000    |\n",
      "|    ent_loss       | -3.99e-05 |\n",
      "|    entropy        | 0.0399    |\n",
      "|    epoch          | 7         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 5.67e+03  |\n",
      "|    loss           | 0.0188    |\n",
      "|    neglogp        | 0.0188    |\n",
      "|    prob_true_act  | 0.984     |\n",
      "|    samples_so_far | 25600032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "899996batch [40:38, 324.21batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 900000    |\n",
      "|    ent_loss       | -6.41e-05 |\n",
      "|    entropy        | 0.0641    |\n",
      "|    epoch          | 8         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 6.19e+03  |\n",
      "|    loss           | 0.0554    |\n",
      "|    neglogp        | 0.0555    |\n",
      "|    prob_true_act  | 0.962     |\n",
      "|    samples_so_far | 28800032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "999973batch [45:20, 402.39batch/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "| batch_size        | 32        |\n",
      "| bc/               |           |\n",
      "|    batch          | 1000000   |\n",
      "|    ent_loss       | -0.000124 |\n",
      "|    entropy        | 0.124     |\n",
      "|    epoch          | 9         |\n",
      "|    l2_loss        | 0         |\n",
      "|    l2_norm        | 6.72e+03  |\n",
      "|    loss           | 0.0827    |\n",
      "|    neglogp        | 0.0828    |\n",
      "|    prob_true_act  | 0.939     |\n",
      "|    samples_so_far | 32000032  |\n",
      "---------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1071450batch [48:18, 369.72batch/s]\n"
     ]
    }
   ],
   "source": [
    "# bc_trainer.train(log_interval=100000, n_batches=100000) # Test version \n",
    "bc_trainer.train(n_epochs=10, log_interval=100000) # Fianl version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evalnoma(bc_trainer.policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "eval_env1 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rur.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "eval_env2 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd1.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "eval_env3 = gym.make(\"SimKubeEnv-v0\", reward_file='eval_rbd2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "# Test version\n",
    "# eval_callback1 = EvalCallback(eval_env1, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "# eval_callback2 = EvalCallback(eval_env2, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "# eval_callback3 = EvalCallback(eval_env3, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False)\n",
    "\n",
    "# Final verison\n",
    "eval_callback1 = EvalCallback(eval_env1, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rur\")\n",
    "eval_callback2 = EvalCallback(eval_env2, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rbd1\")\n",
    "eval_callback3 = EvalCallback(eval_env3, eval_freq=100000, n_eval_episodes=3, deterministic=True, render=False, log_path=\"results/poc_1/rbd2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "model1 = DQN('MlpPolicy', env, verbose=1)\n",
    "model2 = DQN('MlpPolicy', env, verbose=1)\n",
    "model3 = DQN('MlpPolicy', env, verbose=1)\n",
    "\n",
    "model1.policy = bc_trainer.policy\n",
    "model2.policy = bc_trainer.policy\n",
    "model3.policy = bc_trainer.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Test version\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# model1.learn(total_timesteps=5e5, \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m#             log_interval=50, \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m \n\u001b[1;32m      8\u001b[0m \u001b[39m# Final version\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m model1\u001b[39m.\u001b[39;49mlearn(total_timesteps\u001b[39m=\u001b[39;49m\u001b[39m1e6\u001b[39;49m, \n\u001b[1;32m     10\u001b[0m             log_interval\u001b[39m=\u001b[39;49m\u001b[39m50\u001b[39;49m, \n\u001b[1;32m     11\u001b[0m             \u001b[39m# progress_bar=True, \u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m             callback\u001b[39m=\u001b[39;49meval_callback1\n\u001b[1;32m     13\u001b[0m            )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/dqn/dqn.py:269\u001b[0m, in \u001b[0;36mDQN.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlearn\u001b[39m(\n\u001b[1;32m    261\u001b[0m     \u001b[39mself\u001b[39m: SelfDQN,\n\u001b[1;32m    262\u001b[0m     total_timesteps: \u001b[39mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    267\u001b[0m     progress_bar: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    268\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m SelfDQN:\n\u001b[0;32m--> 269\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mlearn(\n\u001b[1;32m    270\u001b[0m         total_timesteps\u001b[39m=\u001b[39;49mtotal_timesteps,\n\u001b[1;32m    271\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    272\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    273\u001b[0m         tb_log_name\u001b[39m=\u001b[39;49mtb_log_name,\n\u001b[1;32m    274\u001b[0m         reset_num_timesteps\u001b[39m=\u001b[39;49mreset_num_timesteps,\n\u001b[1;32m    275\u001b[0m         progress_bar\u001b[39m=\u001b[39;49mprogress_bar,\n\u001b[1;32m    276\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:311\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    308\u001b[0m callback\u001b[39m.\u001b[39mon_training_start(\u001b[39mlocals\u001b[39m(), \u001b[39mglobals\u001b[39m())\n\u001b[1;32m    310\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 311\u001b[0m     rollout \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollect_rollouts(\n\u001b[1;32m    312\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv,\n\u001b[1;32m    313\u001b[0m         train_freq\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrain_freq,\n\u001b[1;32m    314\u001b[0m         action_noise\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49maction_noise,\n\u001b[1;32m    315\u001b[0m         callback\u001b[39m=\u001b[39;49mcallback,\n\u001b[1;32m    316\u001b[0m         learning_starts\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlearning_starts,\n\u001b[1;32m    317\u001b[0m         replay_buffer\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreplay_buffer,\n\u001b[1;32m    318\u001b[0m         log_interval\u001b[39m=\u001b[39;49mlog_interval,\n\u001b[1;32m    319\u001b[0m     )\n\u001b[1;32m    321\u001b[0m     \u001b[39mif\u001b[39;00m rollout\u001b[39m.\u001b[39mcontinue_training \u001b[39mis\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    322\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/off_policy_algorithm.py:543\u001b[0m, in \u001b[0;36mOffPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, train_freq, replay_buffer, action_noise, learning_starts, log_interval)\u001b[0m\n\u001b[1;32m    540\u001b[0m actions, buffer_actions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sample_action(learning_starts, action_noise, env\u001b[39m.\u001b[39mnum_envs)\n\u001b[1;32m    542\u001b[0m \u001b[39m# Rescale and perform action\u001b[39;00m\n\u001b[0;32m--> 543\u001b[0m new_obs, rewards, dones, infos \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mstep(actions)\n\u001b[1;32m    545\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_timesteps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mnum_envs\n\u001b[1;32m    546\u001b[0m num_collected_steps \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:163\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    157\u001b[0m \u001b[39mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[39m:param actions: the action\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[39m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    162\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 163\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstep_wait()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:54\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep_wait\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     53\u001b[0m     \u001b[39mfor\u001b[39;00m env_idx \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_envs):\n\u001b[0;32m---> 54\u001b[0m         obs, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_rews[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menvs[env_idx]\u001b[39m.\u001b[39;49mstep(\n\u001b[1;32m     55\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mactions[env_idx]\n\u001b[1;32m     56\u001b[0m         )\n\u001b[1;32m     57\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     58\u001b[0m             \u001b[39m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     59\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbuf_infos[env_idx][\u001b[39m\"\u001b[39m\u001b[39mterminal_observation\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m obs\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/stable_baselines3/common/monitor.py:95\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mneeds_reset:\n\u001b[1;32m     94\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mTried to step environment that needs reset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     96\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrewards\u001b[39m.\u001b[39mappend(reward)\n\u001b[1;32m     97\u001b[0m \u001b[39mif\u001b[39;00m done:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/gym/wrappers/order_enforcing.py:11\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action):\n\u001b[1;32m     10\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_reset, \u001b[39m\"\u001b[39m\u001b[39mCannot call env.step() before calling reset()\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m     observation, reward, done, info \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv\u001b[39m.\u001b[39;49mstep(action)\n\u001b[1;32m     12\u001b[0m     \u001b[39mreturn\u001b[39;00m observation, reward, done, info\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env.py:279\u001b[0m, in \u001b[0;36mSimKubeEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfo \u001b[39m=\u001b[39m {\n\u001b[1;32m    274\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mlast_pod\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mNone\u001b[39;00m, \u001b[39m# None\u001b[39;00m\n\u001b[1;32m    275\u001b[0m             \u001b[39m'\u001b[39m\u001b[39mis_scheduled\u001b[39m\u001b[39m'\u001b[39m : \u001b[39mFalse\u001b[39;00m \u001b[39m# None\u001b[39;00m\n\u001b[1;32m    276\u001b[0m         }\n\u001b[1;32m    278\u001b[0m \u001b[39m# Get reward\u001b[39;00m\n\u001b[0;32m--> 279\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_reward(env_prev, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcluster, action, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfo, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtime)\n\u001b[1;32m    281\u001b[0m \u001b[39m# Get done\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_done()\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env.py:166\u001b[0m, in \u001b[0;36mSimKubeEnv.get_reward\u001b[0;34m(self, env_prev, cluster, action, info, time)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_reward\u001b[39m(\u001b[39mself\u001b[39m, env_prev, cluster, action, info, time):\n\u001b[0;32m--> 166\u001b[0m     reward \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_fn\u001b[39m.\u001b[39;49mget_reward(env_prev, cluster, action, info, time, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebug)\n\u001b[1;32m    168\u001b[0m     \u001b[39mreturn\u001b[39;00m reward\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_rl_scheduler/strategies/reward/train_step_dynamic.py:42\u001b[0m, in \u001b[0;36mget_reward\u001b[0;34m(env_prev, cluster, action, info, time, debug)\u001b[0m\n\u001b[1;32m     40\u001b[0m default_scheduler \u001b[39m=\u001b[39m SimHrScheduler(env_prev, \u001b[39m'\u001b[39m\u001b[39mdefault.py\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     41\u001b[0m default_action \u001b[39m=\u001b[39m default_scheduler\u001b[39m.\u001b[39mdecision(env_prev)\n\u001b[0;32m---> 42\u001b[0m _, _, _, default_info \u001b[39m=\u001b[39m env_prev\u001b[39m.\u001b[39;49mstep(default_action)\n\u001b[1;32m     43\u001b[0m default_cluster \u001b[39m=\u001b[39m env_prev\u001b[39m.\u001b[39mcluster\n\u001b[1;32m     44\u001b[0m default_time \u001b[39m=\u001b[39m time\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env_copy.py:147\u001b[0m, in \u001b[0;36mSimKubeEnvCopy.step\u001b[0;34m(self, action, datagen)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mstep\u001b[39m(\u001b[39mself\u001b[39m, action, datagen\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m--> 147\u001b[0m     env_prev \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mduplicate()\n\u001b[1;32m    149\u001b[0m     \u001b[39m# Update last cluster state\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mupdate_last_cluster_state()\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env_copy.py:77\u001b[0m, in \u001b[0;36mSimKubeEnvCopy.duplicate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcopy\u001b[39;00m \u001b[39mimport\u001b[39;00m deepcopy\n\u001b[1;32m     76\u001b[0m new_env \u001b[39m=\u001b[39m SimKubeEnvCopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_file, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscenario_file, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_node, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcpu_pool, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmem_pool, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug)\n\u001b[0;32m---> 77\u001b[0m new_env\u001b[39m.\u001b[39mcluster \u001b[39m=\u001b[39m deepcopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcluster)\n\u001b[1;32m     78\u001b[0m new_env\u001b[39m.\u001b[39mtime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime\n\u001b[1;32m     80\u001b[0m \u001b[39mreturn\u001b[39;00m new_env\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:270\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 270\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    272\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:205\u001b[0m, in \u001b[0;36m_deepcopy_list\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    203\u001b[0m append \u001b[39m=\u001b[39m y\u001b[39m.\u001b[39mappend\n\u001b[1;32m    204\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m x:\n\u001b[0;32m--> 205\u001b[0m     append(deepcopy(a, memo))\n\u001b[1;32m    206\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:172\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 y \u001b[39m=\u001b[39m x\n\u001b[1;32m    171\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 172\u001b[0m                 y \u001b[39m=\u001b[39m _reconstruct(x, memo, \u001b[39m*\u001b[39;49mrv)\n\u001b[1;32m    174\u001b[0m \u001b[39m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[1;32m    175\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m x:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:270\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[39mif\u001b[39;00m state \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    269\u001b[0m     \u001b[39mif\u001b[39;00m deep:\n\u001b[0;32m--> 270\u001b[0m         state \u001b[39m=\u001b[39m deepcopy(state, memo)\n\u001b[1;32m    271\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(y, \u001b[39m'\u001b[39m\u001b[39m__setstate__\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[1;32m    272\u001b[0m         y\u001b[39m.\u001b[39m__setstate__(state)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:146\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39mget(\u001b[39mcls\u001b[39m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n\u001b[1;32m    147\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    148\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39missubclass\u001b[39m(\u001b[39mcls\u001b[39m, \u001b[39mtype\u001b[39m):\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:230\u001b[0m, in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    228\u001b[0m memo[\u001b[39mid\u001b[39m(x)] \u001b[39m=\u001b[39m y\n\u001b[1;32m    229\u001b[0m \u001b[39mfor\u001b[39;00m key, value \u001b[39min\u001b[39;00m x\u001b[39m.\u001b[39mitems():\n\u001b[0;32m--> 230\u001b[0m     y[deepcopy(key, memo)] \u001b[39m=\u001b[39m deepcopy(value, memo)\n\u001b[1;32m    231\u001b[0m \u001b[39mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/copy.py:144\u001b[0m, in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m y\n\u001b[1;32m    142\u001b[0m \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(x)\n\u001b[0;32m--> 144\u001b[0m copier \u001b[39m=\u001b[39m _deepcopy_dispatch\u001b[39m.\u001b[39;49mget(\u001b[39mcls\u001b[39;49m)\n\u001b[1;32m    145\u001b[0m \u001b[39mif\u001b[39;00m copier \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     y \u001b[39m=\u001b[39m copier(x, memo)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test version\n",
    "# model1.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model1.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback1\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test version\n",
    "# model2.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model2.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback2\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test version\n",
    "# model3.learn(total_timesteps=5e5, \n",
    "#             log_interval=50, \n",
    "#             # progress_bar=True, \n",
    "#             callback=eval_callback1\n",
    "#            )\n",
    "\n",
    "# Final version\n",
    "model3.learn(total_timesteps=1e6, \n",
    "            log_interval=50, \n",
    "            # progress_bar=True, \n",
    "            callback=eval_callback3\n",
    "           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
