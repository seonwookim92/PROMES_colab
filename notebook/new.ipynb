{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swkim/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from kube_sim_gym.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rl_model(scenario_file, rl_model):\n",
    "\n",
    "    test_env1 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "    test_env2 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "\n",
    "    # RL Scheduler\n",
    "    rl_model.set_env(test_env1)\n",
    "\n",
    "    # Default Scheduler\n",
    "    from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "    default_scheduler = SimHrScheduler(test_env2, 'default.py')\n",
    "\n",
    "\n",
    "    # Test the model\n",
    "    obs1 = test_env1.reset()\n",
    "    obs2 = test_env2.reset()\n",
    "    done1 = False\n",
    "    done2 = False\n",
    "    step1 = 0\n",
    "    step2 = 0\n",
    "    acc_rew1 = 0\n",
    "    acc_rew2 = 0\n",
    "\n",
    "    print(f\"Testing with {scenario_file} (my model vs. default)\")\n",
    "    while not done1 or not done2:\n",
    "        if not done1:\n",
    "            action1, _ = rl_model.predict(obs1)\n",
    "            # action1 = rl_scheduler.decision(test_env1)\n",
    "            obs1, reward1, done1, _ = test_env1.step(action1)\n",
    "            step1 += 1\n",
    "            acc_rew1 += reward1\n",
    "        if not done2:\n",
    "            action2 = default_scheduler.decision(test_env2)\n",
    "            obs2, reward2, done2, _ = test_env2.step(action2)\n",
    "            step2 += 1\n",
    "            acc_rew2 += reward2\n",
    "\n",
    "    acc_rew1 = round(acc_rew1, 2)\n",
    "    acc_rew2 = round(acc_rew2, 2)\n",
    "\n",
    "    print(f\"Test result(reward): {acc_rew1} vs. {acc_rew2}\")\n",
    "    print(f\"Test result(step): {step1} vs. {step2}\")\n",
    "\n",
    "    return acc_rew1, acc_rew2, step1, step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from notebook.net_arch import *\n",
    "import glob\n",
    "\n",
    "def train_rl_model(json_tracker_fname):\n",
    "\n",
    "    date = datetime(1992, 7, 5, 8, 33)\n",
    "    date = date.strftime(\"%m%d%Y%H%M\")\n",
    "\n",
    "    log_name = json_tracker_fname.split('.')[0]\n",
    "    log_path = f'training/log/{log_name}'\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "\n",
    "    logger = configure(log_path, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # Load the json tracker\n",
    "    import json\n",
    "    with open(f'training/{json_tracker_fname}', 'r') as f:\n",
    "        json_tracker = json.load(f)\n",
    "\n",
    "    last_idx = json_tracker['last_idx']\n",
    "    learning_steps = json_tracker['learning_steps']\n",
    "    model_type = json_tracker['model_type']\n",
    "    reward_file = json_tracker['reward_file']\n",
    "    model_fname = json_tracker['model_fname']\n",
    "\n",
    "    # Environment\n",
    "    envs = []\n",
    "    for i in range(1, 50):\n",
    "        env = gym.make('SimKubeEnv-v0', reward_file=reward_file, scenario_file=f'trace2017_100_{i}.csv')\n",
    "        envs.append(env)\n",
    "\n",
    "    current_idx = last_idx + 1 # -1 as default\n",
    "\n",
    "    # Model type : DQN or PPO\n",
    "    if model_type == 'DQN':\n",
    "        model = sb3.DQN\n",
    "    elif model_type == 'PPO':\n",
    "        model = sb3.PPO\n",
    "    else:\n",
    "        print(f\"Unknown model type: {model_type}\")\n",
    "        return\n",
    "    \n",
    "    model_fpath = f'net_arch/{model_fname}.zip'\n",
    "\n",
    "    # Check if the model exists\n",
    "    # Load Model\n",
    "    if os.path.exists(model_fpath):\n",
    "        print(f\"Loading the model from {model_fname}\")\n",
    "        model = model.load(model_fpath)\n",
    "    else: # Error\n",
    "        print(f\"Model file does not exist: {model_fname}\")\n",
    "        return\n",
    "    \n",
    "    # If last_idx is not -1 and there's a model trained in training/model, then load the model\n",
    "    if last_idx != -1 and glob.glob(f'training/model/{model_fname}_*'):\n",
    "        # Load the model with the latest date\n",
    "        model_fpaths = glob.glob(f'training/model/{model_fname}_*')\n",
    "        model_fpaths.sort()\n",
    "        model_fpath = model_fpaths[-1]\n",
    "        print(f\"Loading the model from {model_fpath}\")\n",
    "    \n",
    "    # Save the model, append _{date} to the model name\n",
    "    trained_model_fname = f'{model_fname}_{date}'\n",
    "    trained_model_fpath = f'training/model/{trained_model_fname}'\n",
    "\n",
    "    # Set logger\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    # Train the model\n",
    "    while current_idx < 20: # Target training steps (Can be changed!)\n",
    "        print(f\"Training with {current_idx}th trace\")\n",
    "\n",
    "        # Test the model first\n",
    "        a1, a2, a3, a4 = test_rl_model('scenario-5l-5m-1000p-10m_unbalanced.csv', model)\n",
    "        b1, b2, b3, b4 = test_rl_model('scenario-10l-3m-1000p-10m_unbalanced.csv', model)\n",
    "        c1, c2, c3, c4 = test_rl_model('scenario-3l-10m-1000p-10m_unbalanced.csv', model)\n",
    "\n",
    "        with open(f'training/log/{log_name}/test_result.txt', 'a') as f:\n",
    "            f.write(f\"{current_idx},{a1},{a2},{a3},{a4},{b1},{b2},{b3},{b4},{c1},{c2},{c3},{c4}\\n\")\n",
    "\n",
    "        env = envs[current_idx]\n",
    "        model.set_env(env)\n",
    "        model.learn(total_timesteps=learning_steps)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(trained_model_fpath)\n",
    "\n",
    "        # Update the json tracker\n",
    "        json_tracker['last_idx'] = current_idx\n",
    "        with open(f'training/{json_tracker_fname}', 'w') as f:\n",
    "            json.dump(json_tracker, f)\n",
    "\n",
    "        current_idx += 1\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pr_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, train=True):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        # Drop the row which has 0 for the last -2, -3 columns\n",
    "        # self.data = self.data.drop(self.data[(self.data.iloc[:, -2] == 0) & (self.data.iloc[:, -3] == 0)].index)\n",
    "\n",
    "        if train:\n",
    "            self.data = self.data.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            self.data = self.data.drop(self.data.sample(frac=0.8, random_state=42).index)\n",
    "\n",
    "        self.data = self.transform(self.data)\n",
    "        self.input = self.data[:, :-6]\n",
    "        self.label = self.data[:, -6:]\n",
    "        # Multiply by 100\n",
    "        self.label = self.label * 100\n",
    "\n",
    "    def transform(self, data):\n",
    "        return torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_2.csv\")\n",
    "train_pr_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_pr_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_pr_dataloader = DataLoader(train_pr_dataset, batch_size=32, shuffle=False)\n",
    "test_pr_dataloader = DataLoader(test_pr_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 12]) torch.Size([32, 6])\n",
      "input1: tensor([[0.4000, 0.2700, 0.8800, 0.6200, 0.8200, 0.3000, 0.0200, 0.6800, 0.1100,\n",
      "         0.8800, 0.2500, 0.4600],\n",
      "        [0.3700, 0.1300, 0.0800, 0.5100, 0.6800, 0.5100, 0.6100, 0.2700, 0.1900,\n",
      "         0.1300, 0.3700, 0.4300],\n",
      "        [0.3100, 0.2000, 0.7700, 0.0400, 0.2000, 0.5300, 0.4000, 0.4500, 0.8100,\n",
      "         0.9800, 0.4100, 0.4800],\n",
      "        [0.6100, 0.4300, 0.0000, 0.8800, 0.1000, 0.7200, 0.6400, 0.2900, 0.4300,\n",
      "         0.5300, 0.1700, 0.2100],\n",
      "        [0.6300, 0.7300, 0.1000, 0.3700, 0.5500, 0.0300, 0.8800, 0.9600, 0.6100,\n",
      "         0.3800, 0.0900, 0.2200],\n",
      "        [0.0900, 0.9200, 0.5500, 0.1800, 0.1900, 0.2800, 0.4200, 0.8500, 0.3100,\n",
      "         0.5900, 0.0000, 0.0000],\n",
      "        [0.5800, 0.2700, 0.2100, 0.5500, 0.5000, 0.4200, 0.5500, 0.4900, 0.7500,\n",
      "         0.9000, 0.1300, 0.0400],\n",
      "        [0.3900, 0.0600, 0.0700, 0.6000, 0.4800, 0.1900, 0.0100, 0.7700, 0.3200,\n",
      "         0.1700, 0.1100, 0.1900],\n",
      "        [0.5100, 0.9700, 0.8700, 0.8200, 0.7300, 0.9200, 0.6700, 0.0700, 0.0800,\n",
      "         0.1400, 0.0000, 0.0000],\n",
      "        [0.1200, 0.7000, 0.3400, 0.0900, 0.1400, 0.9800, 0.1600, 0.9500, 0.6200,\n",
      "         0.4800, 0.0000, 0.0000],\n",
      "        [0.4400, 0.7900, 0.3900, 0.6500, 0.6300, 0.6100, 0.0600, 0.3900, 0.8100,\n",
      "         0.1200, 0.0700, 0.1600],\n",
      "        [0.2400, 0.0300, 0.9800, 0.0400, 0.2500, 0.5900, 0.0500, 0.8900, 0.5000,\n",
      "         0.4700, 0.1000, 0.1700],\n",
      "        [0.6500, 0.2700, 0.6200, 0.1400, 0.2600, 0.6600, 0.6500, 0.1000, 0.7700,\n",
      "         0.1900, 0.4100, 0.4600],\n",
      "        [0.1500, 0.3600, 0.7300, 0.3700, 0.0200, 0.9400, 0.0600, 0.8300, 0.8800,\n",
      "         0.7100, 0.1600, 0.0500],\n",
      "        [0.1200, 0.3100, 0.2500, 0.9900, 0.4700, 0.1500, 0.4300, 0.6700, 0.2000,\n",
      "         0.4700, 0.2100, 0.1700],\n",
      "        [0.8500, 0.7100, 0.6300, 0.3700, 0.9200, 0.1100, 0.3600, 0.2700, 0.2400,\n",
      "         0.0500, 0.1100, 0.4200],\n",
      "        [0.4700, 0.0600, 0.4400, 0.4600, 0.3700, 0.8300, 0.5200, 0.3000, 0.3000,\n",
      "         0.6700, 0.0000, 0.0000],\n",
      "        [0.5300, 0.6000, 0.8000, 0.8200, 0.0600, 0.0300, 0.6200, 0.1400, 0.0500,\n",
      "         0.3400, 0.2700, 0.3600],\n",
      "        [0.8200, 0.0000, 0.1800, 0.6400, 0.9600, 0.5400, 0.4200, 0.1100, 0.2500,\n",
      "         0.1700, 0.0000, 0.0000],\n",
      "        [0.9700, 0.2100, 0.3300, 0.4800, 0.4000, 0.6700, 0.9600, 0.0900, 0.7600,\n",
      "         0.6400, 0.0000, 0.0000],\n",
      "        [0.7400, 0.2900, 0.8300, 0.4400, 0.0900, 0.3300, 0.6900, 0.3700, 0.4400,\n",
      "         0.3400, 0.1900, 0.2500],\n",
      "        [0.1400, 0.4600, 0.8700, 0.6600, 0.7000, 0.1400, 0.1200, 0.4700, 0.3200,\n",
      "         0.0300, 0.2700, 0.3700],\n",
      "        [0.1000, 0.0900, 0.7900, 0.6300, 0.3100, 0.5900, 0.0600, 0.5400, 0.4800,\n",
      "         0.2800, 0.4900, 0.1100],\n",
      "        [0.0400, 0.6800, 0.6700, 0.8900, 0.0400, 0.4000, 0.4300, 0.3700, 0.4600,\n",
      "         0.6800, 0.3900, 0.4100],\n",
      "        [0.7200, 0.9700, 0.4600, 0.5900, 0.7500, 0.9600, 0.4600, 0.0100, 0.8700,\n",
      "         0.7200, 0.2000, 0.0200],\n",
      "        [0.0600, 0.3900, 0.1200, 0.6500, 0.1400, 0.5800, 0.4600, 0.1000, 0.6400,\n",
      "         0.0900, 0.4000, 0.4700],\n",
      "        [0.1300, 0.3800, 0.1600, 0.2800, 0.9200, 0.0300, 0.9400, 0.4500, 0.1000,\n",
      "         0.6700, 0.0000, 0.0000],\n",
      "        [0.2200, 0.6000, 0.1400, 0.9100, 0.7800, 0.3400, 0.5200, 0.8100, 0.2000,\n",
      "         0.3400, 0.0000, 0.0000],\n",
      "        [0.0000, 0.8200, 0.4800, 0.8600, 0.9100, 0.0300, 0.8700, 0.1900, 0.7100,\n",
      "         0.7900, 0.0000, 0.0000],\n",
      "        [0.0800, 0.6800, 0.8400, 0.6000, 0.1400, 0.7600, 0.2800, 0.2300, 0.5400,\n",
      "         0.4200, 0.0900, 0.4100],\n",
      "        [0.7900, 0.8400, 0.6900, 0.6000, 0.2200, 0.9000, 0.7400, 0.6800, 0.9000,\n",
      "         0.1500, 0.1800, 0.1800],\n",
      "        [0.0900, 0.6100, 0.9000, 0.7200, 0.5500, 0.8000, 0.7800, 0.8200, 0.7700,\n",
      "         0.2800, 0.4500, 0.2900]])\n",
      "labels: tensor([[  0.0000,  59.0000,  32.0000,  28.0000,  32.0000,  19.0000],\n",
      "        [  0.0000,  55.0000,  44.0000,  42.0000,  41.0000,  69.0000],\n",
      "        [  0.0000,  59.0000,  21.0000,  43.0000,  54.0000,  25.0000],\n",
      "        [  0.0000,  56.0000,  24.0000,  39.0000,  50.0000,  61.0000],\n",
      "        [  0.0000,  53.0000,  67.0000,  52.0000,  42.0000,  56.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,  59.0000,  60.0000,  69.0000,  67.0000,  47.0000],\n",
      "        [  0.0000,  65.0000,  49.0000,  61.0000,  35.0000,  73.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,  46.0000,  55.0000,  62.0000,  67.0000,  36.0000],\n",
      "        [  0.0000,  76.0000,  21.0000,  55.0000,  28.0000,  68.0000],\n",
      "        [  0.0000,  36.0000,  35.0000,  35.0000,  32.0000,  25.0000],\n",
      "        [  0.0000,  71.0000,  49.0000,  25.0000,  34.0000,  46.0000],\n",
      "        [  0.0000,  70.0000,  23.0000,  59.0000,  51.0000,  60.0000],\n",
      "        [  0.0000,  41.0000,  49.0000,  20.0000,  67.0000,  70.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,  53.0000,  43.0000,  80.0000,  41.0000,  60.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,  41.0000,  38.0000,  67.0000,  47.0000,  65.0000],\n",
      "        [  0.0000,  53.0000,  35.0000,  35.0000,  52.0000,  61.0000],\n",
      "        [  0.0000,  80.0000,  41.0000,  48.0000,  46.0000,  56.0000],\n",
      "        [  0.0000,  30.0000,  30.0000,  51.0000,  57.0000,  41.0000],\n",
      "        [  0.0000,  40.0000,  62.0000,  41.0000,  60.0000,  47.0000],\n",
      "        [  0.0000,  51.0000,  32.0000,  38.0000,  46.0000,  32.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [100.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000],\n",
      "        [  0.0000,  38.0000,  40.0000,  34.0000,  72.0000,  57.0000],\n",
      "        [  0.0000,  48.0000,  54.0000,  29.0000,  53.0000,  27.0000],\n",
      "        [  0.0000,  38.0000,  32.0000,  35.0000,  40.0000,  31.0000]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_pr_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Multi-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0007,  0.0049,  0.0037, -0.0023, -0.0082, -0.0036]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 182.4380, Train Acc: 42.84%, Test Loss: 66.3190, Test Acc: 63.11%\n",
      "Epoch 2: Train Loss: 27.5916, Train Acc: 83.25%, Test Loss: 0.9740, Test Acc: 96.92%\n",
      "Epoch 3: Train Loss: 2.4533, Train Acc: 97.80%, Test Loss: 0.6422, Test Acc: 98.03%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 98:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc3 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)     # (batch_size, 16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_SM_net(\n",
       "    (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_SM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_SM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_sm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_sm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 156.8899, Train Acc: 66.82%, Test Loss: 17.2506, Test Acc: 89.24%\n",
      "Epoch 2: Train Loss: 8.9965, Train Acc: 93.89%, Test Loss: 7.8074, Test Acc: 96.61%\n",
      "Epoch 3: Train Loss: 3.0422, Train Acc: 96.85%, Test Loss: 1.3424, Test Acc: 97.11%\n",
      "Epoch 4: Train Loss: 2.5247, Train Acc: 97.22%, Test Loss: 1.4347, Test Acc: 97.68%\n",
      "Epoch 5: Train Loss: 1.9221, Train Acc: 97.47%, Test Loss: 1.3788, Test Acc: 97.49%\n",
      "Epoch 6: Train Loss: 1.3837, Train Acc: 97.68%, Test Loss: 0.6301, Test Acc: 97.82%\n",
      "Epoch 7: Train Loss: 1.5111, Train Acc: 97.77%, Test Loss: 0.5548, Test Acc: 98.12%\n"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_sm_action_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_sm_action_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 98:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_sm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_sm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_sm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Multi-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0613,  0.0038,  0.0910, -0.1583,  0.0962,  0.0991]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_MM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_MM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_mm_model = DQN_MM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_mm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_mm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_mm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_mm_model.features_extractor.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_mm_model.features_extractor.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_SM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_MM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_sm_model = DQN_SM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_sm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_sm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_sm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_sm_model.features_extractor.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_sm_model.features_extractor.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
