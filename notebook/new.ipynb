{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swkim/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from kube_sim_gym.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rl_model(scenario_file, rl_model):\n",
    "\n",
    "    test_env1 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "    test_env2 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "\n",
    "    # RL Scheduler\n",
    "    rl_model.set_env(test_env1)\n",
    "\n",
    "    # Default Scheduler\n",
    "    from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "    default_scheduler = SimHrScheduler(test_env2, 'default.py')\n",
    "\n",
    "\n",
    "    # Test the model\n",
    "    obs1 = test_env1.reset()\n",
    "    obs2 = test_env2.reset()\n",
    "    done1 = False\n",
    "    done2 = False\n",
    "    step1 = 0\n",
    "    step2 = 0\n",
    "    acc_rew1 = 0\n",
    "    acc_rew2 = 0\n",
    "\n",
    "    print(f\"Testing with {scenario_file} (my model vs. default)\")\n",
    "    while not done1 or not done2:\n",
    "        if not done1:\n",
    "            action1, _ = rl_model.predict(obs1)\n",
    "            # action1 = rl_scheduler.decision(test_env1)\n",
    "            obs1, reward1, done1, _ = test_env1.step(action1)\n",
    "            step1 += 1\n",
    "            acc_rew1 += reward1\n",
    "        if not done2:\n",
    "            action2 = default_scheduler.decision(test_env2)\n",
    "            obs2, reward2, done2, _ = test_env2.step(action2)\n",
    "            step2 += 1\n",
    "            acc_rew2 += reward2\n",
    "\n",
    "    acc_rew1 = round(acc_rew1, 2)\n",
    "    acc_rew2 = round(acc_rew2, 2)\n",
    "\n",
    "    print(f\"Test result(reward): {acc_rew1} vs. {acc_rew2}\")\n",
    "    print(f\"Test result(step): {step1} vs. {step2}\")\n",
    "\n",
    "    return acc_rew1, acc_rew2, step1, step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from notebook.net_arch import *\n",
    "import glob\n",
    "\n",
    "def train_rl_model(json_tracker_fname):\n",
    "\n",
    "    date = datetime(1992, 7, 5, 8, 33)\n",
    "    date = date.strftime(\"%m%d%Y%H%M\")\n",
    "\n",
    "    log_name = json_tracker_fname.split('.')[0]\n",
    "    log_path = f'training/log/{log_name}'\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "\n",
    "    logger = configure(log_path, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # Load the json tracker\n",
    "    import json\n",
    "    with open(f'training/{json_tracker_fname}', 'r') as f:\n",
    "        json_tracker = json.load(f)\n",
    "\n",
    "    last_idx = json_tracker['last_idx']\n",
    "    learning_steps = json_tracker['learning_steps']\n",
    "    model_type = json_tracker['model_type']\n",
    "    reward_file = json_tracker['reward_file']\n",
    "    model_fname = json_tracker['model_fname']\n",
    "\n",
    "    # Environment\n",
    "    envs = []\n",
    "    for i in range(1, 50):\n",
    "        env = gym.make('SimKubeEnv-v0', reward_file=reward_file, scenario_file=f'trace2017_100_{i}.csv')\n",
    "        envs.append(env)\n",
    "\n",
    "    current_idx = last_idx + 1 # -1 as default\n",
    "\n",
    "    # Model type : DQN or PPO\n",
    "    if model_type == 'DQN':\n",
    "        model = sb3.DQN\n",
    "    elif model_type == 'PPO':\n",
    "        model = sb3.PPO\n",
    "    else:\n",
    "        print(f\"Unknown model type: {model_type}\")\n",
    "        return\n",
    "    \n",
    "    model_fpath = f'net_arch/{model_fname}.zip'\n",
    "\n",
    "    # Check if the model exists\n",
    "    # Load Model\n",
    "    if os.path.exists(model_fpath):\n",
    "        print(f\"Loading the model from {model_fname}\")\n",
    "        model = model.load(model_fpath)\n",
    "    else: # Error\n",
    "        print(f\"Model file does not exist: {model_fname}\")\n",
    "        return\n",
    "    \n",
    "    # If last_idx is not -1 and there's a model trained in training/model, then load the model\n",
    "    if last_idx != -1 and glob.glob(f'training/model/{model_fname}_*'):\n",
    "        # Load the model with the latest date\n",
    "        model_fpaths = glob.glob(f'training/model/{model_fname}_*')\n",
    "        model_fpaths.sort()\n",
    "        model_fpath = model_fpaths[-1]\n",
    "        print(f\"Loading the model from {model_fpath}\")\n",
    "    \n",
    "    # Save the model, append _{date} to the model name\n",
    "    trained_model_fname = f'{model_fname}_{date}'\n",
    "    trained_model_fpath = f'training/model/{trained_model_fname}'\n",
    "\n",
    "    # Set logger\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    # Train the model\n",
    "    while current_idx < 20: # Target training steps (Can be changed!)\n",
    "        print(f\"Training with {current_idx}th trace\")\n",
    "\n",
    "        # Test the model first\n",
    "        a1, a2, a3, a4 = test_rl_model('scenario-5l-5m-1000p-10m_unbalanced.csv', model)\n",
    "        b1, b2, b3, b4 = test_rl_model('scenario-10l-3m-1000p-10m_unbalanced.csv', model)\n",
    "        c1, c2, c3, c4 = test_rl_model('scenario-3l-10m-1000p-10m_unbalanced.csv', model)\n",
    "\n",
    "        with open(f'training/log/{log_name}/test_result.txt', 'a') as f:\n",
    "            f.write(f\"{current_idx},{a1},{a2},{a3},{a4},{b1},{b2},{b3},{b4},{c1},{c2},{c3},{c4}\\n\")\n",
    "\n",
    "        env = envs[current_idx]\n",
    "        model.set_env(env)\n",
    "        model.learn(total_timesteps=learning_steps)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(trained_model_fpath)\n",
    "\n",
    "        # Update the json tracker\n",
    "        json_tracker['last_idx'] = current_idx\n",
    "        with open(f'training/{json_tracker_fname}', 'w') as f:\n",
    "            json.dump(json_tracker, f)\n",
    "\n",
    "        current_idx += 1\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pr_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, train=True):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        # Drop the row which has 0 for the last -2, -3 columns\n",
    "        # self.data = self.data.drop(self.data[(self.data.iloc[:, -2] == 0) & (self.data.iloc[:, -3] == 0)].index)\n",
    "\n",
    "        if train:\n",
    "            self.data = self.data.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            self.data = self.data.drop(self.data.sample(frac=0.8, random_state=42).index)\n",
    "\n",
    "        self.data = self.transform(self.data)\n",
    "        self.input = self.data[:, :-6]\n",
    "        self.label = self.data[:, -6:]\n",
    "\n",
    "    def transform(self, data):\n",
    "        return torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic.csv\")\n",
    "train_dynamic_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_dynamic_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_dynamic_dataloader = DataLoader(train_dynamic_dataset, batch_size=64, shuffle=False)\n",
    "test_dynamic_dataloader = DataLoader(test_dynamic_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.3300, 0.8700, 0.3100, 0.3000, 0.4900, 0.8000, 0.6700, 0.5200, 0.1300,\n",
      "         0.1500, 0.2900, 0.0000],\n",
      "        [0.4100, 0.2400, 0.7900, 0.6800, 0.6000, 0.5300, 0.2100, 0.7600, 0.3700,\n",
      "         0.2900, 0.2300, 0.0000],\n",
      "        [1.0000, 0.9700, 0.9500, 0.8300, 0.9200, 0.8100, 0.8600, 0.7700, 0.8300,\n",
      "         0.9700, 0.0900, 0.2400],\n",
      "        [0.8200, 0.4400, 0.6100, 0.2400, 0.2200, 0.0400, 0.8700, 0.0900, 0.8300,\n",
      "         0.8200, 0.0600, 0.0200],\n",
      "        [0.8600, 0.5200, 0.5500, 0.8400, 0.7100, 0.9900, 0.8000, 0.5700, 0.3200,\n",
      "         0.1200, 0.0500, 0.0300],\n",
      "        [0.0400, 0.4100, 0.4300, 0.6700, 0.6200, 0.6900, 0.5100, 0.6800, 0.1300,\n",
      "         0.9300, 0.2800, 0.1300],\n",
      "        [0.0300, 0.5900, 0.4200, 0.4800, 0.4400, 0.1900, 0.8900, 0.6700, 0.1400,\n",
      "         0.5000, 0.2000, 0.1000],\n",
      "        [0.6600, 0.3000, 0.1100, 0.7900, 0.9600, 0.5200, 0.0200, 0.4300, 0.7100,\n",
      "         0.8500, 0.1500, 0.0100],\n",
      "        [0.8400, 0.5300, 0.5300, 0.6200, 0.2200, 0.7200, 0.8100, 0.6600, 0.0800,\n",
      "         0.3500, 0.0600, 0.2500],\n",
      "        [0.9700, 0.9400, 0.9300, 0.9700, 0.8400, 0.9300, 0.9700, 0.9900, 0.7100,\n",
      "         0.7200, 0.2800, 0.0400],\n",
      "        [0.8800, 0.9700, 0.8200, 0.7200, 0.8500, 0.8200, 0.9200, 0.9600, 0.8300,\n",
      "         0.7300, 0.0100, 0.2300],\n",
      "        [0.7900, 0.9400, 0.4400, 0.6300, 0.8400, 0.0800, 0.1300, 0.4400, 0.3800,\n",
      "         0.5400, 0.0200, 0.2200],\n",
      "        [0.0500, 0.8900, 0.1200, 0.9900, 0.6900, 0.2300, 0.1600, 0.6700, 0.0400,\n",
      "         0.8900, 0.1900, 0.2400],\n",
      "        [0.4400, 0.1700, 0.1700, 0.1900, 0.8500, 0.1300, 0.2600, 0.7400, 0.2600,\n",
      "         0.7600, 0.1100, 0.1800],\n",
      "        [0.4400, 0.8900, 0.1100, 0.9400, 0.8500, 0.5400, 0.1100, 0.8400, 0.9700,\n",
      "         0.4800, 0.2700, 0.0800],\n",
      "        [0.2100, 0.7800, 0.8400, 0.5000, 0.6300, 0.0300, 0.7200, 0.3400, 0.5300,\n",
      "         0.7600, 0.0200, 0.0700],\n",
      "        [0.3900, 0.0300, 0.5800, 0.7500, 0.3300, 0.9200, 0.9500, 0.9900, 0.5700,\n",
      "         0.8700, 0.1200, 0.2500],\n",
      "        [0.6300, 0.5200, 0.9000, 0.1600, 0.5700, 0.3300, 0.5800, 0.3200, 0.2300,\n",
      "         0.3800, 0.1100, 0.1000],\n",
      "        [0.2500, 0.2200, 0.6400, 0.9300, 0.1400, 0.5600, 0.5200, 0.6800, 0.1800,\n",
      "         0.4500, 0.0100, 0.0600],\n",
      "        [0.7200, 0.0600, 0.0900, 0.9900, 0.7300, 0.5300, 0.9500, 0.9000, 0.3000,\n",
      "         0.4600, 0.0800, 0.1800],\n",
      "        [0.8600, 0.9200, 0.9800, 0.8000, 0.8500, 0.9700, 0.7300, 0.8800, 0.7600,\n",
      "         0.7400, 0.0800, 0.0800],\n",
      "        [0.5400, 0.7900, 0.9000, 0.8100, 0.5200, 0.1500, 0.8300, 0.4400, 0.9100,\n",
      "         0.3300, 0.0100, 0.1500],\n",
      "        [0.6100, 0.5700, 0.7900, 0.5300, 0.5200, 0.7900, 0.9600, 0.5400, 0.5200,\n",
      "         0.2300, 0.2100, 0.1600],\n",
      "        [0.6800, 0.8400, 0.8100, 0.8600, 0.5400, 0.2000, 0.2200, 0.0500, 0.4200,\n",
      "         0.8600, 0.2000, 0.0500],\n",
      "        [0.5100, 0.1200, 0.8500, 0.3600, 0.0500, 0.8700, 0.3400, 0.2600, 0.2500,\n",
      "         0.7900, 0.1100, 0.0300],\n",
      "        [0.3900, 0.4500, 0.5200, 0.9200, 0.4300, 0.8000, 0.0300, 0.5500, 0.8500,\n",
      "         0.3500, 0.0200, 0.2700],\n",
      "        [0.9700, 0.9100, 0.9000, 0.9300, 0.8900, 0.8200, 0.8600, 0.8900, 0.7200,\n",
      "         0.8600, 0.1600, 0.0500],\n",
      "        [0.5300, 0.6400, 0.6300, 0.8400, 0.8200, 0.1200, 0.9400, 0.5100, 0.9900,\n",
      "         0.9900, 0.1400, 0.0500],\n",
      "        [0.3700, 0.1400, 0.1600, 0.1600, 0.7700, 0.7200, 0.3200, 0.1200, 0.6300,\n",
      "         0.3100, 0.0200, 0.1700],\n",
      "        [0.8400, 0.4500, 0.0200, 0.6900, 0.6200, 0.0100, 0.0300, 0.5500, 0.1800,\n",
      "         0.8700, 0.2700, 0.1900],\n",
      "        [0.3100, 0.6200, 0.5500, 0.1700, 0.2100, 0.5300, 0.3900, 0.1800, 0.1200,\n",
      "         0.0800, 0.2400, 0.1000],\n",
      "        [0.9100, 0.7200, 0.9800, 0.7500, 0.8600, 0.8600, 0.7200, 0.7400, 0.7400,\n",
      "         0.7600, 0.0300, 0.2900],\n",
      "        [0.0300, 0.0600, 0.8200, 0.5700, 0.2700, 0.3400, 0.7000, 0.8300, 0.8900,\n",
      "         0.2800, 0.0800, 0.0600],\n",
      "        [0.9900, 0.9800, 0.8800, 0.7400, 0.7900, 0.7100, 0.9400, 0.9600, 0.8300,\n",
      "         0.9200, 0.0300, 0.0700],\n",
      "        [0.9900, 0.9600, 0.8700, 0.7300, 0.7900, 0.7900, 0.7100, 0.9800, 0.9400,\n",
      "         0.9800, 0.0800, 0.1800],\n",
      "        [0.2200, 0.3200, 0.1500, 0.0100, 0.3500, 0.7900, 0.8300, 0.5100, 0.9300,\n",
      "         0.3200, 0.2800, 0.2300],\n",
      "        [0.1200, 0.6300, 0.9000, 0.6400, 0.1800, 0.8300, 0.8300, 0.1200, 0.7300,\n",
      "         0.4300, 0.1500, 0.1300],\n",
      "        [0.5500, 0.5700, 0.0500, 1.0000, 0.5800, 0.4200, 0.5700, 0.2600, 0.9200,\n",
      "         0.7300, 0.0900, 0.1500],\n",
      "        [0.3500, 0.0500, 0.6200, 0.4000, 0.5000, 0.9400, 0.9300, 0.4900, 0.4900,\n",
      "         0.8700, 0.2900, 0.0100],\n",
      "        [0.3300, 0.6600, 0.1400, 0.5700, 0.4300, 0.8600, 0.5800, 0.1800, 0.1900,\n",
      "         0.8400, 0.0300, 0.1400],\n",
      "        [0.9400, 0.9700, 0.4800, 0.8200, 0.7300, 0.5800, 0.5400, 0.9000, 0.2400,\n",
      "         0.6700, 0.2700, 0.2300],\n",
      "        [0.9700, 0.8000, 0.8600, 0.7200, 0.9900, 0.8700, 0.9000, 0.9500, 0.9600,\n",
      "         0.9400, 0.0700, 0.2200],\n",
      "        [0.2100, 0.4300, 0.4600, 0.3700, 0.9700, 0.4600, 0.2900, 0.8400, 0.0100,\n",
      "         0.6600, 0.2000, 0.2400],\n",
      "        [0.9900, 0.7000, 0.7800, 0.7700, 0.7900, 0.7700, 0.8500, 0.8700, 0.7200,\n",
      "         0.9000, 0.2300, 0.2100],\n",
      "        [0.1100, 0.7600, 0.3000, 0.4100, 0.6500, 0.3300, 0.7600, 0.0500, 0.4800,\n",
      "         0.4900, 0.1100, 0.0300],\n",
      "        [0.2100, 0.8200, 0.3300, 0.6900, 0.3100, 0.0500, 0.4800, 0.8100, 0.1200,\n",
      "         0.2800, 0.2900, 0.1900],\n",
      "        [0.8200, 0.8100, 0.7600, 1.0000, 0.7600, 0.9600, 0.7700, 0.8800, 0.9800,\n",
      "         0.9100, 0.2600, 0.2900],\n",
      "        [0.5800, 0.9600, 0.7100, 0.6500, 0.4700, 0.0300, 0.9300, 0.9000, 0.5300,\n",
      "         0.9000, 0.1400, 0.1200],\n",
      "        [0.9800, 0.3600, 0.4100, 0.3300, 0.9800, 0.6900, 0.4400, 0.7100, 0.3100,\n",
      "         0.4700, 0.3000, 0.2000],\n",
      "        [0.9100, 0.9000, 0.9300, 0.8700, 0.9400, 0.7100, 0.8300, 0.8800, 0.9900,\n",
      "         0.8700, 0.2200, 0.2800],\n",
      "        [0.3000, 0.1600, 0.9800, 0.1000, 0.9000, 0.0400, 0.6800, 0.5000, 0.7900,\n",
      "         0.6500, 0.2500, 0.1100],\n",
      "        [0.5700, 0.6100, 0.3600, 0.8100, 0.1200, 0.0200, 0.5500, 0.6400, 0.6900,\n",
      "         0.2400, 0.2700, 0.0400],\n",
      "        [0.9300, 0.7400, 0.8100, 0.5300, 0.4700, 0.7400, 0.1000, 0.6000, 0.9400,\n",
      "         0.7500, 0.2900, 0.0100],\n",
      "        [0.0400, 0.5600, 0.1900, 0.5800, 0.8100, 0.8700, 1.0000, 0.8500, 0.7400,\n",
      "         0.0300, 0.2300, 0.2500],\n",
      "        [0.9100, 0.8900, 0.6000, 0.8300, 0.9000, 0.6900, 0.6900, 0.5000, 0.8800,\n",
      "         0.7500, 0.0600, 0.1200],\n",
      "        [0.8300, 0.7900, 0.8700, 0.7200, 0.7900, 0.8100, 0.8200, 0.8600, 0.7600,\n",
      "         0.9600, 0.0000, 0.2300],\n",
      "        [0.4100, 0.9000, 0.1600, 0.1800, 0.2200, 0.4000, 0.8000, 0.1300, 0.2100,\n",
      "         0.4700, 0.2300, 0.2500],\n",
      "        [0.8400, 0.7900, 0.9100, 0.8100, 0.8200, 1.0000, 0.9000, 0.9700, 0.9700,\n",
      "         0.9600, 0.1700, 0.0600],\n",
      "        [0.7300, 0.1300, 0.7200, 0.8200, 0.5400, 0.6700, 0.7600, 0.8500, 0.3100,\n",
      "         0.1800, 0.0900, 0.1100],\n",
      "        [0.0700, 0.4300, 0.4500, 0.6500, 0.2800, 0.5400, 0.4800, 0.2500, 0.7700,\n",
      "         0.3200, 0.1300, 0.0600],\n",
      "        [0.8900, 0.1800, 0.9500, 0.9100, 0.7700, 0.3900, 0.4200, 0.8500, 0.0200,\n",
      "         0.4700, 0.0100, 0.2000],\n",
      "        [0.7200, 0.6000, 0.5900, 0.6600, 0.1300, 0.9100, 0.5900, 0.1100, 0.5400,\n",
      "         0.9300, 0.1900, 0.1800],\n",
      "        [0.9800, 0.9500, 0.9200, 0.7200, 0.7700, 0.9200, 0.7700, 0.7600, 0.9300,\n",
      "         0.9400, 0.2000, 0.1100],\n",
      "        [0.0200, 0.5500, 0.1400, 0.3800, 0.6400, 0.6700, 0.6900, 0.5700, 0.0900,\n",
      "         0.9600, 0.1600, 0.1000]])\n",
      "labels: tensor([[-0.5000,  0.1134, -0.2169, -0.0123, -0.4797,  0.5000],\n",
      "        [-0.5000, -0.0273, -1.0000, -0.1437,  1.0000,  0.5000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.1083, -0.0693,  0.5000, -0.1008, -0.1257],\n",
      "        [-0.5000, -0.1089, -0.0573, -1.0000, -0.1032,  0.5000],\n",
      "        [-0.5000,  0.5000, -0.4368, -0.6531, -0.4902, -1.0000],\n",
      "        [-0.5000,  1.0000,  0.5000,  0.0387, -1.0000,  1.0000],\n",
      "        [-0.5000, -0.3261, -0.0345, -1.0000,  0.5000, -0.1857],\n",
      "        [-0.5000, -0.1173, -0.3930, -0.4353, -0.2781,  0.5000],\n",
      "        [-0.5000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5000],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000, -1.0000, -0.0159],\n",
      "        [-0.5000, -1.0000, -0.0996,  1.0000,  0.5000, -0.0576],\n",
      "        [-0.5000, -1.0000, -1.0000,  1.0000,  0.5000, -1.0000],\n",
      "        [-0.5000,  0.0174,  0.5000, -0.0672, -0.2217, -0.2280],\n",
      "        [-0.5000,  0.5000, -1.0000, -1.0000,  0.1680, -1.0000],\n",
      "        [-0.5000,  0.0153,  1.0000,  1.0000,  1.0000,  0.5000],\n",
      "        [-0.5000,  0.5000, -0.5676, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.2043, -1.0000, -0.1017, -0.0996,  0.5000],\n",
      "        [-0.5000,  0.5000, -0.1557, -0.0864, -0.1158, -0.0708],\n",
      "        [-0.5000,  1.0000, -1.0000,  0.0315, -1.0000,  0.5000],\n",
      "        [-0.5000, -0.1617, -1.0000, -1.0000, -0.0681,  0.5000],\n",
      "        [-0.5000, -0.3909, -0.3015,  0.5000, -0.1194, -0.0828],\n",
      "        [-0.5000,  0.5000, -0.0972,  0.0300, -1.0000,  0.2706],\n",
      "        [-0.5000, -0.1647, -1.0000, -0.2190,  0.5000, -0.0249],\n",
      "        [-0.5000, -0.0321, -0.1233,  1.0000,  0.5000,  0.0864],\n",
      "        [-0.5000,  0.5000, -1.0000, -1.0000, -0.0600,  1.0000],\n",
      "        [-0.5000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5000],\n",
      "        [-0.5000,  0.5000, -0.0753, -0.2007, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.1776,  0.5000, -0.1989,  1.0000,  0.0867],\n",
      "        [-0.5000, -1.0000, -0.0456, -0.1407,  0.5000, -1.0000],\n",
      "        [-0.5000, -0.1695, -0.3882, -0.0570, -0.2829,  0.5000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000, -0.2097, -0.0759, -0.2031, -0.1797],\n",
      "        [-0.5000, -1.0000, -0.0363,  0.5000, -1.0000, -0.1383],\n",
      "        [-0.5000, -1.0000,  1.0000,  0.5000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.1677,  0.5000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000, -1.0000, -0.0765, -0.0429,  0.5000],\n",
      "        [-0.5000,  0.5000, -1.0000,  1.0000,  1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000, -0.2433,  0.1770, -1.0000,  0.1869],\n",
      "        [-0.5000, -0.2820,  0.5000, -0.1257,  1.0000, -0.0924],\n",
      "        [-0.5000, -1.0000, -1.0000,  0.5000, -1.0000,  1.0000],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.0114,  0.5000, -1.0000, -1.0000, -0.1014],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.0318,  0.5000, -0.1863, -0.1920, -0.1416],\n",
      "        [-0.5000, -1.0000, -0.4170, -0.2718, -0.5859,  0.5000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000, -0.2619,  0.5000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000, -0.1587,  0.0774],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000, -1.0000, -1.0000, -0.3750, -1.0000],\n",
      "        [-0.5000, -0.3915, -0.0297,  0.5000, -0.3210, -0.4824],\n",
      "        [-0.5000, -1.0000, -1.0000, -0.2577,  0.5000, -1.0000],\n",
      "        [-0.5000,  0.0726,  0.5000, -1.0000, -1.0000,  1.0000],\n",
      "        [-0.5000, -1.0000, -0.2301, -0.1770,  0.5000, -0.2022],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000,  0.5000, -0.1605, -1.0000, -0.1899],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.1287, -0.2961, -0.2106, -0.3132,  0.5000],\n",
      "        [-0.5000,  1.0000, -0.0450,  0.0774,  0.5000, -0.1059],\n",
      "        [-0.5000,  1.0000, -1.0000,  1.0000, -1.0000,  0.5000],\n",
      "        [-0.5000, -0.0528,  0.5000, -1.0000,  0.2073, -1.0000],\n",
      "        [-0.5000, -1.0000, -1.0000, -1.0000,  0.5000, -1.0000],\n",
      "        [-0.5000, -0.0144,  0.5000, -0.2895, -0.3093, -1.0000]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dynamic_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic2.csv\")\n",
    "train_dynamic2_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_dynamic2_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_dynamic2_dataloader = DataLoader(train_dynamic2_dataset, batch_size=64, shuffle=False)\n",
    "test_dynamic2_dataloader = DataLoader(test_dynamic2_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.9800, 0.8100, 0.7200, 0.7700, 0.9000, 0.7900, 0.9100, 0.8700, 0.9100,\n",
      "         0.9100, 0.0700, 0.0100],\n",
      "        [0.6600, 0.0400, 0.4300, 0.9800, 0.7100, 0.3500, 0.0800, 0.1700, 0.8300,\n",
      "         0.1400, 0.1100, 0.1700],\n",
      "        [0.7500, 0.9400, 0.9200, 0.8700, 0.9100, 0.8800, 0.9600, 0.8100, 0.9700,\n",
      "         0.9100, 0.1900, 0.3000],\n",
      "        [0.8800, 0.4500, 0.5500, 0.2100, 0.2000, 0.0700, 0.8300, 0.3400, 0.3900,\n",
      "         0.4700, 0.0200, 0.0200],\n",
      "        [0.9200, 0.8200, 0.4700, 0.9400, 0.1300, 0.7900, 0.5300, 0.8500, 0.4400,\n",
      "         0.5500, 0.1700, 0.1900],\n",
      "        [0.4300, 0.1600, 0.2300, 0.8100, 0.4400, 0.2800, 0.4100, 0.7900, 0.7600,\n",
      "         0.0800, 0.2100, 0.1800],\n",
      "        [0.7300, 0.9600, 0.7900, 0.7600, 0.7800, 0.7600, 0.9500, 0.9400, 0.8500,\n",
      "         0.8800, 0.0400, 0.2100],\n",
      "        [0.5500, 0.4700, 1.0000, 0.8600, 0.1000, 0.1400, 0.1900, 0.5500, 0.7500,\n",
      "         0.7200, 0.1800, 0.0200],\n",
      "        [0.8900, 0.3600, 0.6900, 0.3900, 0.1700, 0.5000, 0.7300, 0.8800, 0.7600,\n",
      "         0.5700, 0.1600, 0.1000],\n",
      "        [0.1400, 0.7400, 0.2700, 0.9400, 0.6100, 0.2000, 0.1200, 0.9100, 0.6800,\n",
      "         0.8400, 0.0500, 0.1300],\n",
      "        [0.3200, 0.2000, 0.7300, 0.5500, 0.3000, 0.0200, 0.2900, 0.6100, 0.8000,\n",
      "         0.5800, 0.2300, 0.2900],\n",
      "        [0.3300, 0.4300, 0.6000, 0.4900, 0.9000, 0.1900, 0.3800, 0.2100, 0.9200,\n",
      "         0.4300, 0.2400, 0.2500],\n",
      "        [0.2300, 0.6800, 0.0100, 0.3900, 0.9700, 0.3800, 0.6800, 0.4500, 0.9600,\n",
      "         0.5500, 0.0900, 0.1900],\n",
      "        [0.2100, 0.9800, 0.8800, 0.7700, 0.6100, 0.6300, 0.4900, 0.7000, 0.5700,\n",
      "         0.8900, 0.0800, 0.2000],\n",
      "        [0.8200, 0.5900, 0.8900, 0.6600, 0.0900, 0.0500, 0.2700, 0.2700, 0.7300,\n",
      "         0.8000, 0.1400, 0.2700],\n",
      "        [0.8700, 0.9300, 0.9600, 0.7300, 0.7100, 0.7500, 0.7100, 1.0000, 0.9400,\n",
      "         0.7800, 0.2300, 0.0800],\n",
      "        [0.9300, 0.2400, 0.5000, 0.2600, 0.5500, 0.5100, 0.9400, 0.1000, 0.7200,\n",
      "         0.0300, 0.0400, 0.2200],\n",
      "        [0.2600, 0.8900, 0.2300, 0.0900, 0.6500, 0.2800, 0.5100, 0.6400, 0.2700,\n",
      "         0.7400, 0.0300, 0.0400],\n",
      "        [0.4100, 0.1800, 0.0400, 0.9600, 0.6800, 0.3200, 0.4300, 0.0300, 0.8900,\n",
      "         0.7100, 0.1100, 0.1600],\n",
      "        [0.8500, 0.3900, 0.6500, 0.9100, 0.8100, 0.2300, 0.4800, 0.4100, 0.9200,\n",
      "         1.0000, 0.1400, 0.0500],\n",
      "        [0.7600, 0.9300, 0.9500, 0.8800, 0.8500, 0.9100, 0.9000, 0.8600, 0.7100,\n",
      "         0.7600, 0.2300, 0.2900],\n",
      "        [0.5500, 0.6300, 0.2900, 0.5200, 0.0700, 0.6000, 0.3100, 0.2900, 0.4100,\n",
      "         0.0100, 0.0400, 0.0600],\n",
      "        [0.4300, 0.2700, 0.6400, 0.8800, 0.7600, 0.8500, 0.8100, 0.2400, 0.0300,\n",
      "         0.2200, 0.0900, 0.2000],\n",
      "        [0.5900, 0.6200, 0.9200, 0.4600, 0.8300, 0.5500, 0.9800, 0.8300, 0.9100,\n",
      "         0.2100, 0.1500, 0.0500],\n",
      "        [0.9500, 0.8500, 0.9400, 0.9400, 0.9700, 0.9200, 0.8500, 0.8300, 0.8000,\n",
      "         0.7400, 0.0600, 0.0300],\n",
      "        [0.6800, 0.8900, 0.2700, 0.8600, 0.6300, 0.5600, 0.8000, 0.6100, 0.9900,\n",
      "         0.5100, 0.1200, 0.2000],\n",
      "        [0.8900, 0.9400, 0.6900, 1.0000, 0.3600, 0.0500, 0.8400, 0.3500, 0.3200,\n",
      "         0.4900, 0.0900, 0.2200],\n",
      "        [0.8400, 0.8200, 0.2500, 0.4400, 0.9300, 0.7600, 0.7100, 0.5600, 0.8800,\n",
      "         0.6100, 0.2000, 0.0300],\n",
      "        [0.9300, 0.7600, 0.8000, 0.9600, 0.7800, 0.8700, 0.7400, 0.8200, 0.9300,\n",
      "         0.8300, 0.2900, 0.0500],\n",
      "        [0.7600, 0.9400, 0.9700, 0.7100, 0.9300, 0.7900, 0.7500, 0.9800, 0.7200,\n",
      "         0.8700, 0.0700, 0.2700],\n",
      "        [0.7200, 0.8200, 0.3400, 0.8100, 0.6300, 0.9500, 0.3000, 0.3200, 0.3200,\n",
      "         0.5800, 0.2500, 0.2800],\n",
      "        [0.6100, 0.0300, 0.7600, 0.2900, 0.9500, 0.9800, 0.2900, 0.7600, 0.2900,\n",
      "         0.7000, 0.2900, 0.2100],\n",
      "        [0.8100, 1.0000, 0.7300, 0.9800, 0.7600, 0.9000, 0.7700, 0.9300, 0.9100,\n",
      "         0.8800, 0.0400, 0.0700],\n",
      "        [0.6700, 0.8500, 0.8000, 0.7300, 0.5100, 0.2100, 0.5800, 0.0500, 0.8300,\n",
      "         0.9200, 0.1500, 0.0700],\n",
      "        [0.8700, 0.8600, 0.7100, 0.7600, 0.8300, 0.9200, 0.9900, 0.8400, 0.8100,\n",
      "         0.8400, 0.2300, 0.2800],\n",
      "        [0.8200, 0.8800, 0.7400, 0.9600, 0.7900, 0.8200, 0.9400, 0.9400, 0.9000,\n",
      "         0.7000, 0.1100, 0.1000],\n",
      "        [0.7900, 0.2500, 0.2000, 0.8100, 0.6500, 0.9100, 0.9300, 0.4600, 0.9300,\n",
      "         0.6700, 0.2700, 0.1100],\n",
      "        [0.8100, 0.4600, 0.4900, 0.5000, 0.6000, 0.4200, 0.9300, 0.2300, 0.0400,\n",
      "         0.6000, 0.1400, 0.2800],\n",
      "        [0.8900, 0.7700, 0.7900, 0.7200, 0.9300, 0.9800, 0.7400, 0.8600, 0.9500,\n",
      "         0.7700, 0.2200, 0.1700],\n",
      "        [0.6900, 0.5700, 0.9100, 0.0100, 0.2800, 0.8900, 0.3200, 0.2100, 0.1500,\n",
      "         0.2000, 0.0400, 0.1500],\n",
      "        [0.2500, 0.1700, 0.7100, 0.1100, 0.9000, 0.9500, 0.2000, 0.5700, 0.6400,\n",
      "         0.0100, 0.2600, 0.0200],\n",
      "        [0.5800, 0.1200, 0.4100, 0.1800, 0.8000, 0.4400, 0.1700, 0.8300, 0.6100,\n",
      "         0.1500, 0.2600, 0.0600],\n",
      "        [0.7500, 0.7300, 0.9300, 0.9700, 0.7000, 0.9300, 0.9200, 0.8800, 0.9600,\n",
      "         0.8300, 0.1600, 0.1000],\n",
      "        [0.9200, 0.3000, 0.6700, 0.6200, 0.2000, 0.3700, 0.3900, 0.2000, 0.7000,\n",
      "         0.1100, 0.2500, 0.2600],\n",
      "        [0.0100, 0.7200, 0.6200, 0.3300, 0.4600, 0.7900, 0.6000, 0.1100, 0.8100,\n",
      "         0.3600, 0.0700, 0.1600],\n",
      "        [0.9800, 0.0100, 0.0400, 0.7500, 0.0900, 0.5500, 0.3500, 0.5800, 0.4700,\n",
      "         0.4300, 0.0500, 0.2500],\n",
      "        [0.9100, 0.9500, 0.8700, 0.8000, 0.8100, 0.9000, 0.8800, 0.9100, 0.7400,\n",
      "         0.7300, 0.0400, 0.0400],\n",
      "        [0.4800, 0.1600, 0.7900, 0.9100, 0.4200, 0.5800, 0.0300, 0.6900, 0.3300,\n",
      "         0.6800, 0.0600, 0.0600],\n",
      "        [0.2700, 0.1600, 0.3800, 0.4400, 0.2500, 0.7600, 0.6900, 0.7200, 0.6100,\n",
      "         0.2200, 0.1800, 0.1600],\n",
      "        [0.1900, 0.8500, 0.9500, 0.2700, 0.2000, 0.8700, 0.7300, 0.3300, 0.4600,\n",
      "         0.3300, 0.1400, 0.1700],\n",
      "        [0.1000, 0.4000, 0.6900, 0.5600, 0.3700, 0.1800, 0.7400, 0.0800, 0.8700,\n",
      "         0.7000, 0.0500, 0.0200],\n",
      "        [0.6500, 0.9300, 0.8300, 0.6900, 0.7500, 0.6500, 0.5300, 0.8800, 0.9700,\n",
      "         0.4400, 0.0200, 0.0900],\n",
      "        [0.8700, 0.3300, 0.3300, 0.8400, 0.8700, 0.6200, 0.3400, 0.5900, 0.6000,\n",
      "         0.1100, 0.0800, 0.1900],\n",
      "        [0.7400, 0.7200, 0.7400, 0.9900, 0.8200, 0.7100, 0.9200, 0.8800, 0.7700,\n",
      "         0.8000, 0.1400, 0.2700],\n",
      "        [0.9100, 0.7700, 0.8900, 0.8500, 0.7600, 0.9300, 0.7700, 0.9100, 0.9700,\n",
      "         0.7300, 0.2300, 0.0400],\n",
      "        [0.9700, 0.8200, 0.9900, 0.7400, 0.2900, 0.5500, 0.0600, 0.6700, 0.5900,\n",
      "         0.9400, 0.2600, 0.1600],\n",
      "        [0.3200, 0.3700, 0.7800, 0.5000, 0.7800, 0.4600, 0.6100, 0.7500, 0.3900,\n",
      "         0.6700, 0.0600, 0.2200],\n",
      "        [0.2000, 0.5400, 0.4400, 0.7500, 0.1900, 0.7200, 0.9900, 0.7200, 0.3700,\n",
      "         0.3200, 0.0500, 0.0300],\n",
      "        [0.5600, 0.2800, 0.3700, 0.8100, 0.1400, 0.6800, 0.6300, 0.2700, 0.4200,\n",
      "         0.2200, 0.2400, 0.0100],\n",
      "        [0.7600, 0.2700, 0.1000, 0.6600, 0.0700, 0.5500, 1.0000, 0.8900, 0.9700,\n",
      "         0.8000, 0.0200, 0.0300],\n",
      "        [0.2700, 0.9500, 0.3700, 0.6200, 0.4600, 0.4500, 0.5800, 0.2400, 0.1200,\n",
      "         0.7700, 0.3000, 0.2200],\n",
      "        [0.4000, 0.4400, 0.2400, 0.3500, 0.5000, 0.1300, 0.8700, 0.8300, 0.1400,\n",
      "         0.9700, 0.1700, 0.0000],\n",
      "        [0.2300, 0.4200, 0.0300, 0.2100, 0.5000, 0.6600, 0.3800, 0.9400, 0.8100,\n",
      "         0.5100, 0.1000, 0.1600],\n",
      "        [0.6400, 0.8900, 0.9100, 0.1900, 0.5800, 0.2300, 0.9800, 0.2900, 0.7800,\n",
      "         0.9000, 0.0600, 0.0700]])\n",
      "labels: tensor([[-0.0300, -0.5300,  0.0000, -0.0500, -0.0600, -0.0700],\n",
      "        [-0.0600, -0.0100, -0.5600, -0.0500,  0.0000, -0.0300],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0200, -0.0200, -0.0200,  0.0000, -0.0200, -0.0300],\n",
      "        [-0.0900, -0.5900, -0.5900, -0.0400, -0.5900,  0.0000],\n",
      "        [-0.0100,  0.0200,  0.0200,  0.0000, -0.0300, -0.0300],\n",
      "        [-0.0100, -0.5100,  0.0000,  0.0000, -0.5100, -0.5100],\n",
      "        [-0.0400, -0.0600, -0.5400,  0.0000,  0.0400, -0.0900],\n",
      "        [-0.1000, -0.6000, -0.0700,  0.0000, -0.1100, -0.1000],\n",
      "        [-0.0800, -0.0700, -0.5800,  0.0000, -0.5800, -0.1000],\n",
      "        [-0.1000,  0.0000, -0.1600,  0.0600, -0.1100, -0.6000],\n",
      "        [-0.1000, -0.0700, -0.1300, -0.6000,  0.0000, -0.6000],\n",
      "        [-0.0500, -0.0900,  0.0000, -0.5500, -0.0100, -0.5500],\n",
      "        [-0.0400, -0.5400, -0.0200,  0.0000, -0.0100, -0.5400],\n",
      "        [-0.1400, -0.1600, -0.6400,  0.0000, -0.0900, -0.6400],\n",
      "        [-0.0500, -0.5500, -0.5500,  0.0000, -0.5500, -0.5500],\n",
      "        [ 0.0700,  0.1000,  0.1200,  0.0000,  0.1400,  0.1700],\n",
      "        [-0.0200, -0.0200,  0.0000, -0.0200, -0.0200, -0.0100],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.0300,  0.0200, -0.0800],\n",
      "        [-0.0500, -0.0600, -0.0100, -0.0600,  0.0000, -0.5500],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0100, -0.0400, -0.0200,  0.0000,  0.0000,  0.0200],\n",
      "        [-0.0700,  0.0000, -0.5700, -0.5700, -0.0100,  0.0000],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.0700, -0.5600, -0.5600],\n",
      "        [-0.0400, -0.5400, -0.0500, -0.5400, -0.0200,  0.0000],\n",
      "        [-0.0500, -0.5500, -0.5500,  0.0000, -0.0100, -0.5500],\n",
      "        [-0.1400, -0.6400, -0.6400,  0.0000, -0.0800, -0.1100],\n",
      "        [-0.1400, -0.6400,  0.0000, -0.6400, -0.1500, -0.6400],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.1400, -0.6400, -0.6400, -0.6400,  0.0000, -0.0800],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.5600,  0.0000,  0.0100],\n",
      "        [ 0.0000, -0.5000, -0.5000,  0.0100,  0.0000,  0.0000],\n",
      "        [-0.0400, -0.0500, -0.0900,  0.0000, -0.0100, -0.0800],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0200, -0.0300, -0.5200,  0.0000, -0.5200, -0.5200],\n",
      "        [-0.1400, -0.6400,  0.0000, -0.6400, -0.6400, -0.6400],\n",
      "        [ 0.0500,  0.0500,  0.0000,  0.0700, -0.4500,  0.0200],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0100, -0.0100,  0.0400, -0.5100,  0.0400,  0.0000],\n",
      "        [-0.0300,  0.0000, -0.0900, -0.5300,  0.1000, -0.0800],\n",
      "        [ 0.0000, -0.0400,  0.0000, -0.5000,  0.1200, -0.0500],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.5600, -0.5600, -0.5600],\n",
      "        [-0.0500, -0.5500, -0.1400,  0.0000,  0.0200, -0.0100],\n",
      "        [-0.0300, -0.0300,  0.0000, -0.0700,  0.0300, -0.0100],\n",
      "        [ 0.0100, -0.4900, -0.0600, -0.0200, -0.0400,  0.0000],\n",
      "        [-0.0300, -0.0400, -0.0300, -0.0300, -0.0400,  0.0000],\n",
      "        [-0.0300,  0.0000, -0.0400, -0.0200,  0.0000, -0.0100],\n",
      "        [-0.0900,  0.0000, -0.0600, -0.0700, -0.1500, -0.0700],\n",
      "        [-0.0600, -0.5600, -0.5600, -0.5600, -0.0200,  0.0000],\n",
      "        [-0.0100,  0.0300, -0.0200,  0.0000, -0.0200, -0.0300],\n",
      "        [-0.0300, -0.5300, -0.0100,  0.0000, -0.0300,  0.0100],\n",
      "        [ 0.0000,  0.0400, -0.5000,  0.0000,  0.0000,  0.1000],\n",
      "        [-0.0100,  0.0000, -0.5100,  0.0300, -0.5100, -0.5100],\n",
      "        [-0.0400, -0.5400, -0.5400,  0.0100,  0.0000, -0.5400],\n",
      "        [-0.1200, -0.6200, -0.6200,  0.0000,  0.0100, -0.6200],\n",
      "        [-0.0300,  0.0000,  0.0000,  0.0100, -0.1200, -0.0800],\n",
      "        [-0.0200,  0.0000, -0.0200, -0.0100, -0.5200,  0.0000],\n",
      "        [ 0.0500, -0.0400,  0.0900,  0.1700, -0.0500,  0.0000],\n",
      "        [-0.0100,  0.0000, -0.0100,  0.0000, -0.5100, -0.0200],\n",
      "        [ 0.0100, -0.4900,  0.0300,  0.0000,  0.0100,  0.1100],\n",
      "        [-0.0300, -0.0500,  0.0000, -0.0700, -0.5300,  0.0500],\n",
      "        [-0.0600, -0.0400,  0.0000, -0.0900, -0.5600, -0.0700],\n",
      "        [-0.0400, -0.0400, -0.0200,  0.0000, -0.5400, -0.0500]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dynamic2_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_default.csv\")\n",
    "train_default_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_default_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_default_dataloader = DataLoader(train_default_dataset, batch_size=64, shuffle=False)\n",
    "test_default_dataloader = DataLoader(test_default_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.8800, 0.8200, 0.4300, 0.1300, 0.7900, 0.1500, 0.4600, 0.7800, 0.8000,\n",
      "         0.4100, 0.2700, 0.0900],\n",
      "        [0.4500, 0.9100, 0.7200, 0.0900, 0.9700, 0.0700, 0.0300, 0.1500, 1.0000,\n",
      "         0.4300, 0.1400, 0.1300],\n",
      "        [0.9700, 0.9000, 0.8800, 0.9100, 0.9500, 0.9900, 0.9300, 0.7800, 0.8800,\n",
      "         0.9200, 0.1600, 0.1000],\n",
      "        [0.9200, 0.8700, 0.8100, 0.9100, 0.8500, 0.7700, 0.7200, 0.8900, 0.9600,\n",
      "         0.7800, 0.2400, 0.2800],\n",
      "        [0.9800, 0.7600, 0.8900, 0.7200, 0.9200, 0.7900, 0.9200, 0.8500, 0.8500,\n",
      "         0.8200, 0.2700, 0.1200],\n",
      "        [0.7000, 0.6200, 0.2500, 0.6000, 0.2400, 0.6400, 0.6900, 0.2700, 0.6400,\n",
      "         0.4700, 0.0400, 0.0200],\n",
      "        [0.7700, 0.9900, 0.2700, 0.4100, 0.5900, 0.0400, 0.5100, 0.3700, 0.4200,\n",
      "         0.3700, 0.2200, 0.2400],\n",
      "        [0.9600, 0.4700, 0.2600, 0.9000, 0.4800, 0.2400, 0.1700, 0.5400, 0.6200,\n",
      "         0.8000, 0.2800, 0.1000],\n",
      "        [0.6000, 0.3800, 0.4900, 0.6700, 0.6400, 0.7400, 0.1500, 0.2500, 0.1700,\n",
      "         0.4100, 0.2200, 0.0700],\n",
      "        [0.5900, 0.9900, 0.5900, 0.1400, 0.3700, 0.6600, 0.3300, 0.8700, 0.5900,\n",
      "         0.3100, 0.1800, 0.2700],\n",
      "        [0.9000, 0.7200, 0.5400, 0.4500, 0.2600, 0.0300, 0.4100, 0.8800, 0.8100,\n",
      "         0.2700, 0.0800, 0.0900],\n",
      "        [0.4600, 0.7000, 0.5100, 0.7000, 0.4100, 0.5400, 0.3700, 0.4600, 0.6700,\n",
      "         0.0600, 0.1100, 0.2700],\n",
      "        [0.8600, 0.8500, 0.0900, 0.9500, 0.2500, 0.1300, 0.5800, 0.4300, 0.8600,\n",
      "         0.5300, 0.0500, 0.0700],\n",
      "        [0.9100, 0.6600, 0.3300, 0.5600, 0.7500, 0.9100, 0.8900, 0.4400, 0.1600,\n",
      "         0.0300, 0.0800, 0.1600],\n",
      "        [0.6900, 0.2800, 0.6700, 0.8200, 0.4600, 0.7600, 0.8100, 0.6400, 0.2000,\n",
      "         0.7000, 0.1300, 0.0800],\n",
      "        [0.0900, 0.3700, 0.3100, 0.7800, 0.6700, 0.8600, 0.3700, 0.3800, 0.1100,\n",
      "         0.9000, 0.1700, 0.2800],\n",
      "        [0.3100, 0.7200, 0.9800, 0.2200, 0.4700, 0.8100, 0.6400, 0.0900, 0.4100,\n",
      "         0.5100, 0.0700, 0.1500],\n",
      "        [0.8300, 0.8400, 0.8200, 0.9600, 0.8500, 0.9200, 0.7400, 0.7000, 0.7900,\n",
      "         0.8600, 0.1900, 0.1000],\n",
      "        [1.0000, 0.3800, 0.3300, 0.6500, 0.1300, 0.3100, 0.9000, 0.4500, 0.5400,\n",
      "         0.8400, 0.2200, 0.2800],\n",
      "        [0.3100, 0.6600, 0.0200, 0.3300, 0.3700, 0.6900, 0.6300, 0.4000, 0.6700,\n",
      "         0.4400, 0.0200, 0.1200],\n",
      "        [0.7500, 0.7000, 0.7700, 0.9000, 0.8200, 0.7200, 0.8600, 0.7800, 0.9600,\n",
      "         0.7600, 0.0700, 0.2200],\n",
      "        [0.9800, 0.7400, 0.9200, 0.9200, 0.8600, 0.8000, 0.9000, 0.7400, 0.7700,\n",
      "         0.7100, 0.3000, 0.2900],\n",
      "        [0.8700, 0.7600, 0.9600, 0.8700, 0.8700, 0.7400, 0.9200, 0.9000, 0.7600,\n",
      "         0.9400, 0.2100, 0.1800],\n",
      "        [0.8400, 0.8300, 0.9400, 0.8100, 0.9400, 0.7100, 0.8300, 0.8500, 0.7600,\n",
      "         0.8500, 0.1400, 0.0600],\n",
      "        [0.5200, 0.8600, 0.7100, 0.8300, 0.7500, 0.4700, 0.5800, 0.4000, 0.7500,\n",
      "         0.7200, 0.0800, 0.1300],\n",
      "        [0.8800, 0.9000, 0.7500, 0.8400, 0.7700, 0.7100, 1.0000, 0.7300, 0.7200,\n",
      "         0.7000, 0.1800, 0.2500],\n",
      "        [0.3800, 0.9900, 0.7100, 0.1300, 0.9900, 0.9400, 0.7200, 0.7900, 0.5700,\n",
      "         0.0200, 0.1000, 0.2300],\n",
      "        [0.8800, 0.0800, 0.6700, 0.1900, 0.2700, 0.5100, 0.4200, 0.7000, 0.2800,\n",
      "         0.3000, 0.2700, 0.0900],\n",
      "        [0.0600, 0.8200, 0.8900, 0.4700, 0.5300, 0.5300, 0.8300, 0.5000, 0.8900,\n",
      "         0.8800, 0.0400, 0.2100],\n",
      "        [0.0900, 0.0300, 0.3100, 0.6600, 0.3800, 0.6600, 0.5000, 0.8200, 0.7300,\n",
      "         0.3300, 0.1100, 0.2900],\n",
      "        [0.2600, 0.4900, 0.9100, 0.0400, 0.3600, 0.4800, 0.6400, 0.9500, 0.8600,\n",
      "         0.5600, 0.1400, 0.1100],\n",
      "        [0.7900, 0.0600, 0.4900, 0.0700, 0.3200, 0.8000, 0.9600, 0.0700, 0.3100,\n",
      "         0.9100, 0.0500, 0.0000],\n",
      "        [0.9400, 0.9500, 0.8200, 0.7700, 0.8500, 0.8800, 0.7000, 0.8600, 0.8700,\n",
      "         0.9800, 0.2000, 0.2100],\n",
      "        [0.8100, 0.7400, 0.8100, 0.9200, 0.7500, 0.8400, 0.7300, 0.9700, 0.8400,\n",
      "         0.9300, 0.2200, 0.1900],\n",
      "        [0.0100, 0.6400, 0.3700, 0.3500, 0.2100, 0.0100, 0.2100, 0.9200, 0.3600,\n",
      "         0.3500, 0.1000, 0.2400],\n",
      "        [0.8200, 0.7000, 0.7300, 0.8300, 0.7900, 0.8800, 0.7100, 0.9000, 0.9500,\n",
      "         0.8300, 0.1700, 0.1200],\n",
      "        [0.8800, 0.8600, 0.7300, 0.8100, 0.8100, 0.9800, 0.7600, 0.8200, 0.7400,\n",
      "         0.7300, 0.0700, 0.1400],\n",
      "        [0.9800, 0.9300, 0.9400, 0.8000, 0.7700, 0.8900, 0.8400, 0.7200, 0.9600,\n",
      "         0.9900, 0.2200, 0.1800],\n",
      "        [0.7200, 0.9300, 0.9100, 0.9000, 0.9100, 0.9600, 0.8300, 0.9200, 0.8900,\n",
      "         0.8100, 0.2800, 0.1600],\n",
      "        [0.6200, 0.1600, 0.6400, 0.2600, 0.0800, 0.1100, 0.3400, 0.5800, 0.5600,\n",
      "         0.2400, 0.1500, 0.0200],\n",
      "        [0.7700, 0.4700, 0.2800, 0.0300, 0.9800, 0.8700, 0.2000, 0.6200, 0.7100,\n",
      "         0.6300, 0.0000, 0.1300],\n",
      "        [0.9400, 0.1300, 0.9900, 0.0600, 0.2000, 0.2400, 0.3800, 0.4600, 0.9300,\n",
      "         0.8300, 0.1400, 0.1100],\n",
      "        [0.3700, 0.1300, 0.3400, 0.6500, 0.6000, 0.6700, 0.4800, 0.7200, 0.4100,\n",
      "         0.9700, 0.0000, 0.0600],\n",
      "        [0.8900, 0.7100, 0.9700, 0.8800, 0.9000, 0.9300, 0.8000, 0.7600, 0.7300,\n",
      "         0.7700, 0.0000, 0.2300],\n",
      "        [0.7400, 0.8100, 0.7200, 0.4500, 0.5100, 0.9800, 0.8900, 0.0700, 0.3900,\n",
      "         0.2900, 0.2900, 0.0600],\n",
      "        [0.8600, 0.9400, 0.8500, 0.8100, 0.7500, 0.7500, 0.9000, 0.7500, 0.9400,\n",
      "         0.8200, 0.1900, 0.1000],\n",
      "        [0.8500, 0.9100, 0.9500, 0.6000, 0.7700, 0.5700, 0.6400, 0.8200, 0.0600,\n",
      "         0.3200, 0.2500, 0.1500],\n",
      "        [0.0300, 0.7000, 0.4900, 0.8000, 0.2000, 0.6800, 0.2500, 0.2000, 0.3200,\n",
      "         0.4000, 0.0100, 0.1500],\n",
      "        [0.7000, 0.7500, 0.7500, 0.7400, 0.7200, 0.8800, 0.7600, 0.8300, 0.9100,\n",
      "         0.9700, 0.1900, 0.1700],\n",
      "        [0.9000, 0.8300, 0.8200, 0.8600, 0.7900, 0.8100, 0.7000, 0.7800, 0.9600,\n",
      "         0.7200, 0.1300, 0.2600],\n",
      "        [0.9200, 0.6800, 0.2800, 0.5000, 0.9800, 0.1200, 0.1900, 0.6600, 0.5000,\n",
      "         0.4900, 0.2600, 0.2600],\n",
      "        [0.2000, 0.9300, 0.5000, 0.2400, 1.0000, 0.6100, 0.1300, 0.7900, 0.6200,\n",
      "         0.5700, 0.1200, 0.1900],\n",
      "        [0.9700, 0.8900, 0.7700, 0.7500, 0.8900, 0.8700, 0.9100, 0.8900, 0.9400,\n",
      "         0.9000, 0.0500, 0.0700],\n",
      "        [0.3200, 0.9200, 0.8100, 0.3700, 0.2400, 0.6300, 0.1100, 0.5000, 0.9600,\n",
      "         0.2400, 0.2900, 0.1600],\n",
      "        [0.4600, 0.9900, 0.7100, 0.5500, 0.0900, 0.9300, 0.6400, 0.4500, 0.9300,\n",
      "         0.8300, 0.0200, 0.2700],\n",
      "        [0.5800, 0.0500, 0.6100, 0.1500, 0.5000, 0.4500, 0.7400, 0.8800, 0.5600,\n",
      "         0.7800, 0.2500, 0.2000],\n",
      "        [0.5400, 0.9500, 0.5800, 0.3300, 0.4500, 0.6600, 0.2500, 0.9700, 0.8200,\n",
      "         0.0300, 0.2100, 0.0600],\n",
      "        [0.5700, 0.7800, 0.6600, 0.6900, 0.8000, 0.4200, 0.4000, 0.7000, 0.4800,\n",
      "         0.5000, 0.2000, 0.2900],\n",
      "        [0.6000, 0.0300, 0.9200, 0.1200, 0.2600, 0.9600, 0.7800, 0.7400, 0.4200,\n",
      "         0.1500, 0.1200, 0.0800],\n",
      "        [0.8800, 0.7300, 0.7000, 0.3800, 0.2200, 0.6200, 0.0100, 0.6800, 0.6600,\n",
      "         0.5000, 0.1700, 0.3000],\n",
      "        [0.8600, 0.7200, 0.1300, 0.7500, 0.2900, 0.0400, 0.7600, 0.5800, 0.3100,\n",
      "         0.1800, 0.1900, 0.0400],\n",
      "        [0.8600, 0.9200, 0.9800, 0.8000, 0.9500, 0.7300, 0.8100, 0.8900, 0.8600,\n",
      "         0.9800, 0.1100, 0.1000],\n",
      "        [1.0000, 0.1600, 0.0400, 0.6800, 0.5300, 0.8400, 0.1900, 0.1600, 0.3300,\n",
      "         0.4800, 0.2300, 0.0600],\n",
      "        [0.3700, 0.9000, 0.4300, 0.3600, 0.9900, 0.5800, 0.7200, 0.4600, 0.2100,\n",
      "         0.0800, 0.1300, 0.2900]])\n",
      "labels: tensor([[0.3950, 0.1500, 0.5400, 0.5300, 0.2000, 0.3950],\n",
      "        [0.2850, 0.3200, 0.4600, 0.4800, 0.7750, 0.2850],\n",
      "        [0.1000, 0.0650, 0.1050, 0.0300, 0.1450, 0.1000],\n",
      "        [0.1300, 0.1050, 0.1400, 0.1900, 0.1950, 0.1300],\n",
      "        [0.1650, 0.1300, 0.1950, 0.1450, 0.1150, 0.1650],\n",
      "        [0.4450, 0.3100, 0.5450, 0.5300, 0.4900, 0.4150],\n",
      "        [0.6050, 0.1200, 0.4300, 0.4550, 0.3300, 0.3750],\n",
      "        [0.2900, 0.2850, 0.2300, 0.4500, 0.4550, 0.1000],\n",
      "        [0.7100, 0.3650, 0.2750, 0.1650, 0.6550, 0.5650],\n",
      "        [0.5500, 0.2100, 0.4100, 0.2600, 0.4000, 0.3250],\n",
      "        [0.4600, 0.1050, 0.4200, 0.7700, 0.2700, 0.3750],\n",
      "        [0.6350, 0.2300, 0.2050, 0.3350, 0.3950, 0.4450],\n",
      "        [0.3050, 0.0850, 0.4800, 0.7500, 0.4350, 0.2450],\n",
      "        [0.9050, 0.0950, 0.4350, 0.1700, 0.2150, 0.7850],\n",
      "        [0.5500, 0.4100, 0.1500, 0.2850, 0.1700, 0.4450],\n",
      "        [0.4950, 0.5450, 0.4550, 0.2350, 0.4000, 0.4950],\n",
      "        [0.5400, 0.3750, 0.4000, 0.2500, 0.5250, 0.4300],\n",
      "        [0.1750, 0.1650, 0.1100, 0.1150, 0.1350, 0.0300],\n",
      "        [0.3100, 0.3100, 0.2600, 0.5300, 0.3250, 0.3100],\n",
      "        [0.4450, 0.4450, 0.7550, 0.4000, 0.4150, 0.3750],\n",
      "        [0.1400, 0.1300, 0.1650, 0.0850, 0.0350, 0.1400],\n",
      "        [0.2600, 0.1400, 0.0800, 0.1700, 0.1800, 0.2600],\n",
      "        [0.1500, 0.1850, 0.0850, 0.1950, 0.0900, 0.1500],\n",
      "        [0.1950, 0.0650, 0.1250, 0.1750, 0.0600, 0.0950],\n",
      "        [0.2650, 0.2050, 0.1250, 0.2850, 0.4050, 0.1600],\n",
      "        [0.2900, 0.1100, 0.2050, 0.0450, 0.1350, 0.0750],\n",
      "        [0.7050, 0.3150, 0.4150, 0.0350, 0.2450, 0.5400],\n",
      "        [0.7100, 0.5200, 0.3900, 0.4300, 0.2600, 0.5300],\n",
      "        [0.1150, 0.5600, 0.1950, 0.3450, 0.2100, 0.1150],\n",
      "        [0.4700, 0.7400, 0.3150, 0.2800, 0.3400, 0.2700],\n",
      "        [0.2900, 0.5000, 0.5250, 0.4550, 0.2050, 0.1650],\n",
      "        [0.3900, 0.5500, 0.6950, 0.4150, 0.4850, 0.3650],\n",
      "        [0.0750, 0.0550, 0.2050, 0.1350, 0.2200, 0.0750],\n",
      "        [0.1150, 0.2250, 0.1350, 0.2050, 0.1500, 0.1150],\n",
      "        [0.6450, 0.5050, 0.4700, 0.7200, 0.4350, 0.4750],\n",
      "        [0.1100, 0.0950, 0.0750, 0.0200, 0.1950, 0.1100],\n",
      "        [0.2650, 0.0250, 0.1250, 0.1050, 0.1050, 0.1600],\n",
      "        [0.0250, 0.0450, 0.1300, 0.1700, 0.2200, 0.0250],\n",
      "        [0.1500, 0.1750, 0.0950, 0.0650, 0.1250, 0.1500],\n",
      "        [0.6000, 0.5250, 0.4650, 0.8200, 0.4550, 0.5150],\n",
      "        [0.3300, 0.3150, 0.7800, 0.0100, 0.5250, 0.2650],\n",
      "        [0.1200, 0.4650, 0.4750, 0.6550, 0.4550, 0.1200],\n",
      "        [0.3100, 0.7200, 0.4750, 0.3350, 0.3700, 0.3100],\n",
      "        [0.2500, 0.0850, 0.0750, 0.0850, 0.1050, 0.1350],\n",
      "        [0.6600, 0.2250, 0.4150, 0.2550, 0.5200, 0.4850],\n",
      "        [0.1200, 0.1000, 0.1700, 0.1050, 0.1750, 0.1200],\n",
      "        [0.8100, 0.1200, 0.2250, 0.3300, 0.0700, 0.6100],\n",
      "        [0.6400, 0.5550, 0.2750, 0.4800, 0.6950, 0.5600],\n",
      "        [0.0600, 0.0950, 0.0750, 0.2000, 0.0250, 0.0600],\n",
      "        [0.1600, 0.1350, 0.1600, 0.2000, 0.2600, 0.1600],\n",
      "        [0.5050, 0.2000, 0.3500, 0.4500, 0.3150, 0.2450],\n",
      "        [0.4050, 0.4350, 0.4750, 0.1950, 0.3850, 0.2500],\n",
      "        [0.0800, 0.0700, 0.1800, 0.0600, 0.0400, 0.0200],\n",
      "        [0.4000, 0.3800, 0.4100, 0.3400, 0.4700, 0.4000],\n",
      "        [0.1200, 0.2750, 0.2250, 0.4900, 0.3100, 0.1200],\n",
      "        [0.3300, 0.4600, 0.3950, 0.3000, 0.1900, 0.1050],\n",
      "        [0.5750, 0.2550, 0.4100, 0.3100, 0.3900, 0.5750],\n",
      "        [0.5100, 0.3250, 0.0800, 0.1450, 0.2050, 0.2650],\n",
      "        [0.7150, 0.5850, 0.4800, 0.3900, 0.1400, 0.6150],\n",
      "        [0.4200, 0.1950, 0.2250, 0.3450, 0.4200, 0.1850],\n",
      "        [0.7550, 0.2100, 0.4450, 0.7200, 0.2150, 0.6400],\n",
      "        [0.0800, 0.1100, 0.1100, 0.1600, 0.0450, 0.0800],\n",
      "        [0.5950, 0.4200, 0.4950, 0.1700, 0.6800, 0.4500],\n",
      "        [0.8550, 0.3650, 0.3950, 0.2150, 0.2000, 0.6450]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_default_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Multi-modal(Untrained + Pretrained)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward : Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 64):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 32)\n",
    "        self.fc2_2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(64, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x1 = 1 - x1\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0017, -0.0013, -0.0023, -0.0008,  0.0013, -0.0012]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1630, Train Acc: 64.09%, Test Loss: 0.1183, Test Acc: 70.39%\n",
      "Epoch 2: Train Loss: 0.0936, Train Acc: 74.62%, Test Loss: 0.0802, Test Acc: 76.33%\n",
      "Epoch 3: Train Loss: 0.0711, Train Acc: 78.14%, Test Loss: 0.0645, Test Acc: 78.61%\n",
      "Epoch 4: Train Loss: 0.0606, Train Acc: 79.98%, Test Loss: 0.0573, Test Acc: 80.00%\n",
      "Epoch 5: Train Loss: 0.0553, Train Acc: 80.89%, Test Loss: 0.0539, Test Acc: 80.74%\n",
      "Epoch 6: Train Loss: 0.0524, Train Acc: 81.39%, Test Loss: 0.0531, Test Acc: 80.99%\n",
      "Epoch 7: Train Loss: 0.0503, Train Acc: 81.70%, Test Loss: 0.0522, Test Acc: 81.06%\n",
      "Epoch 8: Train Loss: 0.0486, Train Acc: 82.00%, Test Loss: 0.0515, Test Acc: 81.22%\n",
      "Epoch 9: Train Loss: 0.0474, Train Acc: 82.25%, Test Loss: 0.0509, Test Acc: 81.41%\n",
      "Epoch 10: Train Loss: 0.0463, Train Acc: 82.44%, Test Loss: 0.0488, Test Acc: 81.58%\n",
      "Epoch 11: Train Loss: 0.0455, Train Acc: 82.59%, Test Loss: 0.0487, Test Acc: 81.66%\n",
      "Epoch 12: Train Loss: 0.0449, Train Acc: 82.70%, Test Loss: 0.0477, Test Acc: 81.79%\n",
      "Epoch 13: Train Loss: 0.0446, Train Acc: 82.86%, Test Loss: 0.0482, Test Acc: 81.43%\n",
      "Epoch 14: Train Loss: 0.0441, Train Acc: 82.96%, Test Loss: 0.0477, Test Acc: 81.92%\n",
      "Epoch 15: Train Loss: 0.0436, Train Acc: 83.04%, Test Loss: 0.0473, Test Acc: 81.58%\n",
      "Epoch 16: Train Loss: 0.0435, Train Acc: 83.13%, Test Loss: 0.0480, Test Acc: 81.46%\n",
      "Epoch 17: Train Loss: 0.0431, Train Acc: 83.18%, Test Loss: 0.0462, Test Acc: 82.16%\n",
      "Epoch 18: Train Loss: 0.0427, Train Acc: 83.27%, Test Loss: 0.0464, Test Acc: 82.06%\n",
      "Epoch 19: Train Loss: 0.0426, Train Acc: 83.29%, Test Loss: 0.0455, Test Acc: 82.27%\n",
      "Epoch 20: Train Loss: 0.0424, Train Acc: 83.35%, Test Loss: 0.0454, Test Acc: 82.50%\n",
      "Epoch 21: Train Loss: 0.0421, Train Acc: 83.34%, Test Loss: 0.0471, Test Acc: 81.67%\n",
      "Epoch 22: Train Loss: 0.0417, Train Acc: 83.50%, Test Loss: 0.0478, Test Acc: 81.69%\n",
      "Epoch 23: Train Loss: 0.0416, Train Acc: 83.49%, Test Loss: 0.0451, Test Acc: 81.94%\n",
      "Epoch 24: Train Loss: 0.0415, Train Acc: 83.49%, Test Loss: 0.0433, Test Acc: 82.81%\n",
      "Epoch 25: Train Loss: 0.0414, Train Acc: 83.43%, Test Loss: 0.0443, Test Acc: 82.89%\n",
      "Epoch 26: Train Loss: 0.0415, Train Acc: 83.39%, Test Loss: 0.0438, Test Acc: 82.28%\n",
      "Epoch 27: Train Loss: 0.0411, Train Acc: 83.52%, Test Loss: 0.0452, Test Acc: 82.39%\n",
      "Epoch 28: Train Loss: 0.0408, Train Acc: 83.57%, Test Loss: 0.0438, Test Acc: 82.75%\n",
      "Epoch 29: Train Loss: 0.0409, Train Acc: 83.54%, Test Loss: 0.0411, Test Acc: 82.96%\n",
      "Epoch 30: Train Loss: 0.0405, Train Acc: 83.69%, Test Loss: 0.0418, Test Acc: 82.91%\n",
      "Epoch 31: Train Loss: 0.0404, Train Acc: 83.70%, Test Loss: 0.0422, Test Acc: 82.70%\n",
      "Epoch 32: Train Loss: 0.0406, Train Acc: 83.63%, Test Loss: 0.0437, Test Acc: 82.68%\n",
      "Epoch 33: Train Loss: 0.0404, Train Acc: 83.66%, Test Loss: 0.0429, Test Acc: 83.07%\n",
      "Epoch 34: Train Loss: 0.0400, Train Acc: 83.73%, Test Loss: 0.0411, Test Acc: 83.07%\n",
      "Epoch 35: Train Loss: 0.0405, Train Acc: 83.64%, Test Loss: 0.0438, Test Acc: 83.27%\n",
      "Epoch 36: Train Loss: 0.0400, Train Acc: 83.68%, Test Loss: 0.0409, Test Acc: 83.78%\n",
      "Epoch 37: Train Loss: 0.0399, Train Acc: 83.72%, Test Loss: 0.0435, Test Acc: 82.58%\n",
      "Epoch 38: Train Loss: 0.0397, Train Acc: 83.82%, Test Loss: 0.0417, Test Acc: 83.03%\n",
      "Epoch 39: Train Loss: 0.0399, Train Acc: 83.71%, Test Loss: 0.0420, Test Acc: 82.90%\n",
      "Epoch 40: Train Loss: 0.0396, Train Acc: 83.71%, Test Loss: 0.0420, Test Acc: 82.79%\n",
      "Epoch 41: Train Loss: 0.0396, Train Acc: 83.75%, Test Loss: 0.0397, Test Acc: 82.78%\n",
      "Epoch 42: Train Loss: 0.0396, Train Acc: 83.73%, Test Loss: 0.0421, Test Acc: 82.76%\n",
      "Epoch 43: Train Loss: 0.0395, Train Acc: 83.77%, Test Loss: 0.0406, Test Acc: 83.38%\n",
      "Epoch 44: Train Loss: 0.0393, Train Acc: 83.72%, Test Loss: 0.0408, Test Acc: 83.00%\n",
      "Epoch 45: Train Loss: 0.0391, Train Acc: 83.79%, Test Loss: 0.0431, Test Acc: 82.88%\n",
      "Epoch 46: Train Loss: 0.0392, Train Acc: 83.78%, Test Loss: 0.0416, Test Acc: 82.97%\n",
      "Epoch 47: Train Loss: 0.0389, Train Acc: 83.83%, Test Loss: 0.0388, Test Acc: 83.32%\n",
      "Epoch 48: Train Loss: 0.0392, Train Acc: 83.80%, Test Loss: 0.0424, Test Acc: 82.42%\n",
      "Epoch 49: Train Loss: 0.0389, Train Acc: 83.79%, Test Loss: 0.0384, Test Acc: 83.41%\n",
      "Epoch 50: Train Loss: 0.0390, Train Acc: 83.77%, Test Loss: 0.0388, Test Acc: 83.36%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_dynamic_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_dynamic_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())\n",
    "rl_model.policy.mlp_extractor.value_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward : Dynamic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 32)\n",
    "        self.fc2_2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(64, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x1 = 1 - x1\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_dynamic2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0004,  0.0046, -0.0009, -0.0024,  0.0013,  0.0041]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0133, Train Acc: 53.76%, Test Loss: 0.0062, Test Acc: 64.12%\n",
      "Epoch 2: Train Loss: 0.0054, Train Acc: 65.35%, Test Loss: 0.0043, Test Acc: 70.18%\n",
      "Epoch 3: Train Loss: 0.0044, Train Acc: 69.16%, Test Loss: 0.0037, Test Acc: 72.65%\n",
      "Epoch 4: Train Loss: 0.0040, Train Acc: 71.78%, Test Loss: 0.0030, Test Acc: 74.87%\n",
      "Epoch 5: Train Loss: 0.0037, Train Acc: 73.03%, Test Loss: 0.0029, Test Acc: 73.91%\n",
      "Epoch 6: Train Loss: 0.0035, Train Acc: 73.81%, Test Loss: 0.0030, Test Acc: 77.26%\n",
      "Epoch 7: Train Loss: 0.0034, Train Acc: 74.36%, Test Loss: 0.0026, Test Acc: 76.79%\n",
      "Epoch 8: Train Loss: 0.0034, Train Acc: 74.75%, Test Loss: 0.0025, Test Acc: 78.10%\n",
      "Epoch 9: Train Loss: 0.0033, Train Acc: 75.09%, Test Loss: 0.0030, Test Acc: 78.37%\n",
      "Epoch 10: Train Loss: 0.0033, Train Acc: 75.33%, Test Loss: 0.0027, Test Acc: 78.88%\n",
      "Epoch 11: Train Loss: 0.0031, Train Acc: 75.98%, Test Loss: 0.0026, Test Acc: 79.76%\n",
      "Epoch 12: Train Loss: 0.0031, Train Acc: 76.25%, Test Loss: 0.0028, Test Acc: 79.13%\n",
      "Epoch 13: Train Loss: 0.0030, Train Acc: 76.35%, Test Loss: 0.0030, Test Acc: 80.17%\n",
      "Epoch 14: Train Loss: 0.0030, Train Acc: 76.63%, Test Loss: 0.0032, Test Acc: 73.97%\n",
      "Epoch 15: Train Loss: 0.0029, Train Acc: 76.89%, Test Loss: 0.0023, Test Acc: 79.69%\n",
      "Epoch 16: Train Loss: 0.0028, Train Acc: 77.08%, Test Loss: 0.0027, Test Acc: 80.34%\n",
      "Epoch 17: Train Loss: 0.0028, Train Acc: 77.14%, Test Loss: 0.0025, Test Acc: 80.81%\n",
      "Epoch 18: Train Loss: 0.0027, Train Acc: 77.35%, Test Loss: 0.0039, Test Acc: 80.40%\n",
      "Epoch 19: Train Loss: 0.0027, Train Acc: 77.35%, Test Loss: 0.0032, Test Acc: 78.94%\n",
      "Epoch 20: Train Loss: 0.0027, Train Acc: 77.43%, Test Loss: 0.0028, Test Acc: 80.81%\n",
      "Epoch 21: Train Loss: 0.0026, Train Acc: 77.54%, Test Loss: 0.0027, Test Acc: 77.02%\n",
      "Epoch 22: Train Loss: 0.0026, Train Acc: 77.53%, Test Loss: 0.0023, Test Acc: 80.66%\n",
      "Epoch 23: Train Loss: 0.0026, Train Acc: 77.81%, Test Loss: 0.0033, Test Acc: 78.33%\n",
      "Epoch 24: Train Loss: 0.0026, Train Acc: 77.66%, Test Loss: 0.0026, Test Acc: 79.88%\n",
      "Epoch 25: Train Loss: 0.0025, Train Acc: 77.93%, Test Loss: 0.0024, Test Acc: 80.18%\n",
      "Epoch 26: Train Loss: 0.0025, Train Acc: 77.94%, Test Loss: 0.0025, Test Acc: 80.50%\n",
      "Epoch 27: Train Loss: 0.0025, Train Acc: 77.85%, Test Loss: 0.0027, Test Acc: 80.85%\n",
      "Epoch 28: Train Loss: 0.0025, Train Acc: 77.84%, Test Loss: 0.0028, Test Acc: 81.13%\n",
      "Epoch 29: Train Loss: 0.0025, Train Acc: 77.80%, Test Loss: 0.0021, Test Acc: 81.10%\n",
      "Epoch 30: Train Loss: 0.0024, Train Acc: 78.14%, Test Loss: 0.0028, Test Acc: 78.60%\n",
      "Epoch 31: Train Loss: 0.0024, Train Acc: 77.94%, Test Loss: 0.0022, Test Acc: 77.23%\n",
      "Epoch 32: Train Loss: 0.0024, Train Acc: 77.91%, Test Loss: 0.0024, Test Acc: 79.26%\n",
      "Epoch 33: Train Loss: 0.0024, Train Acc: 78.06%, Test Loss: 0.0022, Test Acc: 80.16%\n",
      "Epoch 34: Train Loss: 0.0024, Train Acc: 78.14%, Test Loss: 0.0022, Test Acc: 77.96%\n",
      "Epoch 35: Train Loss: 0.0024, Train Acc: 78.24%, Test Loss: 0.0027, Test Acc: 79.94%\n",
      "Epoch 36: Train Loss: 0.0023, Train Acc: 78.39%, Test Loss: 0.0021, Test Acc: 78.67%\n",
      "Epoch 37: Train Loss: 0.0023, Train Acc: 78.42%, Test Loss: 0.0025, Test Acc: 80.13%\n",
      "Epoch 38: Train Loss: 0.0023, Train Acc: 78.44%, Test Loss: 0.0024, Test Acc: 76.09%\n",
      "Epoch 39: Train Loss: 0.0024, Train Acc: 78.52%, Test Loss: 0.0022, Test Acc: 77.73%\n",
      "Epoch 40: Train Loss: 0.0023, Train Acc: 78.44%, Test Loss: 0.0021, Test Acc: 76.60%\n",
      "Epoch 41: Train Loss: 0.0023, Train Acc: 78.41%, Test Loss: 0.0030, Test Acc: 80.76%\n",
      "Epoch 42: Train Loss: 0.0024, Train Acc: 78.34%, Test Loss: 0.0022, Test Acc: 79.84%\n",
      "Epoch 43: Train Loss: 0.0023, Train Acc: 78.46%, Test Loss: 0.0022, Test Acc: 77.48%\n",
      "Epoch 44: Train Loss: 0.0023, Train Acc: 78.51%, Test Loss: 0.0027, Test Acc: 79.45%\n",
      "Epoch 45: Train Loss: 0.0023, Train Acc: 78.47%, Test Loss: 0.0024, Test Acc: 80.08%\n",
      "Epoch 46: Train Loss: 0.0023, Train Acc: 78.72%, Test Loss: 0.0017, Test Acc: 79.56%\n",
      "Epoch 47: Train Loss: 0.0023, Train Acc: 78.43%, Test Loss: 0.0029, Test Acc: 79.72%\n",
      "Epoch 48: Train Loss: 0.0023, Train Acc: 78.62%, Test Loss: 0.0027, Test Acc: 77.43%\n",
      "Epoch 49: Train Loss: 0.0023, Train Acc: 78.49%, Test Loss: 0.0025, Test Acc: 78.79%\n",
      "Epoch 50: Train Loss: 0.0023, Train Acc: 78.68%, Test Loss: 0.0029, Test Acc: 79.52%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_dynamic2_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_dynamic2_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())\n",
    "rl_model.policy.mlp_extractor.value_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_dynamic2'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward : Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 32)\n",
    "        self.fc2_2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(64, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x1 = 1 - x1\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_default.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0005, -0.0009,  0.0011, -0.0039, -0.0018, -0.0023]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0032, Train Acc: 78.44%, Test Loss: 0.0017, Test Acc: 80.44%\n",
      "Epoch 2: Train Loss: 0.0015, Train Acc: 81.71%, Test Loss: 0.0015, Test Acc: 79.82%\n",
      "Epoch 3: Train Loss: 0.0013, Train Acc: 82.09%, Test Loss: 0.0014, Test Acc: 79.58%\n",
      "Epoch 4: Train Loss: 0.0012, Train Acc: 82.53%, Test Loss: 0.0012, Test Acc: 80.15%\n",
      "Epoch 5: Train Loss: 0.0010, Train Acc: 83.35%, Test Loss: 0.0011, Test Acc: 81.71%\n",
      "Epoch 6: Train Loss: 0.0009, Train Acc: 84.15%, Test Loss: 0.0009, Test Acc: 83.01%\n",
      "Epoch 7: Train Loss: 0.0008, Train Acc: 84.60%, Test Loss: 0.0008, Test Acc: 84.20%\n",
      "Epoch 8: Train Loss: 0.0007, Train Acc: 85.32%, Test Loss: 0.0007, Test Acc: 85.51%\n",
      "Epoch 9: Train Loss: 0.0006, Train Acc: 86.08%, Test Loss: 0.0007, Test Acc: 86.57%\n",
      "Epoch 10: Train Loss: 0.0006, Train Acc: 86.58%, Test Loss: 0.0006, Test Acc: 87.18%\n",
      "Epoch 11: Train Loss: 0.0006, Train Acc: 87.03%, Test Loss: 0.0006, Test Acc: 87.53%\n",
      "Epoch 12: Train Loss: 0.0005, Train Acc: 87.57%, Test Loss: 0.0005, Test Acc: 88.01%\n",
      "Epoch 13: Train Loss: 0.0005, Train Acc: 88.11%, Test Loss: 0.0005, Test Acc: 88.56%\n",
      "Epoch 14: Train Loss: 0.0005, Train Acc: 88.51%, Test Loss: 0.0005, Test Acc: 88.88%\n",
      "Epoch 15: Train Loss: 0.0004, Train Acc: 88.75%, Test Loss: 0.0005, Test Acc: 89.29%\n",
      "Epoch 16: Train Loss: 0.0004, Train Acc: 89.06%, Test Loss: 0.0005, Test Acc: 90.15%\n",
      "Epoch 17: Train Loss: 0.0004, Train Acc: 89.36%, Test Loss: 0.0004, Test Acc: 90.28%\n",
      "Epoch 18: Train Loss: 0.0004, Train Acc: 89.65%, Test Loss: 0.0004, Test Acc: 89.98%\n",
      "Epoch 19: Train Loss: 0.0004, Train Acc: 89.91%, Test Loss: 0.0004, Test Acc: 90.54%\n",
      "Epoch 20: Train Loss: 0.0003, Train Acc: 90.24%, Test Loss: 0.0005, Test Acc: 88.29%\n",
      "Epoch 21: Train Loss: 0.0003, Train Acc: 90.61%, Test Loss: 0.0004, Test Acc: 89.08%\n",
      "Epoch 22: Train Loss: 0.0003, Train Acc: 90.89%, Test Loss: 0.0004, Test Acc: 89.12%\n",
      "Epoch 23: Train Loss: 0.0003, Train Acc: 91.09%, Test Loss: 0.0004, Test Acc: 89.08%\n",
      "Epoch 24: Train Loss: 0.0003, Train Acc: 91.21%, Test Loss: 0.0004, Test Acc: 90.31%\n",
      "Epoch 25: Train Loss: 0.0003, Train Acc: 91.36%, Test Loss: 0.0004, Test Acc: 89.38%\n",
      "Epoch 26: Train Loss: 0.0003, Train Acc: 91.47%, Test Loss: 0.0004, Test Acc: 90.12%\n",
      "Epoch 27: Train Loss: 0.0003, Train Acc: 91.53%, Test Loss: 0.0004, Test Acc: 91.95%\n",
      "Epoch 28: Train Loss: 0.0003, Train Acc: 91.66%, Test Loss: 0.0004, Test Acc: 88.73%\n",
      "Epoch 29: Train Loss: 0.0003, Train Acc: 91.69%, Test Loss: 0.0003, Test Acc: 89.70%\n",
      "Epoch 30: Train Loss: 0.0003, Train Acc: 91.73%, Test Loss: 0.0003, Test Acc: 88.75%\n",
      "Epoch 31: Train Loss: 0.0003, Train Acc: 91.73%, Test Loss: 0.0003, Test Acc: 90.34%\n",
      "Epoch 32: Train Loss: 0.0003, Train Acc: 91.79%, Test Loss: 0.0003, Test Acc: 90.22%\n",
      "Epoch 33: Train Loss: 0.0003, Train Acc: 91.90%, Test Loss: 0.0004, Test Acc: 89.00%\n",
      "Epoch 34: Train Loss: 0.0003, Train Acc: 91.89%, Test Loss: 0.0004, Test Acc: 90.51%\n",
      "Epoch 35: Train Loss: 0.0003, Train Acc: 91.90%, Test Loss: 0.0003, Test Acc: 89.08%\n",
      "Epoch 36: Train Loss: 0.0003, Train Acc: 91.93%, Test Loss: 0.0003, Test Acc: 91.96%\n",
      "Epoch 37: Train Loss: 0.0003, Train Acc: 91.92%, Test Loss: 0.0003, Test Acc: 89.74%\n",
      "Epoch 38: Train Loss: 0.0003, Train Acc: 91.93%, Test Loss: 0.0003, Test Acc: 89.29%\n",
      "Epoch 39: Train Loss: 0.0003, Train Acc: 92.03%, Test Loss: 0.0003, Test Acc: 89.03%\n",
      "Epoch 40: Train Loss: 0.0003, Train Acc: 92.04%, Test Loss: 0.0003, Test Acc: 90.93%\n",
      "Epoch 41: Train Loss: 0.0003, Train Acc: 92.04%, Test Loss: 0.0003, Test Acc: 89.24%\n",
      "Epoch 42: Train Loss: 0.0003, Train Acc: 92.11%, Test Loss: 0.0003, Test Acc: 92.51%\n",
      "Epoch 43: Train Loss: 0.0003, Train Acc: 92.08%, Test Loss: 0.0004, Test Acc: 91.04%\n",
      "Epoch 44: Train Loss: 0.0003, Train Acc: 92.17%, Test Loss: 0.0002, Test Acc: 89.92%\n",
      "Epoch 45: Train Loss: 0.0003, Train Acc: 92.12%, Test Loss: 0.0003, Test Acc: 91.58%\n",
      "Epoch 46: Train Loss: 0.0003, Train Acc: 92.15%, Test Loss: 0.0004, Test Acc: 91.18%\n",
      "Epoch 47: Train Loss: 0.0003, Train Acc: 92.12%, Test Loss: 0.0003, Test Acc: 92.56%\n",
      "Epoch 48: Train Loss: 0.0002, Train Acc: 92.24%, Test Loss: 0.0003, Test Acc: 91.50%\n",
      "Epoch 49: Train Loss: 0.0003, Train Acc: 92.21%, Test Loss: 0.0003, Test Acc: 90.15%\n",
      "Epoch 50: Train Loss: 0.0002, Train Acc: 92.19%, Test Loss: 0.0003, Test Acc: 90.25%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_default_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_default_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())\n",
    "rl_model.policy.mlp_extractor.value_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc3 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)     # (batch_size, 16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_SM_net(\n",
       "    (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_SM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_SM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_sm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_sm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1362, Train Acc: 75.55%, Test Loss: 0.0727, Test Acc: 83.49%\n",
      "Epoch 2: Train Loss: 0.0657, Train Acc: 84.01%, Test Loss: 0.0567, Test Acc: 84.93%\n",
      "Epoch 3: Train Loss: 0.0554, Train Acc: 84.98%, Test Loss: 0.0508, Test Acc: 85.30%\n",
      "Epoch 4: Train Loss: 0.0502, Train Acc: 85.52%, Test Loss: 0.0474, Test Acc: 85.72%\n",
      "Epoch 5: Train Loss: 0.0472, Train Acc: 85.82%, Test Loss: 0.0452, Test Acc: 86.10%\n",
      "Epoch 6: Train Loss: 0.0454, Train Acc: 85.98%, Test Loss: 0.0418, Test Acc: 86.52%\n",
      "Epoch 7: Train Loss: 0.0440, Train Acc: 86.16%, Test Loss: 0.0419, Test Acc: 86.21%\n",
      "Epoch 8: Train Loss: 0.0433, Train Acc: 86.28%, Test Loss: 0.0455, Test Acc: 85.97%\n",
      "Epoch 9: Train Loss: 0.0426, Train Acc: 86.37%, Test Loss: 0.0417, Test Acc: 86.35%\n",
      "Epoch 10: Train Loss: 0.0419, Train Acc: 86.46%, Test Loss: 0.0410, Test Acc: 86.68%\n",
      "Epoch 11: Train Loss: 0.0409, Train Acc: 86.58%, Test Loss: 0.0415, Test Acc: 86.47%\n",
      "Epoch 12: Train Loss: 0.0403, Train Acc: 86.74%, Test Loss: 0.0385, Test Acc: 86.85%\n",
      "Epoch 13: Train Loss: 0.0398, Train Acc: 86.87%, Test Loss: 0.0392, Test Acc: 86.97%\n",
      "Epoch 14: Train Loss: 0.0390, Train Acc: 87.03%, Test Loss: 0.0451, Test Acc: 86.75%\n",
      "Epoch 15: Train Loss: 0.0385, Train Acc: 87.13%, Test Loss: 0.0381, Test Acc: 87.09%\n",
      "Epoch 16: Train Loss: 0.0383, Train Acc: 87.16%, Test Loss: 0.0386, Test Acc: 87.15%\n",
      "Epoch 17: Train Loss: 0.0379, Train Acc: 87.28%, Test Loss: 0.0378, Test Acc: 87.20%\n",
      "Epoch 18: Train Loss: 0.0375, Train Acc: 87.31%, Test Loss: 0.0357, Test Acc: 87.51%\n",
      "Epoch 19: Train Loss: 0.0374, Train Acc: 87.38%, Test Loss: 0.0374, Test Acc: 87.22%\n",
      "Epoch 20: Train Loss: 0.0369, Train Acc: 87.47%, Test Loss: 0.0373, Test Acc: 87.19%\n",
      "Epoch 21: Train Loss: 0.0367, Train Acc: 87.51%, Test Loss: 0.0355, Test Acc: 87.69%\n",
      "Epoch 22: Train Loss: 0.0371, Train Acc: 87.48%, Test Loss: 0.0357, Test Acc: 87.12%\n",
      "Epoch 23: Train Loss: 0.0366, Train Acc: 87.55%, Test Loss: 0.0407, Test Acc: 87.05%\n",
      "Epoch 24: Train Loss: 0.0364, Train Acc: 87.59%, Test Loss: 0.0405, Test Acc: 87.25%\n",
      "Epoch 25: Train Loss: 0.0361, Train Acc: 87.58%, Test Loss: 0.0382, Test Acc: 87.46%\n",
      "Epoch 26: Train Loss: 0.0360, Train Acc: 87.65%, Test Loss: 0.0396, Test Acc: 87.12%\n",
      "Epoch 27: Train Loss: 0.0357, Train Acc: 87.65%, Test Loss: 0.0466, Test Acc: 86.28%\n",
      "Epoch 28: Train Loss: 0.0354, Train Acc: 87.71%, Test Loss: 0.0366, Test Acc: 87.30%\n",
      "Epoch 29: Train Loss: 0.0353, Train Acc: 87.69%, Test Loss: 0.0335, Test Acc: 87.58%\n",
      "Epoch 30: Train Loss: 0.0353, Train Acc: 87.69%, Test Loss: 0.0361, Test Acc: 87.40%\n",
      "Epoch 31: Train Loss: 0.0353, Train Acc: 87.70%, Test Loss: 0.0350, Test Acc: 87.41%\n",
      "Epoch 32: Train Loss: 0.0348, Train Acc: 87.79%, Test Loss: 0.0381, Test Acc: 87.04%\n",
      "Epoch 33: Train Loss: 0.0348, Train Acc: 87.79%, Test Loss: 0.0337, Test Acc: 87.70%\n",
      "Epoch 34: Train Loss: 0.0346, Train Acc: 87.87%, Test Loss: 0.0344, Test Acc: 87.63%\n",
      "Epoch 35: Train Loss: 0.0345, Train Acc: 87.81%, Test Loss: 0.0334, Test Acc: 87.85%\n",
      "Epoch 36: Train Loss: 0.0345, Train Acc: 87.84%, Test Loss: 0.0360, Test Acc: 87.58%\n",
      "Epoch 37: Train Loss: 0.0346, Train Acc: 87.82%, Test Loss: 0.0415, Test Acc: 87.14%\n",
      "Epoch 38: Train Loss: 0.0341, Train Acc: 87.87%, Test Loss: 0.0351, Test Acc: 87.53%\n",
      "Epoch 39: Train Loss: 0.0346, Train Acc: 87.87%, Test Loss: 0.0339, Test Acc: 87.78%\n",
      "Epoch 40: Train Loss: 0.0339, Train Acc: 87.91%, Test Loss: 0.0336, Test Acc: 87.68%\n",
      "Epoch 41: Train Loss: 0.0338, Train Acc: 87.93%, Test Loss: 0.0334, Test Acc: 87.90%\n",
      "Epoch 42: Train Loss: 0.0339, Train Acc: 87.90%, Test Loss: 0.0342, Test Acc: 87.88%\n",
      "Epoch 43: Train Loss: 0.0338, Train Acc: 87.98%, Test Loss: 0.0349, Test Acc: 87.92%\n",
      "Epoch 44: Train Loss: 0.0336, Train Acc: 87.92%, Test Loss: 0.0353, Test Acc: 87.87%\n",
      "Epoch 45: Train Loss: 0.0335, Train Acc: 87.98%, Test Loss: 0.0334, Test Acc: 87.97%\n",
      "Epoch 46: Train Loss: 0.0338, Train Acc: 87.97%, Test Loss: 0.0340, Test Acc: 87.94%\n",
      "Epoch 47: Train Loss: 0.0331, Train Acc: 88.03%, Test Loss: 0.0344, Test Acc: 88.07%\n",
      "Epoch 48: Train Loss: 0.0334, Train Acc: 88.00%, Test Loss: 0.0335, Test Acc: 87.96%\n",
      "Epoch 49: Train Loss: 0.0332, Train Acc: 88.01%, Test Loss: 0.0307, Test Acc: 88.41%\n",
      "Epoch 50: Train Loss: 0.0342, Train Acc: 87.97%, Test Loss: 0.0333, Test Acc: 88.18%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_sm_action_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_sm_action_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_sm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_sm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_sm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Multi-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0019, -0.1223,  0.0332, -0.0266, -0.0474, -0.0251]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_MM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_MM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_mm_model = DQN_MM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_mm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1929, Train Acc: 61.92%, Test Loss: 0.1382, Test Acc: 73.71%\n",
      "Epoch 2: Train Loss: 0.1290, Train Acc: 74.93%, Test Loss: 0.1304, Test Acc: 74.60%\n",
      "Epoch 3: Train Loss: 0.1254, Train Acc: 75.37%, Test Loss: 0.1292, Test Acc: 74.84%\n",
      "Epoch 4: Train Loss: 0.1240, Train Acc: 75.56%, Test Loss: 0.1261, Test Acc: 75.12%\n",
      "Epoch 5: Train Loss: 0.1226, Train Acc: 75.73%, Test Loss: 0.1239, Test Acc: 75.25%\n",
      "Epoch 6: Train Loss: 0.1212, Train Acc: 75.81%, Test Loss: 0.1223, Test Acc: 75.21%\n",
      "Epoch 7: Train Loss: 0.1200, Train Acc: 75.88%, Test Loss: 0.1216, Test Acc: 75.15%\n",
      "Epoch 8: Train Loss: 0.1191, Train Acc: 75.99%, Test Loss: 0.1207, Test Acc: 75.11%\n",
      "Epoch 9: Train Loss: 0.1184, Train Acc: 76.04%, Test Loss: 0.1200, Test Acc: 75.21%\n",
      "Epoch 10: Train Loss: 0.1179, Train Acc: 76.13%, Test Loss: 0.1190, Test Acc: 75.27%\n",
      "Epoch 11: Train Loss: 0.1175, Train Acc: 76.18%, Test Loss: 0.1189, Test Acc: 75.39%\n",
      "Epoch 12: Train Loss: 0.1172, Train Acc: 76.22%, Test Loss: 0.1182, Test Acc: 75.43%\n",
      "Epoch 13: Train Loss: 0.1169, Train Acc: 76.23%, Test Loss: 0.1182, Test Acc: 75.36%\n",
      "Epoch 14: Train Loss: 0.1167, Train Acc: 76.28%, Test Loss: 0.1177, Test Acc: 75.58%\n",
      "Epoch 15: Train Loss: 0.1165, Train Acc: 76.30%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 16: Train Loss: 0.1163, Train Acc: 76.34%, Test Loss: 0.1167, Test Acc: 75.63%\n",
      "Epoch 17: Train Loss: 0.1161, Train Acc: 76.35%, Test Loss: 0.1168, Test Acc: 75.69%\n",
      "Epoch 18: Train Loss: 0.1160, Train Acc: 76.33%, Test Loss: 0.1167, Test Acc: 75.55%\n",
      "Epoch 19: Train Loss: 0.1159, Train Acc: 76.36%, Test Loss: 0.1167, Test Acc: 75.64%\n",
      "Epoch 20: Train Loss: 0.1158, Train Acc: 76.33%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 21: Train Loss: 0.1156, Train Acc: 76.35%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 22: Train Loss: 0.1152, Train Acc: 76.41%, Test Loss: 0.1150, Test Acc: 75.92%\n",
      "Epoch 23: Train Loss: 0.1128, Train Acc: 76.72%, Test Loss: 0.1135, Test Acc: 76.14%\n",
      "Epoch 24: Train Loss: 0.1115, Train Acc: 76.92%, Test Loss: 0.1135, Test Acc: 76.12%\n",
      "Epoch 25: Train Loss: 0.1112, Train Acc: 76.98%, Test Loss: 0.1133, Test Acc: 76.23%\n",
      "Epoch 26: Train Loss: 0.1108, Train Acc: 77.00%, Test Loss: 0.1127, Test Acc: 76.39%\n",
      "Epoch 27: Train Loss: 0.1105, Train Acc: 77.07%, Test Loss: 0.1120, Test Acc: 76.35%\n",
      "Epoch 28: Train Loss: 0.1103, Train Acc: 77.07%, Test Loss: 0.1109, Test Acc: 76.57%\n",
      "Epoch 29: Train Loss: 0.1100, Train Acc: 77.09%, Test Loss: 0.1109, Test Acc: 76.51%\n",
      "Epoch 30: Train Loss: 0.1097, Train Acc: 77.11%, Test Loss: 0.1108, Test Acc: 76.45%\n",
      "Epoch 31: Train Loss: 0.1094, Train Acc: 77.10%, Test Loss: 0.1096, Test Acc: 76.66%\n",
      "Epoch 32: Train Loss: 0.1092, Train Acc: 77.12%, Test Loss: 0.1093, Test Acc: 76.56%\n",
      "Epoch 33: Train Loss: 0.1066, Train Acc: 77.18%, Test Loss: 0.1025, Test Acc: 77.42%\n",
      "Epoch 34: Train Loss: 0.1005, Train Acc: 77.90%, Test Loss: 0.0993, Test Acc: 77.65%\n",
      "Epoch 35: Train Loss: 0.0979, Train Acc: 78.25%, Test Loss: 0.0961, Test Acc: 78.25%\n",
      "Epoch 36: Train Loss: 0.0934, Train Acc: 78.84%, Test Loss: 0.0913, Test Acc: 78.82%\n",
      "Epoch 37: Train Loss: 0.0862, Train Acc: 79.80%, Test Loss: 0.0841, Test Acc: 79.70%\n",
      "Epoch 38: Train Loss: 0.0812, Train Acc: 80.52%, Test Loss: 0.0805, Test Acc: 80.22%\n",
      "Epoch 39: Train Loss: 0.0791, Train Acc: 80.73%, Test Loss: 0.0779, Test Acc: 80.58%\n",
      "Epoch 40: Train Loss: 0.0778, Train Acc: 80.86%, Test Loss: 0.0775, Test Acc: 80.64%\n",
      "Epoch 41: Train Loss: 0.0767, Train Acc: 80.97%, Test Loss: 0.0758, Test Acc: 81.15%\n",
      "Epoch 42: Train Loss: 0.0757, Train Acc: 81.09%, Test Loss: 0.0743, Test Acc: 81.30%\n",
      "Epoch 43: Train Loss: 0.0747, Train Acc: 81.21%, Test Loss: 0.0731, Test Acc: 81.43%\n",
      "Epoch 44: Train Loss: 0.0739, Train Acc: 81.34%, Test Loss: 0.0726, Test Acc: 81.61%\n",
      "Epoch 45: Train Loss: 0.0733, Train Acc: 81.46%, Test Loss: 0.0722, Test Acc: 81.69%\n",
      "Epoch 46: Train Loss: 0.0726, Train Acc: 81.58%, Test Loss: 0.0721, Test Acc: 81.74%\n",
      "Epoch 47: Train Loss: 0.0719, Train Acc: 81.67%, Test Loss: 0.0715, Test Acc: 81.75%\n",
      "Epoch 48: Train Loss: 0.0711, Train Acc: 81.73%, Test Loss: 0.0695, Test Acc: 81.96%\n",
      "Epoch 49: Train Loss: 0.0704, Train Acc: 81.79%, Test Loss: 0.0718, Test Acc: 81.53%\n",
      "Epoch 50: Train Loss: 0.0698, Train Acc: 81.80%, Test Loss: 0.0701, Test Acc: 81.77%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_mm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_mm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_mm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_mm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5014, -0.9858, -1.0000, -1.0103, -1.0104, -0.9910]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5059, -0.9256, -0.9021, -1.0459, -1.0538,  0.6572]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net(sample2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_SM_net(\n",
       "      (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_SM_net(\n",
       "      (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1106,  0.0206, -0.0055, -0.0425,  0.0521,  0.1534]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_SM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_SM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_sm_model = DQN_SM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_sm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1568, Train Acc: 71.72%, Test Loss: 0.1036, Test Acc: 77.05%\n",
      "Epoch 2: Train Loss: 0.0836, Train Acc: 80.46%, Test Loss: 0.0699, Test Acc: 82.15%\n",
      "Epoch 3: Train Loss: 0.0595, Train Acc: 84.28%, Test Loss: 0.0589, Test Acc: 83.94%\n",
      "Epoch 4: Train Loss: 0.0523, Train Acc: 85.02%, Test Loss: 0.0518, Test Acc: 84.52%\n",
      "Epoch 5: Train Loss: 0.0478, Train Acc: 85.62%, Test Loss: 0.0496, Test Acc: 84.93%\n",
      "Epoch 6: Train Loss: 0.0442, Train Acc: 86.09%, Test Loss: 0.0467, Test Acc: 85.73%\n",
      "Epoch 7: Train Loss: 0.0420, Train Acc: 86.35%, Test Loss: 0.0453, Test Acc: 85.95%\n",
      "Epoch 8: Train Loss: 0.0407, Train Acc: 86.60%, Test Loss: 0.0431, Test Acc: 86.51%\n",
      "Epoch 9: Train Loss: 0.0398, Train Acc: 86.77%, Test Loss: 0.0402, Test Acc: 86.71%\n",
      "Epoch 10: Train Loss: 0.0390, Train Acc: 86.93%, Test Loss: 0.0392, Test Acc: 86.96%\n",
      "Epoch 11: Train Loss: 0.0382, Train Acc: 87.06%, Test Loss: 0.0376, Test Acc: 87.78%\n",
      "Epoch 12: Train Loss: 0.0375, Train Acc: 87.15%, Test Loss: 0.0419, Test Acc: 86.73%\n",
      "Epoch 13: Train Loss: 0.0371, Train Acc: 87.21%, Test Loss: 0.0396, Test Acc: 87.32%\n",
      "Epoch 14: Train Loss: 0.0367, Train Acc: 87.25%, Test Loss: 0.0408, Test Acc: 87.11%\n",
      "Epoch 15: Train Loss: 0.0363, Train Acc: 87.34%, Test Loss: 0.0363, Test Acc: 87.53%\n",
      "Epoch 16: Train Loss: 0.0360, Train Acc: 87.42%, Test Loss: 0.0391, Test Acc: 87.18%\n",
      "Epoch 17: Train Loss: 0.0357, Train Acc: 87.49%, Test Loss: 0.0373, Test Acc: 87.63%\n",
      "Epoch 18: Train Loss: 0.0353, Train Acc: 87.57%, Test Loss: 0.0356, Test Acc: 87.73%\n",
      "Epoch 19: Train Loss: 0.0351, Train Acc: 87.65%, Test Loss: 0.0355, Test Acc: 87.56%\n",
      "Epoch 20: Train Loss: 0.0348, Train Acc: 87.68%, Test Loss: 0.0400, Test Acc: 87.33%\n",
      "Epoch 21: Train Loss: 0.0345, Train Acc: 87.68%, Test Loss: 0.0362, Test Acc: 87.69%\n",
      "Epoch 22: Train Loss: 0.0345, Train Acc: 87.67%, Test Loss: 0.0380, Test Acc: 87.60%\n",
      "Epoch 23: Train Loss: 0.0340, Train Acc: 87.78%, Test Loss: 0.0403, Test Acc: 87.32%\n",
      "Epoch 24: Train Loss: 0.0339, Train Acc: 87.79%, Test Loss: 0.0373, Test Acc: 87.85%\n",
      "Epoch 25: Train Loss: 0.0338, Train Acc: 87.79%, Test Loss: 0.0378, Test Acc: 87.85%\n",
      "Epoch 26: Train Loss: 0.0334, Train Acc: 87.88%, Test Loss: 0.0393, Test Acc: 87.69%\n",
      "Epoch 27: Train Loss: 0.0330, Train Acc: 87.97%, Test Loss: 0.0317, Test Acc: 88.39%\n",
      "Epoch 28: Train Loss: 0.0331, Train Acc: 87.96%, Test Loss: 0.0341, Test Acc: 88.12%\n",
      "Epoch 29: Train Loss: 0.0328, Train Acc: 87.99%, Test Loss: 0.0347, Test Acc: 88.08%\n",
      "Epoch 30: Train Loss: 0.0327, Train Acc: 88.04%, Test Loss: 0.0306, Test Acc: 88.73%\n",
      "Epoch 31: Train Loss: 0.0326, Train Acc: 88.05%, Test Loss: 0.0347, Test Acc: 88.02%\n",
      "Epoch 32: Train Loss: 0.0323, Train Acc: 88.10%, Test Loss: 0.0304, Test Acc: 88.61%\n",
      "Epoch 33: Train Loss: 0.0323, Train Acc: 88.14%, Test Loss: 0.0436, Test Acc: 87.02%\n",
      "Epoch 34: Train Loss: 0.0322, Train Acc: 88.17%, Test Loss: 0.0300, Test Acc: 88.58%\n",
      "Epoch 35: Train Loss: 0.0319, Train Acc: 88.19%, Test Loss: 0.0319, Test Acc: 88.52%\n",
      "Epoch 36: Train Loss: 0.0317, Train Acc: 88.27%, Test Loss: 0.0314, Test Acc: 88.51%\n",
      "Epoch 37: Train Loss: 0.0316, Train Acc: 88.27%, Test Loss: 0.0331, Test Acc: 88.29%\n",
      "Epoch 38: Train Loss: 0.0315, Train Acc: 88.29%, Test Loss: 0.0406, Test Acc: 86.96%\n",
      "Epoch 39: Train Loss: 0.0315, Train Acc: 88.37%, Test Loss: 0.0349, Test Acc: 88.18%\n",
      "Epoch 40: Train Loss: 0.0314, Train Acc: 88.39%, Test Loss: 0.0355, Test Acc: 88.10%\n",
      "Epoch 41: Train Loss: 0.0313, Train Acc: 88.35%, Test Loss: 0.0364, Test Acc: 88.25%\n",
      "Epoch 42: Train Loss: 0.0311, Train Acc: 88.43%, Test Loss: 0.0299, Test Acc: 88.61%\n",
      "Epoch 43: Train Loss: 0.0311, Train Acc: 88.44%, Test Loss: 0.0316, Test Acc: 88.86%\n",
      "Epoch 44: Train Loss: 0.0310, Train Acc: 88.43%, Test Loss: 0.0305, Test Acc: 88.58%\n",
      "Epoch 45: Train Loss: 0.0309, Train Acc: 88.42%, Test Loss: 0.0304, Test Acc: 88.60%\n",
      "Epoch 46: Train Loss: 0.0308, Train Acc: 88.49%, Test Loss: 0.0309, Test Acc: 88.64%\n",
      "Epoch 47: Train Loss: 0.0307, Train Acc: 88.51%, Test Loss: 0.0305, Test Acc: 88.81%\n",
      "Epoch 48: Train Loss: 0.0307, Train Acc: 88.40%, Test Loss: 0.0304, Test Acc: 88.63%\n",
      "Epoch 49: Train Loss: 0.0304, Train Acc: 88.46%, Test Loss: 0.0313, Test Acc: 88.59%\n",
      "Epoch 50: Train Loss: 0.0306, Train Acc: 88.48%, Test Loss: 0.0297, Test Acc: 89.01%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_sm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_sm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_sm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_sm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Tripple-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_TM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_TM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc1_3 = nn.Linear(5, 16)  # Node difference (CPU, Memory)\n",
    "        # self.fc1_4 = nn.Linear(5, 16) # If the node can deploy the pod\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc2_3 = nn.Linear(16, 8)\n",
    "        # self.fc2_4 = nn.Linear(16, 8)\n",
    "\n",
    "        self.fc3 = nn.Linear(24, 16)\n",
    "        # self.fc3 = nn.Linear(32, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x3 = x1[:, ::2] - x1[:, 1::2] # Takes the difference of x1's odd and even columns\n",
    "\n",
    "        # # Duplicte x2 5 times horizontally\n",
    "        # x2_ = x2.repeat(1, 5).view(-1, 10)\n",
    "        # x4_ = (1-x1) - x2_ >= 0\n",
    "        # x4 = x4_[:, ::2] * x4_[:, 1::2]\n",
    "        # # Convert boolean to float\n",
    "        # x4 = x4.type(torch.FloatTensor)\n",
    "        # # print(f\"x4 : {x4}\")\n",
    "\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x3 = F.relu(self.fc1_3(x3))\n",
    "        # x4 = F.relu(self.fc1_4(x4))\n",
    "\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x3 = F.relu(self.fc2_3(x3))\n",
    "        # x4 = F.relu(self.fc2_4(x4))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        # x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_TM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_TM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=24, out_features=16, bias=True)\n",
       "    (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_tm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0567, -0.0286, -0.0869, -0.0466, -0.0994, -0.0755]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_TM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_TM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tm_model = DQN_TM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Predict 6 vectors (6 actions' scores)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_tm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0233, Train Acc: 59.48%, Test Loss: 0.0135, Test Acc: 69.50%\n",
      "Epoch 2: Train Loss: 0.0136, Train Acc: 69.64%, Test Loss: 0.0126, Test Acc: 71.00%\n",
      "Epoch 3: Train Loss: 0.0115, Train Acc: 72.31%, Test Loss: 0.0093, Test Acc: 74.22%\n",
      "Epoch 4: Train Loss: 0.0082, Train Acc: 76.54%, Test Loss: 0.0068, Test Acc: 78.27%\n",
      "Epoch 5: Train Loss: 0.0061, Train Acc: 78.97%, Test Loss: 0.0052, Test Acc: 79.49%\n",
      "Epoch 6: Train Loss: 0.0053, Train Acc: 80.24%, Test Loss: 0.0045, Test Acc: 81.49%\n",
      "Epoch 7: Train Loss: 0.0051, Train Acc: 80.89%, Test Loss: 0.0050, Test Acc: 80.77%\n",
      "Epoch 8: Train Loss: 0.0049, Train Acc: 81.43%, Test Loss: 0.0051, Test Acc: 81.64%\n",
      "Epoch 9: Train Loss: 0.0048, Train Acc: 81.90%, Test Loss: 0.0053, Test Acc: 81.67%\n",
      "Epoch 10: Train Loss: 0.0047, Train Acc: 82.21%, Test Loss: 0.0044, Test Acc: 82.39%\n",
      "Epoch 11: Train Loss: 0.0046, Train Acc: 82.51%, Test Loss: 0.0044, Test Acc: 82.67%\n",
      "Epoch 12: Train Loss: 0.0045, Train Acc: 82.83%, Test Loss: 0.0044, Test Acc: 83.53%\n",
      "Epoch 13: Train Loss: 0.0044, Train Acc: 83.10%, Test Loss: 0.0046, Test Acc: 83.33%\n",
      "Epoch 14: Train Loss: 0.0044, Train Acc: 83.26%, Test Loss: 0.0039, Test Acc: 83.51%\n",
      "Epoch 15: Train Loss: 0.0043, Train Acc: 83.40%, Test Loss: 0.0044, Test Acc: 83.43%\n",
      "Epoch 16: Train Loss: 0.0042, Train Acc: 83.50%, Test Loss: 0.0043, Test Acc: 84.16%\n",
      "Epoch 17: Train Loss: 0.0042, Train Acc: 83.60%, Test Loss: 0.0038, Test Acc: 83.70%\n",
      "Epoch 18: Train Loss: 0.0042, Train Acc: 83.69%, Test Loss: 0.0038, Test Acc: 84.19%\n",
      "Epoch 19: Train Loss: 0.0041, Train Acc: 83.76%, Test Loss: 0.0047, Test Acc: 81.87%\n",
      "Epoch 20: Train Loss: 0.0041, Train Acc: 83.80%, Test Loss: 0.0041, Test Acc: 83.58%\n",
      "Epoch 21: Train Loss: 0.0041, Train Acc: 83.89%, Test Loss: 0.0041, Test Acc: 83.99%\n",
      "Epoch 22: Train Loss: 0.0040, Train Acc: 83.96%, Test Loss: 0.0037, Test Acc: 83.98%\n",
      "Epoch 23: Train Loss: 0.0040, Train Acc: 84.00%, Test Loss: 0.0039, Test Acc: 84.11%\n",
      "Epoch 24: Train Loss: 0.0040, Train Acc: 84.10%, Test Loss: 0.0037, Test Acc: 84.70%\n",
      "Epoch 25: Train Loss: 0.0039, Train Acc: 84.19%, Test Loss: 0.0041, Test Acc: 84.26%\n",
      "Epoch 26: Train Loss: 0.0039, Train Acc: 84.20%, Test Loss: 0.0037, Test Acc: 84.61%\n",
      "Epoch 27: Train Loss: 0.0039, Train Acc: 84.22%, Test Loss: 0.0043, Test Acc: 83.78%\n",
      "Epoch 28: Train Loss: 0.0039, Train Acc: 84.25%, Test Loss: 0.0042, Test Acc: 83.92%\n",
      "Epoch 29: Train Loss: 0.0039, Train Acc: 84.28%, Test Loss: 0.0037, Test Acc: 84.28%\n",
      "Epoch 30: Train Loss: 0.0038, Train Acc: 84.32%, Test Loss: 0.0041, Test Acc: 83.64%\n",
      "Epoch 31: Train Loss: 0.0038, Train Acc: 84.35%, Test Loss: 0.0037, Test Acc: 84.70%\n",
      "Epoch 32: Train Loss: 0.0038, Train Acc: 84.43%, Test Loss: 0.0041, Test Acc: 83.79%\n",
      "Epoch 33: Train Loss: 0.0038, Train Acc: 84.46%, Test Loss: 0.0039, Test Acc: 83.78%\n",
      "Epoch 34: Train Loss: 0.0038, Train Acc: 84.47%, Test Loss: 0.0043, Test Acc: 84.07%\n",
      "Epoch 35: Train Loss: 0.0038, Train Acc: 84.50%, Test Loss: 0.0047, Test Acc: 82.88%\n",
      "Epoch 36: Train Loss: 0.0037, Train Acc: 84.49%, Test Loss: 0.0038, Test Acc: 84.20%\n",
      "Epoch 37: Train Loss: 0.0037, Train Acc: 84.52%, Test Loss: 0.0036, Test Acc: 84.37%\n",
      "Epoch 38: Train Loss: 0.0037, Train Acc: 84.51%, Test Loss: 0.0040, Test Acc: 83.57%\n",
      "Epoch 39: Train Loss: 0.0037, Train Acc: 84.55%, Test Loss: 0.0041, Test Acc: 83.11%\n",
      "Epoch 40: Train Loss: 0.0036, Train Acc: 84.61%, Test Loss: 0.0038, Test Acc: 83.50%\n",
      "Epoch 41: Train Loss: 0.0036, Train Acc: 84.56%, Test Loss: 0.0042, Test Acc: 83.76%\n",
      "Epoch 42: Train Loss: 0.0036, Train Acc: 84.58%, Test Loss: 0.0037, Test Acc: 83.72%\n",
      "Epoch 43: Train Loss: 0.0036, Train Acc: 84.60%, Test Loss: 0.0041, Test Acc: 84.68%\n",
      "Epoch 44: Train Loss: 0.0036, Train Acc: 84.62%, Test Loss: 0.0037, Test Acc: 83.57%\n",
      "Epoch 45: Train Loss: 0.0036, Train Acc: 84.63%, Test Loss: 0.0039, Test Acc: 84.46%\n",
      "Epoch 46: Train Loss: 0.0036, Train Acc: 84.65%, Test Loss: 0.0036, Test Acc: 84.34%\n",
      "Epoch 47: Train Loss: 0.0036, Train Acc: 84.70%, Test Loss: 0.0040, Test Acc: 83.91%\n",
      "Epoch 48: Train Loss: 0.0036, Train Acc: 84.71%, Test Loss: 0.0034, Test Acc: 85.34%\n",
      "Epoch 49: Train Loss: 0.0036, Train Acc: 84.74%, Test Loss: 0.0038, Test Acc: 85.24%\n",
      "Epoch 50: Train Loss: 0.0035, Train Acc: 84.74%, Test Loss: 0.0040, Test Acc: 84.37%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_tm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_tm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_tm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_tm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_qm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0045, -0.4972, -0.5014, -0.4907, -0.5101, -0.4970]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3976, -0.8768, -0.8936, -0.8247, -0.9237,  0.0061]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2225, -0.7509, -0.7294, -0.0045, -0.7570, -0.7192]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DRS Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_TM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_drs.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_tm_ut_drs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_TM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_TM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tm_model = DQN_TM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Predict 6 vectors (6 actions' scores)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_tm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_tm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_tm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_tm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_tm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_qm_pr_drs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Tripple-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_TM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_TM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc1_3 = nn.Linear(5, 16)  # Node difference (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc2_3 = nn.Linear(16, 8)\n",
    "\n",
    "        self.fc3 = nn.Linear(24, 32)\n",
    "        self.fc4 = nn.Linear(32, 64)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x3 = x1[:, ::2] - x1[:, 1::2] # Takes the difference of x1's odd and even columns\n",
    "\n",
    "        # (node.spec[\"cpu_pool\"] - node.status[\"cpu_util\"] - pod.spec[\"cpu_req\"]) / node.spec[\"cpu_pool\"]\n",
    "        # (node.spec[\"mem_pool\"] - node.status[\"mem_util\"] - pod.spec[\"mem_req\"]) / node.spec[\"mem_pool\"]\n",
    "        # x1 = 1 - x1 - x2.repeat(1, 5)\n",
    "\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x3 = F.relu(self.fc1_3(x3))\n",
    "\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x3 = F.relu(self.fc2_3(x3))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_TM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_TM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=24, out_features=32, bias=True)\n",
       "    (fc4): Linear(in_features=32, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_tm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_TM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_TM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_tm_action_model = PPO_TM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0037,  0.0031, -0.0085, -0.0003,  0.0036,  0.0068]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_tm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_tm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0154, Train Acc: 52.86%, Test Loss: 0.0077, Test Acc: 64.14%\n",
      "Epoch 2: Train Loss: 0.0072, Train Acc: 65.15%, Test Loss: 0.0061, Test Acc: 68.13%\n",
      "Epoch 3: Train Loss: 0.0061, Train Acc: 67.44%, Test Loss: 0.0053, Test Acc: 70.34%\n",
      "Epoch 4: Train Loss: 0.0057, Train Acc: 68.88%, Test Loss: 0.0045, Test Acc: 71.38%\n",
      "Epoch 5: Train Loss: 0.0055, Train Acc: 70.05%, Test Loss: 0.0044, Test Acc: 72.52%\n",
      "Epoch 6: Train Loss: 0.0053, Train Acc: 71.02%, Test Loss: 0.0043, Test Acc: 72.39%\n",
      "Epoch 7: Train Loss: 0.0052, Train Acc: 71.77%, Test Loss: 0.0044, Test Acc: 74.11%\n",
      "Epoch 8: Train Loss: 0.0050, Train Acc: 72.15%, Test Loss: 0.0044, Test Acc: 74.13%\n",
      "Epoch 9: Train Loss: 0.0050, Train Acc: 72.21%, Test Loss: 0.0041, Test Acc: 74.64%\n",
      "Epoch 10: Train Loss: 0.0048, Train Acc: 72.66%, Test Loss: 0.0055, Test Acc: 72.28%\n",
      "Epoch 11: Train Loss: 0.0048, Train Acc: 72.77%, Test Loss: 0.0042, Test Acc: 74.38%\n",
      "Epoch 12: Train Loss: 0.0048, Train Acc: 72.84%, Test Loss: 0.0050, Test Acc: 73.75%\n",
      "Epoch 13: Train Loss: 0.0047, Train Acc: 73.13%, Test Loss: 0.0040, Test Acc: 75.66%\n",
      "Epoch 14: Train Loss: 0.0046, Train Acc: 73.25%, Test Loss: 0.0046, Test Acc: 75.39%\n",
      "Epoch 15: Train Loss: 0.0045, Train Acc: 73.53%, Test Loss: 0.0040, Test Acc: 75.08%\n",
      "Epoch 16: Train Loss: 0.0045, Train Acc: 73.54%, Test Loss: 0.0038, Test Acc: 75.71%\n",
      "Epoch 17: Train Loss: 0.0045, Train Acc: 73.63%, Test Loss: 0.0044, Test Acc: 74.05%\n",
      "Epoch 18: Train Loss: 0.0044, Train Acc: 73.84%, Test Loss: 0.0046, Test Acc: 75.22%\n",
      "Epoch 19: Train Loss: 0.0043, Train Acc: 74.13%, Test Loss: 0.0033, Test Acc: 76.58%\n",
      "Epoch 20: Train Loss: 0.0043, Train Acc: 73.94%, Test Loss: 0.0046, Test Acc: 73.28%\n",
      "Epoch 21: Train Loss: 0.0043, Train Acc: 74.07%, Test Loss: 0.0039, Test Acc: 75.62%\n",
      "Epoch 22: Train Loss: 0.0041, Train Acc: 74.40%, Test Loss: 0.0045, Test Acc: 73.45%\n",
      "Epoch 23: Train Loss: 0.0041, Train Acc: 74.29%, Test Loss: 0.0038, Test Acc: 76.39%\n",
      "Epoch 24: Train Loss: 0.0041, Train Acc: 74.50%, Test Loss: 0.0053, Test Acc: 74.32%\n",
      "Epoch 25: Train Loss: 0.0041, Train Acc: 74.44%, Test Loss: 0.0041, Test Acc: 76.35%\n",
      "Epoch 26: Train Loss: 0.0040, Train Acc: 74.91%, Test Loss: 0.0035, Test Acc: 77.25%\n",
      "Epoch 27: Train Loss: 0.0040, Train Acc: 74.93%, Test Loss: 0.0039, Test Acc: 77.11%\n",
      "Epoch 28: Train Loss: 0.0039, Train Acc: 74.96%, Test Loss: 0.0038, Test Acc: 75.95%\n",
      "Epoch 29: Train Loss: 0.0039, Train Acc: 75.10%, Test Loss: 0.0042, Test Acc: 74.96%\n",
      "Epoch 30: Train Loss: 0.0040, Train Acc: 74.94%, Test Loss: 0.0043, Test Acc: 76.39%\n",
      "Epoch 31: Train Loss: 0.0039, Train Acc: 75.20%, Test Loss: 0.0044, Test Acc: 76.12%\n",
      "Epoch 32: Train Loss: 0.0038, Train Acc: 75.38%, Test Loss: 0.0043, Test Acc: 77.26%\n",
      "Epoch 33: Train Loss: 0.0038, Train Acc: 75.52%, Test Loss: 0.0028, Test Acc: 76.99%\n",
      "Epoch 34: Train Loss: 0.0038, Train Acc: 75.53%, Test Loss: 0.0038, Test Acc: 77.38%\n",
      "Epoch 35: Train Loss: 0.0038, Train Acc: 75.69%, Test Loss: 0.0044, Test Acc: 76.92%\n",
      "Epoch 36: Train Loss: 0.0037, Train Acc: 75.65%, Test Loss: 0.0034, Test Acc: 77.26%\n",
      "Epoch 37: Train Loss: 0.0037, Train Acc: 75.77%, Test Loss: 0.0043, Test Acc: 76.91%\n",
      "Epoch 38: Train Loss: 0.0037, Train Acc: 75.79%, Test Loss: 0.0031, Test Acc: 76.71%\n",
      "Epoch 39: Train Loss: 0.0037, Train Acc: 75.99%, Test Loss: 0.0038, Test Acc: 77.59%\n",
      "Epoch 40: Train Loss: 0.0036, Train Acc: 76.08%, Test Loss: 0.0032, Test Acc: 77.78%\n",
      "Epoch 41: Train Loss: 0.0036, Train Acc: 76.11%, Test Loss: 0.0033, Test Acc: 76.79%\n",
      "Epoch 42: Train Loss: 0.0036, Train Acc: 76.09%, Test Loss: 0.0033, Test Acc: 78.52%\n",
      "Epoch 43: Train Loss: 0.0036, Train Acc: 76.26%, Test Loss: 0.0046, Test Acc: 76.95%\n",
      "Epoch 44: Train Loss: 0.0035, Train Acc: 76.24%, Test Loss: 0.0051, Test Acc: 75.78%\n",
      "Epoch 45: Train Loss: 0.0035, Train Acc: 76.35%, Test Loss: 0.0040, Test Acc: 72.59%\n",
      "Epoch 46: Train Loss: 0.0035, Train Acc: 76.41%, Test Loss: 0.0040, Test Acc: 74.83%\n",
      "Epoch 47: Train Loss: 0.0035, Train Acc: 76.40%, Test Loss: 0.0036, Test Acc: 75.75%\n",
      "Epoch 48: Train Loss: 0.0035, Train Acc: 76.51%, Test Loss: 0.0041, Test Acc: 76.47%\n",
      "Epoch 49: Train Loss: 0.0035, Train Acc: 76.51%, Test Loss: 0.0028, Test Acc: 79.36%\n",
      "Epoch 50: Train Loss: 0.0035, Train Acc: 76.42%, Test Loss: 0.0030, Test Acc: 77.85%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_tm_action_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_tm_action_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_tm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_tm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_tm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_tm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_TM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=24, out_features=16, bias=True)\n",
       "    (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.predict(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5]), None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.predict(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.predict(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_DF_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_DF_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(5, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_nodes = 1 - x[:, :10]\n",
    "        x_pod = x[:, 10:]\n",
    "\n",
    "        x = x_nodes - x_pod.repeat(1, 5)\n",
    "        # Average odd and even\n",
    "        x = (x[:, ::2] + x[:, 1::2]) / 2 # 5 vectors -> (cpu_remain + mem_remain) / 2\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_DF_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[32, 32]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_DF_net(\n",
       "      (fc1): Linear(in_features=5, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=32, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_DF_net(\n",
       "      (fc1): Linear(in_features=5, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=32, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_df_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0485,  0.0233,  0.1265, -0.0524, -0.0899, -0.0192]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_DF_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_DF_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_df_model = DQN_DF_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_df_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0298, Train Acc: 47.83%, Test Loss: 0.0272, Test Acc: 51.21%\n",
      "Epoch 2: Train Loss: 0.0269, Train Acc: 52.18%, Test Loss: 0.0268, Test Acc: 51.44%\n",
      "Epoch 3: Train Loss: 0.0265, Train Acc: 52.73%, Test Loss: 0.0265, Test Acc: 51.92%\n",
      "Epoch 4: Train Loss: 0.0262, Train Acc: 53.22%, Test Loss: 0.0264, Test Acc: 52.08%\n",
      "Epoch 5: Train Loss: 0.0261, Train Acc: 53.31%, Test Loss: 0.0262, Test Acc: 52.23%\n",
      "Epoch 6: Train Loss: 0.0260, Train Acc: 53.27%, Test Loss: 0.0261, Test Acc: 52.00%\n",
      "Epoch 7: Train Loss: 0.0259, Train Acc: 53.26%, Test Loss: 0.0260, Test Acc: 51.78%\n",
      "Epoch 8: Train Loss: 0.0258, Train Acc: 53.23%, Test Loss: 0.0259, Test Acc: 51.51%\n",
      "Epoch 9: Train Loss: 0.0257, Train Acc: 53.11%, Test Loss: 0.0259, Test Acc: 51.65%\n",
      "Epoch 10: Train Loss: 0.0257, Train Acc: 53.21%, Test Loss: 0.0258, Test Acc: 51.55%\n",
      "Epoch 11: Train Loss: 0.0256, Train Acc: 53.21%, Test Loss: 0.0258, Test Acc: 51.92%\n",
      "Epoch 12: Train Loss: 0.0256, Train Acc: 53.23%, Test Loss: 0.0257, Test Acc: 51.99%\n",
      "Epoch 13: Train Loss: 0.0256, Train Acc: 53.33%, Test Loss: 0.0257, Test Acc: 52.16%\n",
      "Epoch 14: Train Loss: 0.0256, Train Acc: 53.41%, Test Loss: 0.0256, Test Acc: 52.20%\n",
      "Epoch 15: Train Loss: 0.0256, Train Acc: 53.43%, Test Loss: 0.0256, Test Acc: 52.40%\n",
      "Epoch 16: Train Loss: 0.0255, Train Acc: 53.48%, Test Loss: 0.0256, Test Acc: 52.35%\n",
      "Epoch 17: Train Loss: 0.0255, Train Acc: 53.46%, Test Loss: 0.0255, Test Acc: 52.20%\n",
      "Epoch 18: Train Loss: 0.0255, Train Acc: 53.44%, Test Loss: 0.0255, Test Acc: 52.23%\n",
      "Epoch 19: Train Loss: 0.0255, Train Acc: 53.47%, Test Loss: 0.0255, Test Acc: 52.32%\n",
      "Epoch 20: Train Loss: 0.0255, Train Acc: 53.64%, Test Loss: 0.0255, Test Acc: 52.49%\n",
      "Epoch 21: Train Loss: 0.0255, Train Acc: 53.81%, Test Loss: 0.0255, Test Acc: 52.49%\n",
      "Epoch 22: Train Loss: 0.0255, Train Acc: 53.80%, Test Loss: 0.0254, Test Acc: 52.83%\n",
      "Epoch 23: Train Loss: 0.0254, Train Acc: 53.86%, Test Loss: 0.0254, Test Acc: 53.17%\n",
      "Epoch 24: Train Loss: 0.0254, Train Acc: 53.92%, Test Loss: 0.0254, Test Acc: 53.19%\n",
      "Epoch 25: Train Loss: 0.0254, Train Acc: 53.97%, Test Loss: 0.0254, Test Acc: 53.08%\n",
      "Epoch 26: Train Loss: 0.0254, Train Acc: 54.00%, Test Loss: 0.0254, Test Acc: 53.17%\n",
      "Epoch 27: Train Loss: 0.0254, Train Acc: 54.00%, Test Loss: 0.0254, Test Acc: 53.20%\n",
      "Epoch 28: Train Loss: 0.0254, Train Acc: 53.99%, Test Loss: 0.0254, Test Acc: 53.15%\n",
      "Epoch 29: Train Loss: 0.0254, Train Acc: 53.97%, Test Loss: 0.0254, Test Acc: 53.48%\n",
      "Epoch 30: Train Loss: 0.0254, Train Acc: 53.99%, Test Loss: 0.0254, Test Acc: 53.25%\n",
      "Epoch 31: Train Loss: 0.0254, Train Acc: 53.98%, Test Loss: 0.0253, Test Acc: 53.40%\n",
      "Epoch 32: Train Loss: 0.0254, Train Acc: 53.95%, Test Loss: 0.0253, Test Acc: 53.33%\n",
      "Epoch 33: Train Loss: 0.0254, Train Acc: 53.92%, Test Loss: 0.0253, Test Acc: 53.49%\n",
      "Epoch 34: Train Loss: 0.0254, Train Acc: 53.95%, Test Loss: 0.0253, Test Acc: 53.45%\n",
      "Epoch 35: Train Loss: 0.0254, Train Acc: 53.91%, Test Loss: 0.0253, Test Acc: 53.62%\n",
      "Epoch 36: Train Loss: 0.0254, Train Acc: 53.90%, Test Loss: 0.0253, Test Acc: 53.53%\n",
      "Epoch 37: Train Loss: 0.0254, Train Acc: 53.90%, Test Loss: 0.0253, Test Acc: 53.62%\n",
      "Epoch 38: Train Loss: 0.0254, Train Acc: 53.90%, Test Loss: 0.0253, Test Acc: 53.57%\n",
      "Epoch 39: Train Loss: 0.0254, Train Acc: 53.88%, Test Loss: 0.0253, Test Acc: 53.37%\n",
      "Epoch 40: Train Loss: 0.0254, Train Acc: 53.93%, Test Loss: 0.0253, Test Acc: 53.64%\n",
      "Epoch 41: Train Loss: 0.0254, Train Acc: 53.94%, Test Loss: 0.0253, Test Acc: 53.52%\n",
      "Epoch 42: Train Loss: 0.0254, Train Acc: 53.94%, Test Loss: 0.0253, Test Acc: 53.44%\n",
      "Epoch 43: Train Loss: 0.0253, Train Acc: 54.01%, Test Loss: 0.0252, Test Acc: 53.55%\n",
      "Epoch 44: Train Loss: 0.0253, Train Acc: 54.00%, Test Loss: 0.0253, Test Acc: 53.38%\n",
      "Epoch 45: Train Loss: 0.0253, Train Acc: 54.05%, Test Loss: 0.0252, Test Acc: 53.57%\n",
      "Epoch 46: Train Loss: 0.0253, Train Acc: 54.00%, Test Loss: 0.0252, Test Acc: 53.75%\n",
      "Epoch 47: Train Loss: 0.0253, Train Acc: 54.02%, Test Loss: 0.0252, Test Acc: 53.78%\n",
      "Epoch 48: Train Loss: 0.0253, Train Acc: 53.99%, Test Loss: 0.0252, Test Acc: 53.77%\n",
      "Epoch 49: Train Loss: 0.0253, Train Acc: 54.07%, Test Loss: 0.0252, Test Acc: 53.95%\n",
      "Epoch 50: Train Loss: 0.0253, Train Acc: 54.04%, Test Loss: 0.0252, Test Acc: 53.77%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_df_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_df_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_df_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_df_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_df_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
