{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/swkim/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from kube_sim_gym.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rl_model(scenario_file, rl_model):\n",
    "\n",
    "    test_env1 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "    test_env2 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "\n",
    "    # RL Scheduler\n",
    "    rl_model.set_env(test_env1)\n",
    "\n",
    "    # Default Scheduler\n",
    "    from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "    default_scheduler = SimHrScheduler(test_env2, 'default.py')\n",
    "\n",
    "\n",
    "    # Test the model\n",
    "    obs1 = test_env1.reset()\n",
    "    obs2 = test_env2.reset()\n",
    "    done1 = False\n",
    "    done2 = False\n",
    "    step1 = 0\n",
    "    step2 = 0\n",
    "    acc_rew1 = 0\n",
    "    acc_rew2 = 0\n",
    "\n",
    "    print(f\"Testing with {scenario_file} (my model vs. default)\")\n",
    "    while not done1 or not done2:\n",
    "        if not done1:\n",
    "            action1, _ = rl_model.predict(obs1)\n",
    "            # action1 = rl_scheduler.decision(test_env1)\n",
    "            obs1, reward1, done1, _ = test_env1.step(action1)\n",
    "            step1 += 1\n",
    "            acc_rew1 += reward1\n",
    "        if not done2:\n",
    "            action2 = default_scheduler.decision(test_env2)\n",
    "            obs2, reward2, done2, _ = test_env2.step(action2)\n",
    "            step2 += 1\n",
    "            acc_rew2 += reward2\n",
    "\n",
    "    acc_rew1 = round(acc_rew1, 2)\n",
    "    acc_rew2 = round(acc_rew2, 2)\n",
    "\n",
    "    print(f\"Test result(reward): {acc_rew1} vs. {acc_rew2}\")\n",
    "    print(f\"Test result(step): {step1} vs. {step2}\")\n",
    "\n",
    "    return acc_rew1, acc_rew2, step1, step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from notebook.net_arch import *\n",
    "import glob\n",
    "\n",
    "def train_rl_model(json_tracker_fname):\n",
    "\n",
    "    date = datetime(1992, 7, 5, 8, 33)\n",
    "    date = date.strftime(\"%m%d%Y%H%M\")\n",
    "\n",
    "    log_name = json_tracker_fname.split('.')[0]\n",
    "    log_path = f'training/log/{log_name}'\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "\n",
    "    logger = configure(log_path, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # Load the json tracker\n",
    "    import json\n",
    "    with open(f'training/{json_tracker_fname}', 'r') as f:\n",
    "        json_tracker = json.load(f)\n",
    "\n",
    "    last_idx = json_tracker['last_idx']\n",
    "    learning_steps = json_tracker['learning_steps']\n",
    "    model_type = json_tracker['model_type']\n",
    "    reward_file = json_tracker['reward_file']\n",
    "    model_fname = json_tracker['model_fname']\n",
    "\n",
    "    # Environment\n",
    "    envs = []\n",
    "    for i in range(1, 50):\n",
    "        env = gym.make('SimKubeEnv-v0', reward_file=reward_file, scenario_file=f'trace2017_100_{i}.csv')\n",
    "        envs.append(env)\n",
    "\n",
    "    current_idx = last_idx + 1 # -1 as default\n",
    "\n",
    "    # Model type : DQN or PPO\n",
    "    if model_type == 'DQN':\n",
    "        model = sb3.DQN\n",
    "    elif model_type == 'PPO':\n",
    "        model = sb3.PPO\n",
    "    else:\n",
    "        print(f\"Unknown model type: {model_type}\")\n",
    "        return\n",
    "    \n",
    "    model_fpath = f'net_arch/{model_fname}.zip'\n",
    "\n",
    "    # Check if the model exists\n",
    "    # Load Model\n",
    "    if os.path.exists(model_fpath):\n",
    "        print(f\"Loading the model from {model_fname}\")\n",
    "        model = model.load(model_fpath)\n",
    "    else: # Error\n",
    "        print(f\"Model file does not exist: {model_fname}\")\n",
    "        return\n",
    "    \n",
    "    # If last_idx is not -1 and there's a model trained in training/model, then load the model\n",
    "    if last_idx != -1 and glob.glob(f'training/model/{model_fname}_*'):\n",
    "        # Load the model with the latest date\n",
    "        model_fpaths = glob.glob(f'training/model/{model_fname}_*')\n",
    "        model_fpaths.sort()\n",
    "        model_fpath = model_fpaths[-1]\n",
    "        print(f\"Loading the model from {model_fpath}\")\n",
    "    \n",
    "    # Save the model, append _{date} to the model name\n",
    "    trained_model_fname = f'{model_fname}_{date}'\n",
    "    trained_model_fpath = f'training/model/{trained_model_fname}'\n",
    "\n",
    "    # Set logger\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    # Train the model\n",
    "    while current_idx < 20: # Target training steps (Can be changed!)\n",
    "        print(f\"Training with {current_idx}th trace\")\n",
    "\n",
    "        # Test the model first\n",
    "        a1, a2, a3, a4 = test_rl_model('scenario-5l-5m-1000p-10m_unbalanced.csv', model)\n",
    "        b1, b2, b3, b4 = test_rl_model('scenario-10l-3m-1000p-10m_unbalanced.csv', model)\n",
    "        c1, c2, c3, c4 = test_rl_model('scenario-3l-10m-1000p-10m_unbalanced.csv', model)\n",
    "\n",
    "        with open(f'training/log/{log_name}/test_result.txt', 'a') as f:\n",
    "            f.write(f\"{current_idx},{a1},{a2},{a3},{a4},{b1},{b2},{b3},{b4},{c1},{c2},{c3},{c4}\\n\")\n",
    "\n",
    "        env = envs[current_idx]\n",
    "        model.set_env(env)\n",
    "        model.learn(total_timesteps=learning_steps)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(trained_model_fpath)\n",
    "\n",
    "        # Update the json tracker\n",
    "        json_tracker['last_idx'] = current_idx\n",
    "        with open(f'training/{json_tracker_fname}', 'w') as f:\n",
    "            json.dump(json_tracker, f)\n",
    "\n",
    "        current_idx += 1\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pr_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, train=True):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        # Drop the row which has 0 for the last -2, -3 columns\n",
    "        # self.data = self.data.drop(self.data[(self.data.iloc[:, -2] == 0) & (self.data.iloc[:, -3] == 0)].index)\n",
    "\n",
    "        if train:\n",
    "            self.data = self.data.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            self.data = self.data.drop(self.data.sample(frac=0.8, random_state=42).index)\n",
    "\n",
    "        self.data = self.transform(self.data)\n",
    "        self.input = self.data[:, :-6]\n",
    "        self.label = self.data[:, -6:]\n",
    "\n",
    "    def transform(self, data):\n",
    "        return torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_5.csv\")\n",
    "train_pr_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_pr_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_pr_dataloader = DataLoader(train_pr_dataset, batch_size=64, shuffle=False)\n",
    "test_pr_dataloader = DataLoader(test_pr_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.4900, 0.3800, 0.3000, 0.2500, 0.1600, 0.0700, 0.8000, 0.9600, 0.4500,\n",
      "         0.6900, 0.2300, 0.4600],\n",
      "        [0.9800, 0.2500, 0.2100, 0.7100, 0.0400, 0.8900, 0.4500, 0.5500, 0.0900,\n",
      "         0.8100, 0.3200, 0.4300],\n",
      "        [0.9300, 0.5900, 0.9800, 0.9300, 0.6300, 0.3700, 0.3600, 0.6600, 0.1300,\n",
      "         0.5500, 0.4600, 0.3500],\n",
      "        [0.8200, 0.8000, 0.8800, 0.9900, 0.9200, 0.9500, 0.7900, 0.8000, 0.8100,\n",
      "         0.8200, 0.3600, 0.4400],\n",
      "        [0.5100, 0.0100, 0.8500, 0.8400, 0.6300, 0.4700, 0.8100, 0.6000, 0.2700,\n",
      "         0.0200, 0.1900, 0.2900],\n",
      "        [0.8200, 0.1000, 0.5700, 0.2600, 0.3900, 0.0700, 0.1600, 0.2000, 0.3100,\n",
      "         0.9800, 0.0900, 0.1600],\n",
      "        [0.1100, 0.2800, 0.6300, 0.4700, 0.3300, 0.5200, 0.0700, 0.6300, 0.5000,\n",
      "         0.0600, 0.1100, 0.2100],\n",
      "        [0.3100, 0.1600, 0.4000, 0.6900, 0.4800, 0.4600, 1.0000, 0.1200, 0.7700,\n",
      "         0.7900, 0.2500, 0.5000],\n",
      "        [0.3200, 0.9600, 1.0000, 0.1900, 0.8700, 0.6200, 0.5900, 0.8100, 0.7900,\n",
      "         0.3400, 0.2500, 0.4900],\n",
      "        [0.5900, 0.3100, 0.5100, 0.8500, 0.9900, 0.2600, 0.3200, 0.0700, 0.7600,\n",
      "         0.8000, 0.4200, 0.3200],\n",
      "        [0.2500, 0.9200, 0.8300, 0.9000, 0.0600, 0.1900, 0.2800, 0.3900, 1.0000,\n",
      "         0.3800, 0.0000, 0.0000],\n",
      "        [0.8600, 0.7500, 0.9700, 0.8200, 0.7900, 0.8100, 0.9200, 0.8200, 0.8200,\n",
      "         0.7300, 0.2300, 0.0700],\n",
      "        [0.6100, 0.2400, 0.9700, 0.5400, 0.7600, 0.0200, 0.4400, 0.7200, 0.6900,\n",
      "         0.5300, 0.4200, 0.3500],\n",
      "        [0.9300, 0.9200, 0.7900, 0.9000, 0.9300, 0.7700, 0.7400, 0.9100, 0.8800,\n",
      "         0.9900, 0.3100, 0.4200],\n",
      "        [0.4000, 0.0900, 0.5600, 0.9500, 0.0600, 0.8500, 0.9300, 0.2200, 0.8000,\n",
      "         0.6400, 0.0000, 0.0000],\n",
      "        [0.4300, 0.2800, 0.2300, 0.8700, 0.2100, 0.0800, 0.0900, 0.9000, 0.0900,\n",
      "         0.2600, 0.0700, 0.1500],\n",
      "        [0.6700, 0.8700, 0.2700, 0.1100, 0.5100, 0.5500, 0.1300, 0.7000, 0.7300,\n",
      "         0.4200, 0.0000, 0.0000],\n",
      "        [0.2100, 0.2800, 0.3900, 0.4900, 0.8000, 0.1300, 0.6700, 0.8300, 0.2300,\n",
      "         0.8000, 0.1500, 0.2700],\n",
      "        [0.3200, 0.4100, 0.9900, 0.1900, 0.5600, 0.3500, 0.1800, 0.7200, 0.8800,\n",
      "         0.7500, 0.2400, 0.0800],\n",
      "        [0.8800, 0.1100, 0.6100, 0.7900, 0.0300, 0.4700, 0.4700, 0.8200, 0.9700,\n",
      "         0.5200, 0.3900, 0.4900],\n",
      "        [0.2700, 0.7600, 0.6800, 0.4800, 0.6900, 0.8400, 0.0800, 0.5400, 0.3900,\n",
      "         0.0200, 0.0100, 0.4700],\n",
      "        [0.5000, 0.9100, 0.2500, 0.7100, 0.9900, 0.6200, 0.4200, 0.8500, 0.3700,\n",
      "         0.9900, 0.0000, 0.0000],\n",
      "        [0.3000, 0.7600, 0.4800, 0.2700, 0.6800, 0.3900, 0.9700, 0.0900, 0.4000,\n",
      "         0.3900, 0.4000, 0.2700],\n",
      "        [0.5700, 0.7600, 0.1100, 0.7700, 0.5100, 0.0700, 0.7100, 0.3300, 0.5900,\n",
      "         0.7200, 0.3600, 0.0100],\n",
      "        [0.8700, 0.7700, 0.1300, 0.3500, 0.3800, 0.8600, 0.8000, 0.9000, 0.4700,\n",
      "         0.7200, 0.0200, 0.3900],\n",
      "        [0.2400, 0.9600, 0.8000, 0.4800, 0.7900, 0.1000, 0.2400, 0.8500, 0.0400,\n",
      "         0.8800, 0.3400, 0.4200],\n",
      "        [0.4500, 0.2400, 0.8100, 1.0000, 0.3600, 0.6900, 0.3200, 0.3200, 0.6000,\n",
      "         0.1100, 0.1400, 0.3100],\n",
      "        [0.9400, 0.9000, 0.9700, 0.7100, 0.8600, 0.9500, 0.7300, 0.8800, 0.9400,\n",
      "         0.9400, 0.2600, 0.3300],\n",
      "        [0.9400, 0.6000, 0.5000, 0.7100, 0.6500, 0.2400, 0.6300, 0.5300, 0.7400,\n",
      "         0.8600, 0.0000, 0.0000],\n",
      "        [0.5700, 0.4600, 0.4000, 1.0000, 0.7000, 0.7800, 0.2100, 0.7600, 0.9200,\n",
      "         0.2400, 0.2000, 0.0900],\n",
      "        [0.3700, 0.5200, 0.2500, 0.2900, 0.5200, 0.9000, 0.4700, 0.2600, 0.9900,\n",
      "         0.5900, 0.2900, 0.2900],\n",
      "        [0.4600, 1.0000, 0.3600, 0.5900, 0.0700, 0.5500, 0.4500, 0.7000, 0.9500,\n",
      "         0.1900, 0.0300, 0.1000],\n",
      "        [0.8500, 0.9500, 0.0800, 0.6800, 0.3000, 0.8800, 0.3900, 0.4600, 0.2000,\n",
      "         0.1100, 0.4500, 0.3400],\n",
      "        [0.2200, 0.3500, 0.7300, 0.3000, 0.2700, 0.4800, 0.7200, 0.3500, 0.8200,\n",
      "         0.5800, 0.3500, 0.0500],\n",
      "        [0.8500, 0.6200, 0.7100, 0.0700, 0.6200, 0.3900, 0.5100, 0.7100, 0.2900,\n",
      "         0.7100, 0.4200, 0.2600],\n",
      "        [0.9100, 0.2800, 0.5100, 0.4600, 0.5200, 0.3300, 0.2000, 0.9700, 0.6800,\n",
      "         0.5400, 0.1100, 0.3600],\n",
      "        [0.6900, 0.4600, 0.4100, 0.8400, 0.6800, 0.3900, 0.7200, 0.7400, 0.6500,\n",
      "         0.5800, 0.3600, 0.2600],\n",
      "        [0.0600, 0.3900, 0.5400, 0.1600, 0.7900, 0.3400, 0.7000, 0.2300, 0.4900,\n",
      "         0.8200, 0.0400, 0.1700],\n",
      "        [0.6800, 0.5500, 0.1600, 0.2900, 0.1100, 0.3200, 0.6800, 0.0400, 0.2700,\n",
      "         0.4200, 0.4200, 0.4600],\n",
      "        [0.9800, 0.3700, 0.7400, 0.2400, 0.1400, 0.4400, 0.6600, 0.6500, 0.2200,\n",
      "         0.6900, 0.2800, 0.0500],\n",
      "        [0.1600, 0.0400, 0.1300, 0.4000, 0.6600, 0.7700, 0.8800, 0.3900, 0.4700,\n",
      "         0.8300, 0.4600, 0.4400],\n",
      "        [0.1700, 0.6900, 0.6600, 0.7400, 0.2800, 0.5900, 0.8500, 0.4500, 0.3700,\n",
      "         0.9700, 0.1300, 0.1200],\n",
      "        [0.4000, 0.3200, 0.5100, 0.0400, 0.8600, 0.4500, 0.2200, 0.5600, 0.2300,\n",
      "         0.9100, 0.0000, 0.0000],\n",
      "        [0.0400, 0.3400, 0.5300, 0.3400, 0.6000, 0.6900, 0.4100, 0.9100, 0.0600,\n",
      "         0.5100, 0.4700, 0.2300],\n",
      "        [0.8700, 0.9700, 1.0000, 0.9400, 0.8500, 0.8900, 0.8000, 0.9700, 0.9400,\n",
      "         0.8900, 0.3400, 0.1100],\n",
      "        [0.4500, 0.9300, 0.4900, 0.6300, 0.9900, 0.9800, 0.1700, 0.8000, 0.6600,\n",
      "         0.8300, 0.2900, 0.1300],\n",
      "        [0.5400, 0.1000, 0.4900, 0.1100, 0.3700, 0.6700, 0.3400, 0.9600, 0.4000,\n",
      "         0.5000, 0.2800, 0.3900],\n",
      "        [0.0500, 0.6700, 0.0200, 0.8900, 0.0500, 0.1700, 0.1300, 0.0200, 0.1500,\n",
      "         0.4100, 0.1300, 0.0100],\n",
      "        [0.3000, 0.4400, 0.0100, 0.5500, 0.8400, 0.3400, 0.0800, 0.7600, 0.7100,\n",
      "         0.5300, 0.3600, 0.0600],\n",
      "        [0.2000, 0.2300, 0.3900, 0.4700, 0.3600, 0.2400, 0.9800, 0.2200, 0.3200,\n",
      "         0.3300, 0.1100, 0.1200],\n",
      "        [0.6900, 0.6000, 0.6300, 0.5200, 0.3500, 0.2500, 0.7400, 0.3600, 0.7900,\n",
      "         0.6700, 0.2700, 0.2400],\n",
      "        [0.0200, 0.3400, 0.9500, 0.2300, 0.3000, 0.4500, 0.2200, 0.0200, 0.4600,\n",
      "         0.3600, 0.4800, 0.1800],\n",
      "        [0.3400, 0.8500, 0.9700, 0.1000, 0.9800, 0.5200, 0.8000, 0.0500, 0.4600,\n",
      "         0.7900, 0.1100, 0.1400],\n",
      "        [0.7300, 0.7600, 0.9000, 0.9400, 0.8900, 0.8100, 0.7900, 0.9500, 0.7200,\n",
      "         0.7600, 0.0200, 0.1500],\n",
      "        [0.8600, 0.6800, 0.8300, 0.3800, 0.0800, 0.6500, 0.8800, 0.1900, 0.8200,\n",
      "         0.1800, 0.4900, 0.2900],\n",
      "        [0.6500, 0.2500, 0.3200, 0.7000, 0.6000, 0.7500, 0.9100, 0.0200, 0.0500,\n",
      "         0.8600, 0.0100, 0.3600],\n",
      "        [0.8500, 0.5800, 0.9000, 0.7700, 0.7100, 0.5300, 0.8700, 0.3100, 0.4900,\n",
      "         0.2900, 0.1300, 0.1800],\n",
      "        [0.1400, 0.7500, 0.6400, 0.1200, 0.9700, 0.6600, 0.0800, 0.1100, 0.3500,\n",
      "         0.0200, 0.0000, 0.0000],\n",
      "        [0.4700, 0.4800, 0.7000, 0.3100, 0.4000, 0.1600, 0.4500, 0.7600, 0.1800,\n",
      "         0.5400, 0.4400, 0.3700],\n",
      "        [0.7300, 0.9600, 0.9600, 0.9600, 0.8000, 0.9500, 0.7800, 0.8800, 0.7100,\n",
      "         0.8000, 0.4400, 0.4800],\n",
      "        [0.9300, 0.7000, 0.8000, 0.9300, 0.8200, 0.7100, 0.8600, 0.8600, 0.9000,\n",
      "         0.8500, 0.4600, 0.0200],\n",
      "        [0.7600, 0.6800, 0.5400, 0.4300, 0.9500, 0.7600, 0.3500, 0.5200, 0.5100,\n",
      "         0.7800, 0.1200, 0.1700],\n",
      "        [0.9700, 0.7300, 0.7800, 0.8300, 0.9900, 0.7500, 0.8700, 0.9500, 0.8000,\n",
      "         0.9200, 0.4900, 0.1000],\n",
      "        [0.8400, 0.7900, 0.8300, 0.9000, 0.8000, 0.9400, 1.0000, 0.8300, 0.9400,\n",
      "         0.7600, 0.2500, 0.4800]])\n",
      "labels: tensor([[-0.1600, -0.1500, -0.1100,  0.0000, -0.6600, -0.6600],\n",
      "        [ 0.0200, -0.4800, -0.4800, -0.4800,  0.0000, -0.4800],\n",
      "        [-0.1700, -0.6700, -0.6700, -0.6700, -0.6700,  0.0000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.1700, -0.0500, -0.6700, -0.1500, -0.2000,  0.0000],\n",
      "        [-0.0300, -0.0100, -0.0100,  0.0200,  0.0000, -0.5300],\n",
      "        [-0.0400,  0.0000, -0.0500, -0.0700, -0.0600,  0.0500],\n",
      "        [-0.1500,  0.0000, -0.6500, -0.1800, -0.6500, -0.6500],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.1800, -0.6800, -0.6800, -0.6800,  0.0000, -0.6800],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0700, -0.0400, -0.5700,  0.0000, -0.5700, -0.0400],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0700,  0.0000, -0.0700,  0.0100, -0.5700, -0.5700],\n",
      "        [-0.0800,  0.0000, -0.5800, -0.0800,  0.0200, -0.5800],\n",
      "        [-0.1400, -0.6400, -0.6400,  0.0000, -0.6400, -0.6400],\n",
      "        [-0.2400, -0.7400, -0.2500, -0.7400, -0.7400,  0.0000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0100, -0.5100,  0.0100, -0.5100, -0.5100,  0.0000],\n",
      "        [ 0.0800,  0.0600,  0.3100,  0.0000, -0.4200,  0.0200],\n",
      "        [-0.1000, -0.6000,  0.0000, -0.6000, -0.6000, -0.6000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0300,  0.0700, -0.5300, -0.0600,  0.0000,  0.0700],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0100,  0.0000, -0.4900,  0.0000,  0.1000, -0.4900],\n",
      "        [-0.1200, -0.0900,  0.0000, -0.6200, -0.0600, -0.6200],\n",
      "        [ 0.0000, -0.5000,  0.0000,  0.0000, -0.0100,  0.0500],\n",
      "        [-0.1500, -0.6500, -0.6500, -0.6500, -0.1200,  0.0000],\n",
      "        [-0.1000,  0.0000, -0.6000,  0.0000, -0.6000, -0.6000],\n",
      "        [-0.0100, -0.5100, -0.5100, -0.5100,  0.0000,  0.1100],\n",
      "        [ 0.0100, -0.4900,  0.0000,  0.1000, -0.4900,  0.0000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0100,  0.0000,  0.0700,  0.0400,  0.0500, -0.0700],\n",
      "        [-0.0300, -0.5300,  0.0000,  0.0100, -0.5300, -0.0800],\n",
      "        [-0.1500, -0.6500, -0.6500,  0.0000, -0.2000, -0.0300],\n",
      "        [-0.2300,  0.0000, -0.1100, -0.7300, -0.7300, -0.7300],\n",
      "        [-0.0300,  0.0100, -0.0600,  0.0000, -0.0200, -0.5300],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.2000,  0.0000, -0.2600, -0.7000, -0.7000, -0.0500],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.1100, -0.6100,  0.0000, -0.6100,  0.0500, -0.0700],\n",
      "        [ 0.0400,  0.0800,  0.1000, -0.4600, -0.4600,  0.0000],\n",
      "        [ 0.0500,  0.0800,  0.0900,  0.0800,  0.0000,  0.0500],\n",
      "        [-0.0400,  0.0000,  0.1100, -0.5400,  0.0700, -0.5400],\n",
      "        [-0.0400,  0.0000, -0.0600, -0.0100, -0.5400, -0.0300],\n",
      "        [-0.1800, -0.2500, -0.2200,  0.0000, -0.6800, -0.6800],\n",
      "        [-0.0500,  0.1000, -0.5500, -0.0700,  0.0000, -0.1600],\n",
      "        [ 0.0100,  0.0100, -0.4900, -0.4900,  0.0600,  0.0000],\n",
      "        [ 0.0000,  0.0000, -0.5000,  0.0200, -0.5000,  0.0000],\n",
      "        [-0.2400, -0.7400, -0.7400,  0.0000, -0.7400, -0.7400],\n",
      "        [-0.1300,  0.0000, -0.6300, -0.6300,  0.0700, -0.6300],\n",
      "        [-0.1100, -0.1200, -0.6100, -0.0900, -0.0700,  0.0000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0600,  0.0000, -0.4400,  0.1300, -0.4400,  0.1400],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0800, -0.0800,  0.0000, -0.5800, -0.0200, -0.0900],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_pr_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Multi-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0008,  0.0047, -0.0008, -0.0017, -0.0088,  0.0065]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1614, Train Acc: 69.41%, Test Loss: 0.1331, Test Acc: 74.33%\n",
      "Epoch 2: Train Loss: 0.1242, Train Acc: 75.68%, Test Loss: 0.1221, Test Acc: 75.80%\n",
      "Epoch 3: Train Loss: 0.1152, Train Acc: 76.85%, Test Loss: 0.1026, Test Acc: 78.37%\n",
      "Epoch 4: Train Loss: 0.0961, Train Acc: 79.07%, Test Loss: 0.0929, Test Acc: 79.35%\n",
      "Epoch 5: Train Loss: 0.0900, Train Acc: 79.99%, Test Loss: 0.0903, Test Acc: 79.55%\n",
      "Epoch 6: Train Loss: 0.0872, Train Acc: 80.30%, Test Loss: 0.0872, Test Acc: 79.63%\n",
      "Epoch 7: Train Loss: 0.0854, Train Acc: 80.50%, Test Loss: 0.0857, Test Acc: 79.87%\n",
      "Epoch 8: Train Loss: 0.0845, Train Acc: 80.65%, Test Loss: 0.0846, Test Acc: 80.07%\n",
      "Epoch 9: Train Loss: 0.0836, Train Acc: 80.73%, Test Loss: 0.0846, Test Acc: 80.15%\n",
      "Epoch 10: Train Loss: 0.0832, Train Acc: 80.77%, Test Loss: 0.0847, Test Acc: 80.10%\n",
      "Epoch 11: Train Loss: 0.0825, Train Acc: 80.87%, Test Loss: 0.0844, Test Acc: 80.17%\n",
      "Epoch 12: Train Loss: 0.0819, Train Acc: 80.94%, Test Loss: 0.0836, Test Acc: 80.35%\n",
      "Epoch 13: Train Loss: 0.0815, Train Acc: 80.98%, Test Loss: 0.0833, Test Acc: 80.50%\n",
      "Epoch 14: Train Loss: 0.0811, Train Acc: 81.07%, Test Loss: 0.0837, Test Acc: 80.47%\n",
      "Epoch 15: Train Loss: 0.0808, Train Acc: 81.13%, Test Loss: 0.0847, Test Acc: 80.38%\n",
      "Epoch 16: Train Loss: 0.0806, Train Acc: 81.14%, Test Loss: 0.0840, Test Acc: 80.36%\n",
      "Epoch 17: Train Loss: 0.0802, Train Acc: 81.21%, Test Loss: 0.0840, Test Acc: 80.33%\n",
      "Epoch 18: Train Loss: 0.0800, Train Acc: 81.26%, Test Loss: 0.0842, Test Acc: 80.32%\n",
      "Epoch 19: Train Loss: 0.0798, Train Acc: 81.29%, Test Loss: 0.0846, Test Acc: 80.45%\n",
      "Epoch 20: Train Loss: 0.0796, Train Acc: 81.28%, Test Loss: 0.0834, Test Acc: 80.59%\n",
      "Epoch 21: Train Loss: 0.0792, Train Acc: 81.34%, Test Loss: 0.0835, Test Acc: 80.60%\n",
      "Epoch 22: Train Loss: 0.0789, Train Acc: 81.39%, Test Loss: 0.0831, Test Acc: 80.70%\n",
      "Epoch 23: Train Loss: 0.0786, Train Acc: 81.42%, Test Loss: 0.0823, Test Acc: 80.61%\n",
      "Epoch 24: Train Loss: 0.0784, Train Acc: 81.44%, Test Loss: 0.0798, Test Acc: 81.13%\n",
      "Epoch 25: Train Loss: 0.0782, Train Acc: 81.46%, Test Loss: 0.0782, Test Acc: 81.35%\n",
      "Epoch 26: Train Loss: 0.0779, Train Acc: 81.49%, Test Loss: 0.0773, Test Acc: 81.52%\n",
      "Epoch 27: Train Loss: 0.0777, Train Acc: 81.49%, Test Loss: 0.0796, Test Acc: 81.05%\n",
      "Epoch 28: Train Loss: 0.0776, Train Acc: 81.49%, Test Loss: 0.0771, Test Acc: 81.37%\n",
      "Epoch 29: Train Loss: 0.0775, Train Acc: 81.48%, Test Loss: 0.0790, Test Acc: 81.18%\n",
      "Epoch 30: Train Loss: 0.0773, Train Acc: 81.50%, Test Loss: 0.0767, Test Acc: 81.50%\n",
      "Epoch 31: Train Loss: 0.0772, Train Acc: 81.49%, Test Loss: 0.0783, Test Acc: 81.30%\n",
      "Epoch 32: Train Loss: 0.0771, Train Acc: 81.49%, Test Loss: 0.0772, Test Acc: 81.31%\n",
      "Epoch 33: Train Loss: 0.0769, Train Acc: 81.49%, Test Loss: 0.0767, Test Acc: 81.39%\n",
      "Epoch 34: Train Loss: 0.0767, Train Acc: 81.54%, Test Loss: 0.0766, Test Acc: 81.44%\n",
      "Epoch 35: Train Loss: 0.0767, Train Acc: 81.54%, Test Loss: 0.0768, Test Acc: 81.12%\n",
      "Epoch 36: Train Loss: 0.0766, Train Acc: 81.55%, Test Loss: 0.0763, Test Acc: 81.36%\n",
      "Epoch 37: Train Loss: 0.0765, Train Acc: 81.58%, Test Loss: 0.0766, Test Acc: 81.27%\n",
      "Epoch 38: Train Loss: 0.0763, Train Acc: 81.53%, Test Loss: 0.0757, Test Acc: 81.47%\n",
      "Epoch 39: Train Loss: 0.0763, Train Acc: 81.58%, Test Loss: 0.0765, Test Acc: 81.19%\n",
      "Epoch 40: Train Loss: 0.0762, Train Acc: 81.55%, Test Loss: 0.0765, Test Acc: 81.38%\n",
      "Epoch 41: Train Loss: 0.0761, Train Acc: 81.60%, Test Loss: 0.0774, Test Acc: 81.30%\n",
      "Epoch 42: Train Loss: 0.0759, Train Acc: 81.59%, Test Loss: 0.0761, Test Acc: 81.51%\n",
      "Epoch 43: Train Loss: 0.0760, Train Acc: 81.61%, Test Loss: 0.0772, Test Acc: 81.26%\n",
      "Epoch 44: Train Loss: 0.0759, Train Acc: 81.60%, Test Loss: 0.0759, Test Acc: 81.51%\n",
      "Epoch 45: Train Loss: 0.0758, Train Acc: 81.64%, Test Loss: 0.0749, Test Acc: 81.67%\n",
      "Epoch 46: Train Loss: 0.0758, Train Acc: 81.65%, Test Loss: 0.0767, Test Acc: 81.40%\n",
      "Epoch 47: Train Loss: 0.0756, Train Acc: 81.66%, Test Loss: 0.0759, Test Acc: 81.43%\n",
      "Epoch 48: Train Loss: 0.0755, Train Acc: 81.68%, Test Loss: 0.0751, Test Acc: 81.52%\n",
      "Epoch 49: Train Loss: 0.0755, Train Acc: 81.64%, Test Loss: 0.0750, Test Acc: 81.71%\n",
      "Epoch 50: Train Loss: 0.0755, Train Acc: 81.65%, Test Loss: 0.0784, Test Acc: 81.09%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc3 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)     # (batch_size, 16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_SM_net(\n",
       "    (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_SM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_SM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_sm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_sm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1362, Train Acc: 75.55%, Test Loss: 0.0727, Test Acc: 83.49%\n",
      "Epoch 2: Train Loss: 0.0657, Train Acc: 84.01%, Test Loss: 0.0567, Test Acc: 84.93%\n",
      "Epoch 3: Train Loss: 0.0554, Train Acc: 84.98%, Test Loss: 0.0508, Test Acc: 85.30%\n",
      "Epoch 4: Train Loss: 0.0502, Train Acc: 85.52%, Test Loss: 0.0474, Test Acc: 85.72%\n",
      "Epoch 5: Train Loss: 0.0472, Train Acc: 85.82%, Test Loss: 0.0452, Test Acc: 86.10%\n",
      "Epoch 6: Train Loss: 0.0454, Train Acc: 85.98%, Test Loss: 0.0418, Test Acc: 86.52%\n",
      "Epoch 7: Train Loss: 0.0440, Train Acc: 86.16%, Test Loss: 0.0419, Test Acc: 86.21%\n",
      "Epoch 8: Train Loss: 0.0433, Train Acc: 86.28%, Test Loss: 0.0455, Test Acc: 85.97%\n",
      "Epoch 9: Train Loss: 0.0426, Train Acc: 86.37%, Test Loss: 0.0417, Test Acc: 86.35%\n",
      "Epoch 10: Train Loss: 0.0419, Train Acc: 86.46%, Test Loss: 0.0410, Test Acc: 86.68%\n",
      "Epoch 11: Train Loss: 0.0409, Train Acc: 86.58%, Test Loss: 0.0415, Test Acc: 86.47%\n",
      "Epoch 12: Train Loss: 0.0403, Train Acc: 86.74%, Test Loss: 0.0385, Test Acc: 86.85%\n",
      "Epoch 13: Train Loss: 0.0398, Train Acc: 86.87%, Test Loss: 0.0392, Test Acc: 86.97%\n",
      "Epoch 14: Train Loss: 0.0390, Train Acc: 87.03%, Test Loss: 0.0451, Test Acc: 86.75%\n",
      "Epoch 15: Train Loss: 0.0385, Train Acc: 87.13%, Test Loss: 0.0381, Test Acc: 87.09%\n",
      "Epoch 16: Train Loss: 0.0383, Train Acc: 87.16%, Test Loss: 0.0386, Test Acc: 87.15%\n",
      "Epoch 17: Train Loss: 0.0379, Train Acc: 87.28%, Test Loss: 0.0378, Test Acc: 87.20%\n",
      "Epoch 18: Train Loss: 0.0375, Train Acc: 87.31%, Test Loss: 0.0357, Test Acc: 87.51%\n",
      "Epoch 19: Train Loss: 0.0374, Train Acc: 87.38%, Test Loss: 0.0374, Test Acc: 87.22%\n",
      "Epoch 20: Train Loss: 0.0369, Train Acc: 87.47%, Test Loss: 0.0373, Test Acc: 87.19%\n",
      "Epoch 21: Train Loss: 0.0367, Train Acc: 87.51%, Test Loss: 0.0355, Test Acc: 87.69%\n",
      "Epoch 22: Train Loss: 0.0371, Train Acc: 87.48%, Test Loss: 0.0357, Test Acc: 87.12%\n",
      "Epoch 23: Train Loss: 0.0366, Train Acc: 87.55%, Test Loss: 0.0407, Test Acc: 87.05%\n",
      "Epoch 24: Train Loss: 0.0364, Train Acc: 87.59%, Test Loss: 0.0405, Test Acc: 87.25%\n",
      "Epoch 25: Train Loss: 0.0361, Train Acc: 87.58%, Test Loss: 0.0382, Test Acc: 87.46%\n",
      "Epoch 26: Train Loss: 0.0360, Train Acc: 87.65%, Test Loss: 0.0396, Test Acc: 87.12%\n",
      "Epoch 27: Train Loss: 0.0357, Train Acc: 87.65%, Test Loss: 0.0466, Test Acc: 86.28%\n",
      "Epoch 28: Train Loss: 0.0354, Train Acc: 87.71%, Test Loss: 0.0366, Test Acc: 87.30%\n",
      "Epoch 29: Train Loss: 0.0353, Train Acc: 87.69%, Test Loss: 0.0335, Test Acc: 87.58%\n",
      "Epoch 30: Train Loss: 0.0353, Train Acc: 87.69%, Test Loss: 0.0361, Test Acc: 87.40%\n",
      "Epoch 31: Train Loss: 0.0353, Train Acc: 87.70%, Test Loss: 0.0350, Test Acc: 87.41%\n",
      "Epoch 32: Train Loss: 0.0348, Train Acc: 87.79%, Test Loss: 0.0381, Test Acc: 87.04%\n",
      "Epoch 33: Train Loss: 0.0348, Train Acc: 87.79%, Test Loss: 0.0337, Test Acc: 87.70%\n",
      "Epoch 34: Train Loss: 0.0346, Train Acc: 87.87%, Test Loss: 0.0344, Test Acc: 87.63%\n",
      "Epoch 35: Train Loss: 0.0345, Train Acc: 87.81%, Test Loss: 0.0334, Test Acc: 87.85%\n",
      "Epoch 36: Train Loss: 0.0345, Train Acc: 87.84%, Test Loss: 0.0360, Test Acc: 87.58%\n",
      "Epoch 37: Train Loss: 0.0346, Train Acc: 87.82%, Test Loss: 0.0415, Test Acc: 87.14%\n",
      "Epoch 38: Train Loss: 0.0341, Train Acc: 87.87%, Test Loss: 0.0351, Test Acc: 87.53%\n",
      "Epoch 39: Train Loss: 0.0346, Train Acc: 87.87%, Test Loss: 0.0339, Test Acc: 87.78%\n",
      "Epoch 40: Train Loss: 0.0339, Train Acc: 87.91%, Test Loss: 0.0336, Test Acc: 87.68%\n",
      "Epoch 41: Train Loss: 0.0338, Train Acc: 87.93%, Test Loss: 0.0334, Test Acc: 87.90%\n",
      "Epoch 42: Train Loss: 0.0339, Train Acc: 87.90%, Test Loss: 0.0342, Test Acc: 87.88%\n",
      "Epoch 43: Train Loss: 0.0338, Train Acc: 87.98%, Test Loss: 0.0349, Test Acc: 87.92%\n",
      "Epoch 44: Train Loss: 0.0336, Train Acc: 87.92%, Test Loss: 0.0353, Test Acc: 87.87%\n",
      "Epoch 45: Train Loss: 0.0335, Train Acc: 87.98%, Test Loss: 0.0334, Test Acc: 87.97%\n",
      "Epoch 46: Train Loss: 0.0338, Train Acc: 87.97%, Test Loss: 0.0340, Test Acc: 87.94%\n",
      "Epoch 47: Train Loss: 0.0331, Train Acc: 88.03%, Test Loss: 0.0344, Test Acc: 88.07%\n",
      "Epoch 48: Train Loss: 0.0334, Train Acc: 88.00%, Test Loss: 0.0335, Test Acc: 87.96%\n",
      "Epoch 49: Train Loss: 0.0332, Train Acc: 88.01%, Test Loss: 0.0307, Test Acc: 88.41%\n",
      "Epoch 50: Train Loss: 0.0342, Train Acc: 87.97%, Test Loss: 0.0333, Test Acc: 88.18%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_sm_action_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_sm_action_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_sm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_sm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_sm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Multi-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0019, -0.1223,  0.0332, -0.0266, -0.0474, -0.0251]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_MM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_MM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_mm_model = DQN_MM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_mm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1929, Train Acc: 61.92%, Test Loss: 0.1382, Test Acc: 73.71%\n",
      "Epoch 2: Train Loss: 0.1290, Train Acc: 74.93%, Test Loss: 0.1304, Test Acc: 74.60%\n",
      "Epoch 3: Train Loss: 0.1254, Train Acc: 75.37%, Test Loss: 0.1292, Test Acc: 74.84%\n",
      "Epoch 4: Train Loss: 0.1240, Train Acc: 75.56%, Test Loss: 0.1261, Test Acc: 75.12%\n",
      "Epoch 5: Train Loss: 0.1226, Train Acc: 75.73%, Test Loss: 0.1239, Test Acc: 75.25%\n",
      "Epoch 6: Train Loss: 0.1212, Train Acc: 75.81%, Test Loss: 0.1223, Test Acc: 75.21%\n",
      "Epoch 7: Train Loss: 0.1200, Train Acc: 75.88%, Test Loss: 0.1216, Test Acc: 75.15%\n",
      "Epoch 8: Train Loss: 0.1191, Train Acc: 75.99%, Test Loss: 0.1207, Test Acc: 75.11%\n",
      "Epoch 9: Train Loss: 0.1184, Train Acc: 76.04%, Test Loss: 0.1200, Test Acc: 75.21%\n",
      "Epoch 10: Train Loss: 0.1179, Train Acc: 76.13%, Test Loss: 0.1190, Test Acc: 75.27%\n",
      "Epoch 11: Train Loss: 0.1175, Train Acc: 76.18%, Test Loss: 0.1189, Test Acc: 75.39%\n",
      "Epoch 12: Train Loss: 0.1172, Train Acc: 76.22%, Test Loss: 0.1182, Test Acc: 75.43%\n",
      "Epoch 13: Train Loss: 0.1169, Train Acc: 76.23%, Test Loss: 0.1182, Test Acc: 75.36%\n",
      "Epoch 14: Train Loss: 0.1167, Train Acc: 76.28%, Test Loss: 0.1177, Test Acc: 75.58%\n",
      "Epoch 15: Train Loss: 0.1165, Train Acc: 76.30%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 16: Train Loss: 0.1163, Train Acc: 76.34%, Test Loss: 0.1167, Test Acc: 75.63%\n",
      "Epoch 17: Train Loss: 0.1161, Train Acc: 76.35%, Test Loss: 0.1168, Test Acc: 75.69%\n",
      "Epoch 18: Train Loss: 0.1160, Train Acc: 76.33%, Test Loss: 0.1167, Test Acc: 75.55%\n",
      "Epoch 19: Train Loss: 0.1159, Train Acc: 76.36%, Test Loss: 0.1167, Test Acc: 75.64%\n",
      "Epoch 20: Train Loss: 0.1158, Train Acc: 76.33%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 21: Train Loss: 0.1156, Train Acc: 76.35%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 22: Train Loss: 0.1152, Train Acc: 76.41%, Test Loss: 0.1150, Test Acc: 75.92%\n",
      "Epoch 23: Train Loss: 0.1128, Train Acc: 76.72%, Test Loss: 0.1135, Test Acc: 76.14%\n",
      "Epoch 24: Train Loss: 0.1115, Train Acc: 76.92%, Test Loss: 0.1135, Test Acc: 76.12%\n",
      "Epoch 25: Train Loss: 0.1112, Train Acc: 76.98%, Test Loss: 0.1133, Test Acc: 76.23%\n",
      "Epoch 26: Train Loss: 0.1108, Train Acc: 77.00%, Test Loss: 0.1127, Test Acc: 76.39%\n",
      "Epoch 27: Train Loss: 0.1105, Train Acc: 77.07%, Test Loss: 0.1120, Test Acc: 76.35%\n",
      "Epoch 28: Train Loss: 0.1103, Train Acc: 77.07%, Test Loss: 0.1109, Test Acc: 76.57%\n",
      "Epoch 29: Train Loss: 0.1100, Train Acc: 77.09%, Test Loss: 0.1109, Test Acc: 76.51%\n",
      "Epoch 30: Train Loss: 0.1097, Train Acc: 77.11%, Test Loss: 0.1108, Test Acc: 76.45%\n",
      "Epoch 31: Train Loss: 0.1094, Train Acc: 77.10%, Test Loss: 0.1096, Test Acc: 76.66%\n",
      "Epoch 32: Train Loss: 0.1092, Train Acc: 77.12%, Test Loss: 0.1093, Test Acc: 76.56%\n",
      "Epoch 33: Train Loss: 0.1066, Train Acc: 77.18%, Test Loss: 0.1025, Test Acc: 77.42%\n",
      "Epoch 34: Train Loss: 0.1005, Train Acc: 77.90%, Test Loss: 0.0993, Test Acc: 77.65%\n",
      "Epoch 35: Train Loss: 0.0979, Train Acc: 78.25%, Test Loss: 0.0961, Test Acc: 78.25%\n",
      "Epoch 36: Train Loss: 0.0934, Train Acc: 78.84%, Test Loss: 0.0913, Test Acc: 78.82%\n",
      "Epoch 37: Train Loss: 0.0862, Train Acc: 79.80%, Test Loss: 0.0841, Test Acc: 79.70%\n",
      "Epoch 38: Train Loss: 0.0812, Train Acc: 80.52%, Test Loss: 0.0805, Test Acc: 80.22%\n",
      "Epoch 39: Train Loss: 0.0791, Train Acc: 80.73%, Test Loss: 0.0779, Test Acc: 80.58%\n",
      "Epoch 40: Train Loss: 0.0778, Train Acc: 80.86%, Test Loss: 0.0775, Test Acc: 80.64%\n",
      "Epoch 41: Train Loss: 0.0767, Train Acc: 80.97%, Test Loss: 0.0758, Test Acc: 81.15%\n",
      "Epoch 42: Train Loss: 0.0757, Train Acc: 81.09%, Test Loss: 0.0743, Test Acc: 81.30%\n",
      "Epoch 43: Train Loss: 0.0747, Train Acc: 81.21%, Test Loss: 0.0731, Test Acc: 81.43%\n",
      "Epoch 44: Train Loss: 0.0739, Train Acc: 81.34%, Test Loss: 0.0726, Test Acc: 81.61%\n",
      "Epoch 45: Train Loss: 0.0733, Train Acc: 81.46%, Test Loss: 0.0722, Test Acc: 81.69%\n",
      "Epoch 46: Train Loss: 0.0726, Train Acc: 81.58%, Test Loss: 0.0721, Test Acc: 81.74%\n",
      "Epoch 47: Train Loss: 0.0719, Train Acc: 81.67%, Test Loss: 0.0715, Test Acc: 81.75%\n",
      "Epoch 48: Train Loss: 0.0711, Train Acc: 81.73%, Test Loss: 0.0695, Test Acc: 81.96%\n",
      "Epoch 49: Train Loss: 0.0704, Train Acc: 81.79%, Test Loss: 0.0718, Test Acc: 81.53%\n",
      "Epoch 50: Train Loss: 0.0698, Train Acc: 81.80%, Test Loss: 0.0701, Test Acc: 81.77%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_mm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_mm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_mm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_mm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5014, -0.9858, -1.0000, -1.0103, -1.0104, -0.9910]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5059, -0.9256, -0.9021, -1.0459, -1.0538,  0.6572]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net(sample2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_SM_net(\n",
       "      (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_SM_net(\n",
       "      (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1106,  0.0206, -0.0055, -0.0425,  0.0521,  0.1534]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_SM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_SM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_sm_model = DQN_SM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_sm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1568, Train Acc: 71.72%, Test Loss: 0.1036, Test Acc: 77.05%\n",
      "Epoch 2: Train Loss: 0.0836, Train Acc: 80.46%, Test Loss: 0.0699, Test Acc: 82.15%\n",
      "Epoch 3: Train Loss: 0.0595, Train Acc: 84.28%, Test Loss: 0.0589, Test Acc: 83.94%\n",
      "Epoch 4: Train Loss: 0.0523, Train Acc: 85.02%, Test Loss: 0.0518, Test Acc: 84.52%\n",
      "Epoch 5: Train Loss: 0.0478, Train Acc: 85.62%, Test Loss: 0.0496, Test Acc: 84.93%\n",
      "Epoch 6: Train Loss: 0.0442, Train Acc: 86.09%, Test Loss: 0.0467, Test Acc: 85.73%\n",
      "Epoch 7: Train Loss: 0.0420, Train Acc: 86.35%, Test Loss: 0.0453, Test Acc: 85.95%\n",
      "Epoch 8: Train Loss: 0.0407, Train Acc: 86.60%, Test Loss: 0.0431, Test Acc: 86.51%\n",
      "Epoch 9: Train Loss: 0.0398, Train Acc: 86.77%, Test Loss: 0.0402, Test Acc: 86.71%\n",
      "Epoch 10: Train Loss: 0.0390, Train Acc: 86.93%, Test Loss: 0.0392, Test Acc: 86.96%\n",
      "Epoch 11: Train Loss: 0.0382, Train Acc: 87.06%, Test Loss: 0.0376, Test Acc: 87.78%\n",
      "Epoch 12: Train Loss: 0.0375, Train Acc: 87.15%, Test Loss: 0.0419, Test Acc: 86.73%\n",
      "Epoch 13: Train Loss: 0.0371, Train Acc: 87.21%, Test Loss: 0.0396, Test Acc: 87.32%\n",
      "Epoch 14: Train Loss: 0.0367, Train Acc: 87.25%, Test Loss: 0.0408, Test Acc: 87.11%\n",
      "Epoch 15: Train Loss: 0.0363, Train Acc: 87.34%, Test Loss: 0.0363, Test Acc: 87.53%\n",
      "Epoch 16: Train Loss: 0.0360, Train Acc: 87.42%, Test Loss: 0.0391, Test Acc: 87.18%\n",
      "Epoch 17: Train Loss: 0.0357, Train Acc: 87.49%, Test Loss: 0.0373, Test Acc: 87.63%\n",
      "Epoch 18: Train Loss: 0.0353, Train Acc: 87.57%, Test Loss: 0.0356, Test Acc: 87.73%\n",
      "Epoch 19: Train Loss: 0.0351, Train Acc: 87.65%, Test Loss: 0.0355, Test Acc: 87.56%\n",
      "Epoch 20: Train Loss: 0.0348, Train Acc: 87.68%, Test Loss: 0.0400, Test Acc: 87.33%\n",
      "Epoch 21: Train Loss: 0.0345, Train Acc: 87.68%, Test Loss: 0.0362, Test Acc: 87.69%\n",
      "Epoch 22: Train Loss: 0.0345, Train Acc: 87.67%, Test Loss: 0.0380, Test Acc: 87.60%\n",
      "Epoch 23: Train Loss: 0.0340, Train Acc: 87.78%, Test Loss: 0.0403, Test Acc: 87.32%\n",
      "Epoch 24: Train Loss: 0.0339, Train Acc: 87.79%, Test Loss: 0.0373, Test Acc: 87.85%\n",
      "Epoch 25: Train Loss: 0.0338, Train Acc: 87.79%, Test Loss: 0.0378, Test Acc: 87.85%\n",
      "Epoch 26: Train Loss: 0.0334, Train Acc: 87.88%, Test Loss: 0.0393, Test Acc: 87.69%\n",
      "Epoch 27: Train Loss: 0.0330, Train Acc: 87.97%, Test Loss: 0.0317, Test Acc: 88.39%\n",
      "Epoch 28: Train Loss: 0.0331, Train Acc: 87.96%, Test Loss: 0.0341, Test Acc: 88.12%\n",
      "Epoch 29: Train Loss: 0.0328, Train Acc: 87.99%, Test Loss: 0.0347, Test Acc: 88.08%\n",
      "Epoch 30: Train Loss: 0.0327, Train Acc: 88.04%, Test Loss: 0.0306, Test Acc: 88.73%\n",
      "Epoch 31: Train Loss: 0.0326, Train Acc: 88.05%, Test Loss: 0.0347, Test Acc: 88.02%\n",
      "Epoch 32: Train Loss: 0.0323, Train Acc: 88.10%, Test Loss: 0.0304, Test Acc: 88.61%\n",
      "Epoch 33: Train Loss: 0.0323, Train Acc: 88.14%, Test Loss: 0.0436, Test Acc: 87.02%\n",
      "Epoch 34: Train Loss: 0.0322, Train Acc: 88.17%, Test Loss: 0.0300, Test Acc: 88.58%\n",
      "Epoch 35: Train Loss: 0.0319, Train Acc: 88.19%, Test Loss: 0.0319, Test Acc: 88.52%\n",
      "Epoch 36: Train Loss: 0.0317, Train Acc: 88.27%, Test Loss: 0.0314, Test Acc: 88.51%\n",
      "Epoch 37: Train Loss: 0.0316, Train Acc: 88.27%, Test Loss: 0.0331, Test Acc: 88.29%\n",
      "Epoch 38: Train Loss: 0.0315, Train Acc: 88.29%, Test Loss: 0.0406, Test Acc: 86.96%\n",
      "Epoch 39: Train Loss: 0.0315, Train Acc: 88.37%, Test Loss: 0.0349, Test Acc: 88.18%\n",
      "Epoch 40: Train Loss: 0.0314, Train Acc: 88.39%, Test Loss: 0.0355, Test Acc: 88.10%\n",
      "Epoch 41: Train Loss: 0.0313, Train Acc: 88.35%, Test Loss: 0.0364, Test Acc: 88.25%\n",
      "Epoch 42: Train Loss: 0.0311, Train Acc: 88.43%, Test Loss: 0.0299, Test Acc: 88.61%\n",
      "Epoch 43: Train Loss: 0.0311, Train Acc: 88.44%, Test Loss: 0.0316, Test Acc: 88.86%\n",
      "Epoch 44: Train Loss: 0.0310, Train Acc: 88.43%, Test Loss: 0.0305, Test Acc: 88.58%\n",
      "Epoch 45: Train Loss: 0.0309, Train Acc: 88.42%, Test Loss: 0.0304, Test Acc: 88.60%\n",
      "Epoch 46: Train Loss: 0.0308, Train Acc: 88.49%, Test Loss: 0.0309, Test Acc: 88.64%\n",
      "Epoch 47: Train Loss: 0.0307, Train Acc: 88.51%, Test Loss: 0.0305, Test Acc: 88.81%\n",
      "Epoch 48: Train Loss: 0.0307, Train Acc: 88.40%, Test Loss: 0.0304, Test Acc: 88.63%\n",
      "Epoch 49: Train Loss: 0.0304, Train Acc: 88.46%, Test Loss: 0.0313, Test Acc: 88.59%\n",
      "Epoch 50: Train Loss: 0.0306, Train Acc: 88.48%, Test Loss: 0.0297, Test Acc: 89.01%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_sm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_sm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_sm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_sm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New try : DQN quadruple-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class FE_QM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_QM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc1_3 = nn.Linear(5, 16)  # Node difference (CPU, Memory)\n",
    "        # self.fc1_4 = nn.Linear(5, 16) # If the node can deploy the pod\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc2_3 = nn.Linear(16, 8)\n",
    "        # self.fc2_4 = nn.Linear(16, 8)\n",
    "\n",
    "        self.fc3 = nn.Linear(24, 16)\n",
    "        # self.fc3 = nn.Linear(32, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x3 = x1[:, ::2] - x1[:, 1::2] # Takes the difference of x1's odd and even columns\n",
    "\n",
    "        # # Duplicte x2 5 times horizontally\n",
    "        # x2_ = x2.repeat(1, 5).view(-1, 10)\n",
    "        # x4_ = (1-x1) - x2_ >= 0\n",
    "        # x4 = x4_[:, ::2] * x4_[:, 1::2]\n",
    "        # # Convert boolean to float\n",
    "        # x4 = x4.type(torch.FloatTensor)\n",
    "        # # print(f\"x4 : {x4}\")\n",
    "\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x3 = F.relu(self.fc1_3(x3))\n",
    "        # x4 = F.relu(self.fc1_4(x4))\n",
    "\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x3 = F.relu(self.fc2_3(x3))\n",
    "        # x4 = F.relu(self.fc2_4(x4))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        # x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_QM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_QM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_QM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=24, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_qm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0302, -0.0928, -0.0139, -0.0749,  0.1131, -0.0629]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_QM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_QM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_qm_model = DQN_QM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Predict 6 vectors (6 actions' scores)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_qm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0060, Train Acc: 68.59%, Test Loss: 0.0039, Test Acc: 72.20%\n",
      "Epoch 2: Train Loss: 0.0037, Train Acc: 74.28%, Test Loss: 0.0035, Test Acc: 75.66%\n",
      "Epoch 3: Train Loss: 0.0034, Train Acc: 75.43%, Test Loss: 0.0033, Test Acc: 76.20%\n",
      "Epoch 4: Train Loss: 0.0033, Train Acc: 76.11%, Test Loss: 0.0032, Test Acc: 77.03%\n",
      "Epoch 5: Train Loss: 0.0032, Train Acc: 76.82%, Test Loss: 0.0031, Test Acc: 77.52%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(dqn_qm_model, train_pr_dataloader, criterion, optimizer)\n\u001b[1;32m      5\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test(dqn_qm_model, test_pr_dataloader, criterion)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m: Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[56], line 5\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      3\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      4\u001b[0m correct \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfor\u001b[39;00m state, target \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      6\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m      7\u001b[0m     output \u001b[39m=\u001b[39m model(state)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[7], line 23\u001b[0m, in \u001b[0;36mPr_Dataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, idx):\n\u001b[0;32m---> 23\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput[idx], \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel[idx]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_qm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_qm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_qm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_qm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_qm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0032, -0.5091, -0.4996, -0.5009, -0.4956, -0.4971]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.4486, -0.9389, -0.8977, -0.8838, -0.9370, -0.0978]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3366, -0.8217, -0.8227, -0.0550, -0.8426, -0.8394]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
