{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stable_baselines3 as sb3\n",
    "from stable_baselines3.common.torch_layers import BaseFeaturesExtractor\n",
    "from stable_baselines3.common.logger import configure\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from kube_sim_gym.envs import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample1 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.0, 0.0]])\n",
    "sample2 = torch.tensor([[0.99, 0.90, 0.80, 0.80, 0.95, 0.95, 0.90, 0.85, 0.0, 0.0, 0.6, 0.7]])\n",
    "sample3 = torch.tensor([[0.99, 0.90, 0.40, 0.40, 0.15, 0.15, 0.90, 0.85, 0.8, 0.8, 0.6, 0.7]])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RL Training utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_rl_model(scenario_file, rl_model):\n",
    "\n",
    "    test_env1 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "    test_env2 = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file=scenario_file)\n",
    "\n",
    "    # RL Scheduler\n",
    "    rl_model.set_env(test_env1)\n",
    "\n",
    "    # Default Scheduler\n",
    "    from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "    default_scheduler = SimHrScheduler(test_env2, 'default.py')\n",
    "\n",
    "\n",
    "    # Test the model\n",
    "    obs1 = test_env1.reset()\n",
    "    obs2 = test_env2.reset()\n",
    "    done1 = False\n",
    "    done2 = False\n",
    "    step1 = 0\n",
    "    step2 = 0\n",
    "    acc_rew1 = 0\n",
    "    acc_rew2 = 0\n",
    "\n",
    "    print(f\"Testing with {scenario_file} (my model vs. default)\")\n",
    "    while not done1 or not done2:\n",
    "        if not done1:\n",
    "            action1, _ = rl_model.predict(obs1)\n",
    "            # action1 = rl_scheduler.decision(test_env1)\n",
    "            obs1, reward1, done1, _ = test_env1.step(action1)\n",
    "            step1 += 1\n",
    "            acc_rew1 += reward1\n",
    "        if not done2:\n",
    "            action2 = default_scheduler.decision(test_env2)\n",
    "            obs2, reward2, done2, _ = test_env2.step(action2)\n",
    "            step2 += 1\n",
    "            acc_rew2 += reward2\n",
    "\n",
    "    acc_rew1 = round(acc_rew1, 2)\n",
    "    acc_rew2 = round(acc_rew2, 2)\n",
    "\n",
    "    print(f\"Test result(reward): {acc_rew1} vs. {acc_rew2}\")\n",
    "    print(f\"Test result(step): {step1} vs. {step2}\")\n",
    "\n",
    "    return acc_rew1, acc_rew2, step1, step2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "from notebook.net_arch import *\n",
    "import glob\n",
    "\n",
    "def train_rl_model(json_tracker_fname):\n",
    "\n",
    "    date = datetime(1992, 7, 5, 8, 33)\n",
    "    date = date.strftime(\"%m%d%Y%H%M\")\n",
    "\n",
    "    log_name = json_tracker_fname.split('.')[0]\n",
    "    log_path = f'training/log/{log_name}'\n",
    "\n",
    "    if not os.path.exists(log_path):\n",
    "        os.makedirs(log_path)\n",
    "\n",
    "    logger = configure(log_path, ['stdout', 'csv', 'tensorboard'])\n",
    "    \n",
    "    # Load the json tracker\n",
    "    import json\n",
    "    with open(f'training/{json_tracker_fname}', 'r') as f:\n",
    "        json_tracker = json.load(f)\n",
    "\n",
    "    last_idx = json_tracker['last_idx']\n",
    "    learning_steps = json_tracker['learning_steps']\n",
    "    model_type = json_tracker['model_type']\n",
    "    reward_file = json_tracker['reward_file']\n",
    "    model_fname = json_tracker['model_fname']\n",
    "\n",
    "    # Environment\n",
    "    envs = []\n",
    "    for i in range(1, 50):\n",
    "        env = gym.make('SimKubeEnv-v0', reward_file=reward_file, scenario_file=f'trace2017_100_{i}.csv')\n",
    "        envs.append(env)\n",
    "\n",
    "    current_idx = last_idx + 1 # -1 as default\n",
    "\n",
    "    # Model type : DQN or PPO\n",
    "    if model_type == 'DQN':\n",
    "        model = sb3.DQN\n",
    "    elif model_type == 'PPO':\n",
    "        model = sb3.PPO\n",
    "    else:\n",
    "        print(f\"Unknown model type: {model_type}\")\n",
    "        return\n",
    "    \n",
    "    model_fpath = f'net_arch/{model_fname}.zip'\n",
    "\n",
    "    # Check if the model exists\n",
    "    # Load Model\n",
    "    if os.path.exists(model_fpath):\n",
    "        print(f\"Loading the model from {model_fname}\")\n",
    "        model = model.load(model_fpath)\n",
    "    else: # Error\n",
    "        print(f\"Model file does not exist: {model_fname}\")\n",
    "        return\n",
    "    \n",
    "    # If last_idx is not -1 and there's a model trained in training/model, then load the model\n",
    "    if last_idx != -1 and glob.glob(f'training/model/{model_fname}_*'):\n",
    "        # Load the model with the latest date\n",
    "        model_fpaths = glob.glob(f'training/model/{model_fname}_*')\n",
    "        model_fpaths.sort()\n",
    "        model_fpath = model_fpaths[-1]\n",
    "        print(f\"Loading the model from {model_fpath}\")\n",
    "    \n",
    "    # Save the model, append _{date} to the model name\n",
    "    trained_model_fname = f'{model_fname}_{date}'\n",
    "    trained_model_fpath = f'training/model/{trained_model_fname}'\n",
    "\n",
    "    # Set logger\n",
    "    model.set_logger(logger)\n",
    "\n",
    "    # Train the model\n",
    "    while current_idx < 20: # Target training steps (Can be changed!)\n",
    "        print(f\"Training with {current_idx}th trace\")\n",
    "\n",
    "        # Test the model first\n",
    "        a1, a2, a3, a4 = test_rl_model('scenario-5l-5m-1000p-10m_unbalanced.csv', model)\n",
    "        b1, b2, b3, b4 = test_rl_model('scenario-10l-3m-1000p-10m_unbalanced.csv', model)\n",
    "        c1, c2, c3, c4 = test_rl_model('scenario-3l-10m-1000p-10m_unbalanced.csv', model)\n",
    "\n",
    "        with open(f'training/log/{log_name}/test_result.txt', 'a') as f:\n",
    "            f.write(f\"{current_idx},{a1},{a2},{a3},{a4},{b1},{b2},{b3},{b4},{c1},{c2},{c3},{c4}\\n\")\n",
    "\n",
    "        env = envs[current_idx]\n",
    "        model.set_env(env)\n",
    "        model.learn(total_timesteps=learning_steps)\n",
    "\n",
    "        # Save the model\n",
    "        model.save(trained_model_fpath)\n",
    "\n",
    "        # Update the json tracker\n",
    "        json_tracker['last_idx'] = current_idx\n",
    "        with open(f'training/{json_tracker_fname}', 'w') as f:\n",
    "            json.dump(json_tracker, f)\n",
    "\n",
    "        current_idx += 1\n",
    "\n",
    "        clear_output()\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pr_Dataset(Dataset):\n",
    "    def __init__(self, csv_path, train=True):\n",
    "        self.data = pd.read_csv(csv_path)\n",
    "        # Drop the row which has 0 for the last -2, -3 columns\n",
    "        # self.data = self.data.drop(self.data[(self.data.iloc[:, -2] == 0) & (self.data.iloc[:, -3] == 0)].index)\n",
    "\n",
    "        if train:\n",
    "            self.data = self.data.sample(frac=0.8, random_state=42)\n",
    "        else:\n",
    "            self.data = self.data.drop(self.data.sample(frac=0.8, random_state=42).index)\n",
    "\n",
    "        self.data = self.transform(self.data)\n",
    "        self.input = self.data[:, :-6]\n",
    "        self.label = self.data[:, -6:]\n",
    "\n",
    "    def transform(self, data):\n",
    "        return torch.tensor(data.values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.input[idx], self.label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic.csv\")\n",
    "train_dynamic_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_dynamic_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_dynamic_dataloader = DataLoader(train_dynamic_dataset, batch_size=64, shuffle=False)\n",
    "test_dynamic_dataloader = DataLoader(test_dynamic_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.5600, 0.6700, 0.5200, 0.9400, 0.9100, 0.7400, 0.3200, 0.2900, 0.7300,\n",
      "         0.4100, 0.2800, 0.1300],\n",
      "        [0.5900, 0.7400, 0.2100, 0.2000, 0.7800, 0.8000, 0.1200, 0.9400, 0.0600,\n",
      "         0.7200, 0.3000, 0.2200],\n",
      "        [0.9700, 0.8100, 0.7500, 0.7100, 0.8400, 0.8000, 0.8100, 0.8800, 0.9500,\n",
      "         0.9800, 0.0300, 0.0800],\n",
      "        [0.1000, 0.6000, 0.1000, 0.9000, 0.5800, 0.1500, 0.8400, 0.8900, 0.1300,\n",
      "         0.9700, 0.1200, 0.2400],\n",
      "        [0.8700, 0.9500, 0.9100, 0.7700, 0.8500, 0.9300, 0.8300, 0.7200, 0.8600,\n",
      "         0.9400, 0.0900, 0.1200],\n",
      "        [0.9700, 0.8700, 0.7500, 0.7200, 0.8300, 0.8000, 0.8500, 0.8900, 0.9200,\n",
      "         0.8800, 0.2500, 0.2000],\n",
      "        [1.0000, 0.9300, 0.8600, 0.7200, 0.8000, 0.7100, 0.8900, 0.9900, 1.0000,\n",
      "         0.9900, 0.2900, 0.1600],\n",
      "        [0.9300, 0.2300, 0.7000, 0.8200, 0.4100, 0.2000, 0.0600, 0.7900, 0.5800,\n",
      "         0.3800, 0.1300, 0.1600],\n",
      "        [0.8900, 0.9000, 0.7300, 0.8200, 0.8700, 0.7900, 0.9600, 0.9600, 0.9600,\n",
      "         0.7300, 0.2000, 0.0200],\n",
      "        [0.8800, 0.9600, 0.9500, 0.8600, 0.7500, 0.9800, 0.8800, 0.9200, 0.9100,\n",
      "         0.8800, 0.2200, 0.0500],\n",
      "        [0.9700, 0.9800, 0.9700, 0.8800, 0.8800, 0.7800, 0.7600, 0.8900, 0.9100,\n",
      "         0.7000, 0.2100, 0.1800],\n",
      "        [0.5100, 0.5900, 0.0800, 0.7100, 0.7400, 0.2700, 1.0000, 0.2100, 0.7300,\n",
      "         0.2000, 0.2100, 0.2700],\n",
      "        [0.0900, 0.1400, 0.6400, 0.5400, 0.1800, 0.1100, 0.2100, 0.1000, 0.7900,\n",
      "         0.9200, 0.2500, 0.0700],\n",
      "        [0.3400, 0.4600, 0.5700, 0.0400, 0.6400, 0.2500, 0.3800, 0.7500, 0.0900,\n",
      "         0.9500, 0.1500, 0.1700],\n",
      "        [0.7200, 0.7300, 0.8600, 0.7500, 0.7200, 0.8800, 0.9600, 0.7500, 0.8700,\n",
      "         0.7200, 0.1400, 0.0200],\n",
      "        [0.8700, 0.6000, 0.6400, 0.3300, 0.3000, 0.7300, 0.3600, 0.4900, 0.5700,\n",
      "         0.2100, 0.2200, 0.2500],\n",
      "        [0.4900, 0.8700, 0.9100, 0.0500, 0.8500, 0.9000, 0.8400, 0.8800, 0.4900,\n",
      "         0.5000, 0.0100, 0.1200],\n",
      "        [0.7800, 0.8600, 0.6100, 0.1000, 0.5100, 0.2600, 0.5700, 0.9000, 0.8800,\n",
      "         0.7300, 0.2600, 0.2500],\n",
      "        [0.7700, 0.2100, 0.8000, 0.5700, 0.1600, 0.4200, 0.7700, 0.6200, 0.9400,\n",
      "         0.2900, 0.1200, 0.0200],\n",
      "        [0.5500, 0.7300, 0.8900, 0.5900, 0.8200, 0.0400, 0.1100, 0.4000, 0.6900,\n",
      "         0.1200, 0.2000, 0.0500],\n",
      "        [0.5000, 0.9600, 0.7800, 0.0500, 0.6200, 0.3900, 0.6300, 0.7100, 0.9000,\n",
      "         0.6400, 0.2800, 0.0500],\n",
      "        [0.5700, 0.1900, 0.5500, 0.3100, 0.4900, 0.0300, 0.9800, 0.2700, 0.1500,\n",
      "         0.3800, 0.1600, 0.1400],\n",
      "        [0.9700, 0.5500, 0.1200, 0.4200, 0.1100, 0.2900, 0.9400, 0.5500, 0.2800,\n",
      "         0.5100, 0.1500, 0.0800],\n",
      "        [0.6100, 0.1800, 0.9400, 0.5800, 0.2300, 0.4400, 0.1800, 0.9000, 0.8200,\n",
      "         0.6800, 0.0400, 0.2200],\n",
      "        [0.8200, 0.9800, 0.8400, 0.7600, 0.7900, 0.9800, 0.9500, 0.7200, 0.7200,\n",
      "         0.8000, 0.2200, 0.1800],\n",
      "        [0.9000, 0.9800, 0.4800, 0.6100, 0.5900, 0.5000, 0.6300, 0.8100, 0.0900,\n",
      "         0.7100, 0.2100, 0.2600],\n",
      "        [0.6000, 0.8700, 0.3100, 0.3700, 0.3000, 0.4000, 0.5700, 0.0600, 0.1900,\n",
      "         0.1600, 0.0600, 0.0500],\n",
      "        [0.8000, 0.7200, 0.8700, 0.8700, 0.8800, 0.7100, 0.9200, 0.8100, 0.7400,\n",
      "         0.9600, 0.2300, 0.2500],\n",
      "        [0.8000, 0.7800, 0.9000, 0.9300, 0.8100, 0.9100, 0.7900, 0.9400, 0.7300,\n",
      "         0.8800, 0.0600, 0.0600],\n",
      "        [0.9000, 0.7200, 0.8400, 0.9000, 0.7600, 0.9000, 0.7100, 0.8600, 0.9200,\n",
      "         0.7700, 0.1000, 0.1000],\n",
      "        [0.7900, 0.8800, 0.7800, 0.9200, 0.7900, 0.7800, 0.7100, 0.7700, 0.7400,\n",
      "         0.8200, 0.1200, 0.2800],\n",
      "        [0.8200, 0.7600, 0.5000, 0.8900, 0.4300, 0.3900, 0.8400, 0.2700, 0.1400,\n",
      "         0.1200, 0.2000, 0.1500],\n",
      "        [0.2200, 0.3000, 0.2700, 0.9300, 0.1100, 0.8900, 0.3000, 0.2900, 0.0800,\n",
      "         0.8700, 0.1900, 0.0800],\n",
      "        [0.6100, 0.7500, 0.6300, 0.2900, 0.6100, 0.4200, 0.3900, 0.6400, 0.4100,\n",
      "         0.6800, 0.0700, 0.0200],\n",
      "        [0.7400, 0.3600, 0.4200, 0.3400, 0.0800, 0.8900, 0.6800, 0.7400, 0.9700,\n",
      "         0.0800, 0.1900, 0.1300],\n",
      "        [0.9200, 0.7100, 0.6500, 0.4000, 0.9700, 0.8700, 0.2900, 0.1000, 0.5100,\n",
      "         0.1400, 0.1300, 0.2300],\n",
      "        [0.9600, 0.7700, 0.8300, 0.9600, 0.7800, 0.7200, 0.9500, 0.8500, 0.9900,\n",
      "         0.7100, 0.1800, 0.2500],\n",
      "        [0.7300, 0.8900, 0.3400, 0.3500, 0.3200, 0.8100, 0.8700, 0.2500, 0.3100,\n",
      "         0.0700, 0.0500, 0.2900],\n",
      "        [0.2800, 0.0700, 0.3600, 0.3500, 0.6500, 0.0900, 0.1700, 0.6900, 0.2900,\n",
      "         0.8800, 0.0400, 0.0800],\n",
      "        [0.0700, 0.9200, 0.9400, 0.9700, 0.5300, 0.3400, 0.7600, 0.7200, 0.7600,\n",
      "         0.9000, 0.2700, 0.1400],\n",
      "        [0.4600, 0.3800, 0.4000, 0.2800, 0.2300, 0.5800, 0.8700, 0.8700, 0.9700,\n",
      "         0.0200, 0.2500, 0.0100],\n",
      "        [0.2000, 0.0900, 0.9000, 0.8800, 0.8900, 0.4800, 0.3600, 0.7400, 0.5500,\n",
      "         0.3400, 0.2500, 0.2100],\n",
      "        [0.1300, 0.0200, 0.2600, 0.4700, 0.5700, 0.0600, 0.3800, 0.5400, 0.8200,\n",
      "         0.8900, 0.2300, 0.1600],\n",
      "        [0.1700, 0.9600, 0.1900, 0.3000, 0.1500, 0.1500, 0.3800, 0.0300, 0.0100,\n",
      "         0.8400, 0.1200, 0.1900],\n",
      "        [1.0000, 0.7400, 0.9400, 0.9000, 0.9500, 0.8900, 0.8200, 0.7100, 0.8700,\n",
      "         0.9800, 0.0500, 0.2700],\n",
      "        [0.8000, 0.7500, 0.8200, 0.7000, 0.8900, 0.9300, 0.7300, 0.9300, 0.9500,\n",
      "         0.7100, 0.0900, 0.1700],\n",
      "        [0.1300, 0.2500, 0.2000, 0.6500, 0.6000, 0.0600, 0.1500, 0.8600, 0.7000,\n",
      "         0.8600, 0.1800, 0.1400],\n",
      "        [0.7200, 0.6100, 0.3300, 0.5100, 0.1200, 0.3000, 0.9900, 0.3300, 0.2300,\n",
      "         0.9500, 0.1900, 0.2700],\n",
      "        [0.0700, 0.1500, 0.0600, 0.1300, 0.5700, 0.1800, 0.8700, 0.1800, 0.2900,\n",
      "         0.4500, 0.1300, 0.2000],\n",
      "        [0.0300, 0.2200, 0.8400, 0.9800, 0.1300, 0.6300, 0.4800, 0.0300, 0.2500,\n",
      "         0.9200, 0.1300, 0.1800],\n",
      "        [0.9200, 0.7600, 0.9600, 0.9700, 0.7300, 0.9200, 0.9200, 0.7900, 0.9800,\n",
      "         0.7700, 0.2700, 0.1200],\n",
      "        [0.9900, 0.9000, 0.8100, 0.9100, 0.9200, 0.8200, 0.8300, 0.8100, 0.8200,\n",
      "         0.7100, 0.1900, 0.1000],\n",
      "        [0.9700, 0.1300, 0.1600, 0.4300, 0.4100, 0.7500, 0.2200, 0.3000, 0.2100,\n",
      "         0.8700, 0.0600, 0.1900],\n",
      "        [0.1900, 0.4000, 0.3100, 0.2800, 0.8700, 0.1700, 0.3500, 0.8900, 0.8000,\n",
      "         0.4200, 0.1800, 0.1800],\n",
      "        [0.1100, 0.0900, 0.1800, 0.9800, 0.4500, 0.0200, 0.4500, 0.6900, 0.2100,\n",
      "         0.8800, 0.0700, 0.1000],\n",
      "        [0.9700, 0.5100, 0.5300, 0.0100, 0.8200, 0.8100, 0.7400, 0.6300, 0.7700,\n",
      "         0.1900, 0.1600, 0.0400],\n",
      "        [0.6000, 0.7300, 0.7200, 0.1900, 0.4200, 0.0400, 0.3100, 0.6800, 0.7200,\n",
      "         0.0200, 0.1000, 0.0500],\n",
      "        [0.1700, 0.0400, 0.4200, 0.8600, 0.8300, 0.0200, 0.2100, 0.7700, 0.2000,\n",
      "         0.9600, 0.2900, 0.0600],\n",
      "        [0.8200, 0.7500, 0.8200, 0.7600, 0.9100, 0.7900, 0.9900, 0.7400, 0.8000,\n",
      "         0.9400, 0.0800, 0.0100],\n",
      "        [0.7100, 0.9800, 0.9600, 0.7600, 0.8200, 0.7700, 0.8800, 0.9400, 0.7600,\n",
      "         0.7100, 0.0900, 0.1000],\n",
      "        [0.4100, 0.6800, 0.7400, 0.0900, 0.9400, 0.8400, 0.9600, 0.6600, 0.0700,\n",
      "         0.1700, 0.0100, 0.1900],\n",
      "        [0.9500, 0.5000, 0.4700, 0.2200, 0.9200, 0.7100, 0.4400, 0.6900, 0.4800,\n",
      "         0.2000, 0.1000, 0.0300],\n",
      "        [0.8600, 0.9700, 0.7600, 0.7200, 0.7900, 0.8600, 0.9300, 0.7400, 0.9300,\n",
      "         0.7200, 0.0300, 0.1200],\n",
      "        [0.8000, 0.9700, 0.6100, 0.0800, 0.4400, 0.3800, 0.7700, 0.0900, 0.6200,\n",
      "         0.7200, 0.0200, 0.1000]])\n",
      "labels: tensor([[-0.5000, -0.2256, -1.0000, -1.0000,  0.5000, -1.0000],\n",
      "        [-0.5000, -0.4407,  0.5000, -1.0000, -1.0000, -0.1098],\n",
      "        [-0.5000, -0.0942,  0.5000, -0.0738, -0.1551, -1.0000],\n",
      "        [-0.5000, -0.2796, -1.0000,  0.5000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000, -0.1443, -1.0000,  0.5000, -1.0000],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000, -0.3264,  0.5000, -0.1395, -0.1137],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000,  0.1317,  1.0000, -1.0000,  1.0000],\n",
      "        [-0.5000,  0.5000, -0.3972, -0.1098, -0.1257, -1.0000],\n",
      "        [-0.5000,  0.5000,  1.0000, -0.0360, -0.1038, -1.0000],\n",
      "        [-0.5000,  0.5000, -0.1374,  0.1026, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000, -0.0156, -0.1047,  0.5000,  1.0000],\n",
      "        [-0.5000, -0.0792,  1.0000, -1.0000, -0.0924,  0.5000],\n",
      "        [-0.5000, -1.0000, -0.0300,  0.5000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.2769, -0.3120,  0.5000, -0.3081, -1.0000],\n",
      "        [-0.5000, -0.2538, -1.0000, -1.0000,  0.5000, -0.4233],\n",
      "        [-0.5000, -1.0000, -1.0000,  0.5000,  0.0543, -1.0000],\n",
      "        [-0.5000, -0.0741, -0.1419,  0.0888, -1.0000,  0.5000],\n",
      "        [-0.5000, -1.0000, -0.0759,  0.5000, -1.0000, -0.1560],\n",
      "        [-0.5000,  1.0000,  0.0858,  0.5000, -1.0000, -0.0240],\n",
      "        [-0.5000, -1.0000, -1.0000, -1.0000, -1.0000,  0.5000],\n",
      "        [-0.5000, -1.0000, -0.1305,  0.5000, -1.0000, -0.0660],\n",
      "        [-0.5000, -0.1533, -0.0390, -0.0399, -0.0732,  0.5000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000, -0.1551, -0.0972, -0.1005, -0.0240],\n",
      "        [-0.5000, -0.0192,  0.5000,  0.0579,  0.5000, -1.0000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000, -1.0000, -0.2391, -1.0000,  0.5000],\n",
      "        [-0.5000,  1.0000, -1.0000,  1.0000,  0.5000,  1.0000],\n",
      "        [-0.5000,  0.0372,  0.0240,  0.5000,  1.0000,  1.0000],\n",
      "        [-0.5000, -0.1218,  0.5000, -1.0000, -0.1269, -1.0000],\n",
      "        [-0.5000, -1.0000, -0.2619, -1.0000,  0.5000, -0.0951],\n",
      "        [-0.5000, -1.0000, -1.0000,  0.5000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000,  0.2784,  1.0000],\n",
      "        [-0.5000,  1.0000,  0.5000,  0.0345, -0.0327, -0.0780],\n",
      "        [-0.5000, -1.0000, -1.0000,  0.5000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.0348,  0.5000,  1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000, -1.0000, -1.0000, -0.3483, -0.3303],\n",
      "        [-0.5000,  0.5000, -0.1380, -0.2658, -0.2274, -1.0000],\n",
      "        [-0.5000, -1.0000, -0.0708,  0.5000,  0.0690, -1.0000],\n",
      "        [-0.5000, -1.0000, -1.0000, -1.0000,  0.5000, -1.0000],\n",
      "        [-0.5000,  0.5000,  1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000,  0.5000, -0.1377, -0.1953, -0.1644, -0.3924],\n",
      "        [-0.5000, -0.3252, -0.2286,  0.5000, -1.0000, -1.0000],\n",
      "        [-0.5000, -0.0231,  0.5000, -0.0933, -0.1617, -0.3063],\n",
      "        [-0.5000,  0.5000, -1.0000, -0.1488, -0.0300, -1.0000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.5000, -1.0000, -1.0000, -1.0000, -1.0000, -1.0000],\n",
      "        [-0.5000, -1.0000, -0.0480, -0.2007,  0.5000, -1.0000],\n",
      "        [-0.5000, -0.0210,  0.5000, -1.0000, -1.0000, -0.2445],\n",
      "        [-0.5000,  0.5000, -1.0000, -0.0759, -0.2139, -0.1743],\n",
      "        [-0.5000, -1.0000,  0.5000, -0.0630,  0.5000, -0.1977],\n",
      "        [-0.5000, -0.0723, -0.1206,  0.5000,  0.0441, -0.1038],\n",
      "        [-0.5000,  0.5000,  0.0351, -1.0000,  0.1821, -1.0000],\n",
      "        [-0.5000,  0.0060, -0.0060, -0.0591, -1.0000,  0.0837],\n",
      "        [-0.5000, -1.0000, -1.0000, -0.0741, -1.0000,  0.5000],\n",
      "        [-0.5000, -0.1998,  1.0000, -1.0000,  0.0132,  0.5000],\n",
      "        [-0.5000, -1.0000,  0.5000, -1.0000,  0.0534, -0.0120],\n",
      "        [-0.5000, -1.0000,  0.5000, -0.1515,  0.0060,  0.0162],\n",
      "        [-0.5000, -1.0000,  1.0000,  0.5000,  1.0000, -0.1461]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dynamic_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic2.csv\")\n",
    "train_dynamic2_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_dynamic2_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_dynamic2_dataloader = DataLoader(train_dynamic2_dataset, batch_size=64, shuffle=False)\n",
    "test_dynamic2_dataloader = DataLoader(test_dynamic2_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.9800, 0.8100, 0.7200, 0.7700, 0.9000, 0.7900, 0.9100, 0.8700, 0.9100,\n",
      "         0.9100, 0.0700, 0.0100],\n",
      "        [0.6600, 0.0400, 0.4300, 0.9800, 0.7100, 0.3500, 0.0800, 0.1700, 0.8300,\n",
      "         0.1400, 0.1100, 0.1700],\n",
      "        [0.7500, 0.9400, 0.9200, 0.8700, 0.9100, 0.8800, 0.9600, 0.8100, 0.9700,\n",
      "         0.9100, 0.1900, 0.3000],\n",
      "        [0.8800, 0.4500, 0.5500, 0.2100, 0.2000, 0.0700, 0.8300, 0.3400, 0.3900,\n",
      "         0.4700, 0.0200, 0.0200],\n",
      "        [0.9200, 0.8200, 0.4700, 0.9400, 0.1300, 0.7900, 0.5300, 0.8500, 0.4400,\n",
      "         0.5500, 0.1700, 0.1900],\n",
      "        [0.4300, 0.1600, 0.2300, 0.8100, 0.4400, 0.2800, 0.4100, 0.7900, 0.7600,\n",
      "         0.0800, 0.2100, 0.1800],\n",
      "        [0.7300, 0.9600, 0.7900, 0.7600, 0.7800, 0.7600, 0.9500, 0.9400, 0.8500,\n",
      "         0.8800, 0.0400, 0.2100],\n",
      "        [0.5500, 0.4700, 1.0000, 0.8600, 0.1000, 0.1400, 0.1900, 0.5500, 0.7500,\n",
      "         0.7200, 0.1800, 0.0200],\n",
      "        [0.8900, 0.3600, 0.6900, 0.3900, 0.1700, 0.5000, 0.7300, 0.8800, 0.7600,\n",
      "         0.5700, 0.1600, 0.1000],\n",
      "        [0.1400, 0.7400, 0.2700, 0.9400, 0.6100, 0.2000, 0.1200, 0.9100, 0.6800,\n",
      "         0.8400, 0.0500, 0.1300],\n",
      "        [0.3200, 0.2000, 0.7300, 0.5500, 0.3000, 0.0200, 0.2900, 0.6100, 0.8000,\n",
      "         0.5800, 0.2300, 0.2900],\n",
      "        [0.3300, 0.4300, 0.6000, 0.4900, 0.9000, 0.1900, 0.3800, 0.2100, 0.9200,\n",
      "         0.4300, 0.2400, 0.2500],\n",
      "        [0.2300, 0.6800, 0.0100, 0.3900, 0.9700, 0.3800, 0.6800, 0.4500, 0.9600,\n",
      "         0.5500, 0.0900, 0.1900],\n",
      "        [0.2100, 0.9800, 0.8800, 0.7700, 0.6100, 0.6300, 0.4900, 0.7000, 0.5700,\n",
      "         0.8900, 0.0800, 0.2000],\n",
      "        [0.8200, 0.5900, 0.8900, 0.6600, 0.0900, 0.0500, 0.2700, 0.2700, 0.7300,\n",
      "         0.8000, 0.1400, 0.2700],\n",
      "        [0.8700, 0.9300, 0.9600, 0.7300, 0.7100, 0.7500, 0.7100, 1.0000, 0.9400,\n",
      "         0.7800, 0.2300, 0.0800],\n",
      "        [0.9300, 0.2400, 0.5000, 0.2600, 0.5500, 0.5100, 0.9400, 0.1000, 0.7200,\n",
      "         0.0300, 0.0400, 0.2200],\n",
      "        [0.2600, 0.8900, 0.2300, 0.0900, 0.6500, 0.2800, 0.5100, 0.6400, 0.2700,\n",
      "         0.7400, 0.0300, 0.0400],\n",
      "        [0.4100, 0.1800, 0.0400, 0.9600, 0.6800, 0.3200, 0.4300, 0.0300, 0.8900,\n",
      "         0.7100, 0.1100, 0.1600],\n",
      "        [0.8500, 0.3900, 0.6500, 0.9100, 0.8100, 0.2300, 0.4800, 0.4100, 0.9200,\n",
      "         1.0000, 0.1400, 0.0500],\n",
      "        [0.7600, 0.9300, 0.9500, 0.8800, 0.8500, 0.9100, 0.9000, 0.8600, 0.7100,\n",
      "         0.7600, 0.2300, 0.2900],\n",
      "        [0.5500, 0.6300, 0.2900, 0.5200, 0.0700, 0.6000, 0.3100, 0.2900, 0.4100,\n",
      "         0.0100, 0.0400, 0.0600],\n",
      "        [0.4300, 0.2700, 0.6400, 0.8800, 0.7600, 0.8500, 0.8100, 0.2400, 0.0300,\n",
      "         0.2200, 0.0900, 0.2000],\n",
      "        [0.5900, 0.6200, 0.9200, 0.4600, 0.8300, 0.5500, 0.9800, 0.8300, 0.9100,\n",
      "         0.2100, 0.1500, 0.0500],\n",
      "        [0.9500, 0.8500, 0.9400, 0.9400, 0.9700, 0.9200, 0.8500, 0.8300, 0.8000,\n",
      "         0.7400, 0.0600, 0.0300],\n",
      "        [0.6800, 0.8900, 0.2700, 0.8600, 0.6300, 0.5600, 0.8000, 0.6100, 0.9900,\n",
      "         0.5100, 0.1200, 0.2000],\n",
      "        [0.8900, 0.9400, 0.6900, 1.0000, 0.3600, 0.0500, 0.8400, 0.3500, 0.3200,\n",
      "         0.4900, 0.0900, 0.2200],\n",
      "        [0.8400, 0.8200, 0.2500, 0.4400, 0.9300, 0.7600, 0.7100, 0.5600, 0.8800,\n",
      "         0.6100, 0.2000, 0.0300],\n",
      "        [0.9300, 0.7600, 0.8000, 0.9600, 0.7800, 0.8700, 0.7400, 0.8200, 0.9300,\n",
      "         0.8300, 0.2900, 0.0500],\n",
      "        [0.7600, 0.9400, 0.9700, 0.7100, 0.9300, 0.7900, 0.7500, 0.9800, 0.7200,\n",
      "         0.8700, 0.0700, 0.2700],\n",
      "        [0.7200, 0.8200, 0.3400, 0.8100, 0.6300, 0.9500, 0.3000, 0.3200, 0.3200,\n",
      "         0.5800, 0.2500, 0.2800],\n",
      "        [0.6100, 0.0300, 0.7600, 0.2900, 0.9500, 0.9800, 0.2900, 0.7600, 0.2900,\n",
      "         0.7000, 0.2900, 0.2100],\n",
      "        [0.8100, 1.0000, 0.7300, 0.9800, 0.7600, 0.9000, 0.7700, 0.9300, 0.9100,\n",
      "         0.8800, 0.0400, 0.0700],\n",
      "        [0.6700, 0.8500, 0.8000, 0.7300, 0.5100, 0.2100, 0.5800, 0.0500, 0.8300,\n",
      "         0.9200, 0.1500, 0.0700],\n",
      "        [0.8700, 0.8600, 0.7100, 0.7600, 0.8300, 0.9200, 0.9900, 0.8400, 0.8100,\n",
      "         0.8400, 0.2300, 0.2800],\n",
      "        [0.8200, 0.8800, 0.7400, 0.9600, 0.7900, 0.8200, 0.9400, 0.9400, 0.9000,\n",
      "         0.7000, 0.1100, 0.1000],\n",
      "        [0.7900, 0.2500, 0.2000, 0.8100, 0.6500, 0.9100, 0.9300, 0.4600, 0.9300,\n",
      "         0.6700, 0.2700, 0.1100],\n",
      "        [0.8100, 0.4600, 0.4900, 0.5000, 0.6000, 0.4200, 0.9300, 0.2300, 0.0400,\n",
      "         0.6000, 0.1400, 0.2800],\n",
      "        [0.8900, 0.7700, 0.7900, 0.7200, 0.9300, 0.9800, 0.7400, 0.8600, 0.9500,\n",
      "         0.7700, 0.2200, 0.1700],\n",
      "        [0.6900, 0.5700, 0.9100, 0.0100, 0.2800, 0.8900, 0.3200, 0.2100, 0.1500,\n",
      "         0.2000, 0.0400, 0.1500],\n",
      "        [0.2500, 0.1700, 0.7100, 0.1100, 0.9000, 0.9500, 0.2000, 0.5700, 0.6400,\n",
      "         0.0100, 0.2600, 0.0200],\n",
      "        [0.5800, 0.1200, 0.4100, 0.1800, 0.8000, 0.4400, 0.1700, 0.8300, 0.6100,\n",
      "         0.1500, 0.2600, 0.0600],\n",
      "        [0.7500, 0.7300, 0.9300, 0.9700, 0.7000, 0.9300, 0.9200, 0.8800, 0.9600,\n",
      "         0.8300, 0.1600, 0.1000],\n",
      "        [0.9200, 0.3000, 0.6700, 0.6200, 0.2000, 0.3700, 0.3900, 0.2000, 0.7000,\n",
      "         0.1100, 0.2500, 0.2600],\n",
      "        [0.0100, 0.7200, 0.6200, 0.3300, 0.4600, 0.7900, 0.6000, 0.1100, 0.8100,\n",
      "         0.3600, 0.0700, 0.1600],\n",
      "        [0.9800, 0.0100, 0.0400, 0.7500, 0.0900, 0.5500, 0.3500, 0.5800, 0.4700,\n",
      "         0.4300, 0.0500, 0.2500],\n",
      "        [0.9100, 0.9500, 0.8700, 0.8000, 0.8100, 0.9000, 0.8800, 0.9100, 0.7400,\n",
      "         0.7300, 0.0400, 0.0400],\n",
      "        [0.4800, 0.1600, 0.7900, 0.9100, 0.4200, 0.5800, 0.0300, 0.6900, 0.3300,\n",
      "         0.6800, 0.0600, 0.0600],\n",
      "        [0.2700, 0.1600, 0.3800, 0.4400, 0.2500, 0.7600, 0.6900, 0.7200, 0.6100,\n",
      "         0.2200, 0.1800, 0.1600],\n",
      "        [0.1900, 0.8500, 0.9500, 0.2700, 0.2000, 0.8700, 0.7300, 0.3300, 0.4600,\n",
      "         0.3300, 0.1400, 0.1700],\n",
      "        [0.1000, 0.4000, 0.6900, 0.5600, 0.3700, 0.1800, 0.7400, 0.0800, 0.8700,\n",
      "         0.7000, 0.0500, 0.0200],\n",
      "        [0.6500, 0.9300, 0.8300, 0.6900, 0.7500, 0.6500, 0.5300, 0.8800, 0.9700,\n",
      "         0.4400, 0.0200, 0.0900],\n",
      "        [0.8700, 0.3300, 0.3300, 0.8400, 0.8700, 0.6200, 0.3400, 0.5900, 0.6000,\n",
      "         0.1100, 0.0800, 0.1900],\n",
      "        [0.7400, 0.7200, 0.7400, 0.9900, 0.8200, 0.7100, 0.9200, 0.8800, 0.7700,\n",
      "         0.8000, 0.1400, 0.2700],\n",
      "        [0.9100, 0.7700, 0.8900, 0.8500, 0.7600, 0.9300, 0.7700, 0.9100, 0.9700,\n",
      "         0.7300, 0.2300, 0.0400],\n",
      "        [0.9700, 0.8200, 0.9900, 0.7400, 0.2900, 0.5500, 0.0600, 0.6700, 0.5900,\n",
      "         0.9400, 0.2600, 0.1600],\n",
      "        [0.3200, 0.3700, 0.7800, 0.5000, 0.7800, 0.4600, 0.6100, 0.7500, 0.3900,\n",
      "         0.6700, 0.0600, 0.2200],\n",
      "        [0.2000, 0.5400, 0.4400, 0.7500, 0.1900, 0.7200, 0.9900, 0.7200, 0.3700,\n",
      "         0.3200, 0.0500, 0.0300],\n",
      "        [0.5600, 0.2800, 0.3700, 0.8100, 0.1400, 0.6800, 0.6300, 0.2700, 0.4200,\n",
      "         0.2200, 0.2400, 0.0100],\n",
      "        [0.7600, 0.2700, 0.1000, 0.6600, 0.0700, 0.5500, 1.0000, 0.8900, 0.9700,\n",
      "         0.8000, 0.0200, 0.0300],\n",
      "        [0.2700, 0.9500, 0.3700, 0.6200, 0.4600, 0.4500, 0.5800, 0.2400, 0.1200,\n",
      "         0.7700, 0.3000, 0.2200],\n",
      "        [0.4000, 0.4400, 0.2400, 0.3500, 0.5000, 0.1300, 0.8700, 0.8300, 0.1400,\n",
      "         0.9700, 0.1700, 0.0000],\n",
      "        [0.2300, 0.4200, 0.0300, 0.2100, 0.5000, 0.6600, 0.3800, 0.9400, 0.8100,\n",
      "         0.5100, 0.1000, 0.1600],\n",
      "        [0.6400, 0.8900, 0.9100, 0.1900, 0.5800, 0.2300, 0.9800, 0.2900, 0.7800,\n",
      "         0.9000, 0.0600, 0.0700]])\n",
      "labels: tensor([[-0.0300, -0.5300,  0.0000, -0.0500, -0.0600, -0.0700],\n",
      "        [-0.0600, -0.0100, -0.5600, -0.0500,  0.0000, -0.0300],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0200, -0.0200, -0.0200,  0.0000, -0.0200, -0.0300],\n",
      "        [-0.0900, -0.5900, -0.5900, -0.0400, -0.5900,  0.0000],\n",
      "        [-0.0100,  0.0200,  0.0200,  0.0000, -0.0300, -0.0300],\n",
      "        [-0.0100, -0.5100,  0.0000,  0.0000, -0.5100, -0.5100],\n",
      "        [-0.0400, -0.0600, -0.5400,  0.0000,  0.0400, -0.0900],\n",
      "        [-0.1000, -0.6000, -0.0700,  0.0000, -0.1100, -0.1000],\n",
      "        [-0.0800, -0.0700, -0.5800,  0.0000, -0.5800, -0.1000],\n",
      "        [-0.1000,  0.0000, -0.1600,  0.0600, -0.1100, -0.6000],\n",
      "        [-0.1000, -0.0700, -0.1300, -0.6000,  0.0000, -0.6000],\n",
      "        [-0.0500, -0.0900,  0.0000, -0.5500, -0.0100, -0.5500],\n",
      "        [-0.0400, -0.5400, -0.0200,  0.0000, -0.0100, -0.5400],\n",
      "        [-0.1400, -0.1600, -0.6400,  0.0000, -0.0900, -0.6400],\n",
      "        [-0.0500, -0.5500, -0.5500,  0.0000, -0.5500, -0.5500],\n",
      "        [ 0.0700,  0.1000,  0.1200,  0.0000,  0.1400,  0.1700],\n",
      "        [-0.0200, -0.0200,  0.0000, -0.0200, -0.0200, -0.0100],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.0300,  0.0200, -0.0800],\n",
      "        [-0.0500, -0.0600, -0.0100, -0.0600,  0.0000, -0.5500],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0100, -0.0400, -0.0200,  0.0000,  0.0000,  0.0200],\n",
      "        [-0.0700,  0.0000, -0.5700, -0.5700, -0.0100,  0.0000],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.0700, -0.5600, -0.5600],\n",
      "        [-0.0400, -0.5400, -0.0500, -0.5400, -0.0200,  0.0000],\n",
      "        [-0.0500, -0.5500, -0.5500,  0.0000, -0.0100, -0.5500],\n",
      "        [-0.1400, -0.6400, -0.6400,  0.0000, -0.0800, -0.1100],\n",
      "        [-0.1400, -0.6400,  0.0000, -0.6400, -0.1500, -0.6400],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.1400, -0.6400, -0.6400, -0.6400,  0.0000, -0.0800],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.5600,  0.0000,  0.0100],\n",
      "        [ 0.0000, -0.5000, -0.5000,  0.0100,  0.0000,  0.0000],\n",
      "        [-0.0400, -0.0500, -0.0900,  0.0000, -0.0100, -0.0800],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0200, -0.0300, -0.5200,  0.0000, -0.5200, -0.5200],\n",
      "        [-0.1400, -0.6400,  0.0000, -0.6400, -0.6400, -0.6400],\n",
      "        [ 0.0500,  0.0500,  0.0000,  0.0700, -0.4500,  0.0200],\n",
      "        [ 0.0000, -0.5000, -0.5000, -0.5000, -0.5000, -0.5000],\n",
      "        [-0.0100, -0.0100,  0.0400, -0.5100,  0.0400,  0.0000],\n",
      "        [-0.0300,  0.0000, -0.0900, -0.5300,  0.1000, -0.0800],\n",
      "        [ 0.0000, -0.0400,  0.0000, -0.5000,  0.1200, -0.0500],\n",
      "        [-0.0600,  0.0000, -0.5600, -0.5600, -0.5600, -0.5600],\n",
      "        [-0.0500, -0.5500, -0.1400,  0.0000,  0.0200, -0.0100],\n",
      "        [-0.0300, -0.0300,  0.0000, -0.0700,  0.0300, -0.0100],\n",
      "        [ 0.0100, -0.4900, -0.0600, -0.0200, -0.0400,  0.0000],\n",
      "        [-0.0300, -0.0400, -0.0300, -0.0300, -0.0400,  0.0000],\n",
      "        [-0.0300,  0.0000, -0.0400, -0.0200,  0.0000, -0.0100],\n",
      "        [-0.0900,  0.0000, -0.0600, -0.0700, -0.1500, -0.0700],\n",
      "        [-0.0600, -0.5600, -0.5600, -0.5600, -0.0200,  0.0000],\n",
      "        [-0.0100,  0.0300, -0.0200,  0.0000, -0.0200, -0.0300],\n",
      "        [-0.0300, -0.5300, -0.0100,  0.0000, -0.0300,  0.0100],\n",
      "        [ 0.0000,  0.0400, -0.5000,  0.0000,  0.0000,  0.1000],\n",
      "        [-0.0100,  0.0000, -0.5100,  0.0300, -0.5100, -0.5100],\n",
      "        [-0.0400, -0.5400, -0.5400,  0.0100,  0.0000, -0.5400],\n",
      "        [-0.1200, -0.6200, -0.6200,  0.0000,  0.0100, -0.6200],\n",
      "        [-0.0300,  0.0000,  0.0000,  0.0100, -0.1200, -0.0800],\n",
      "        [-0.0200,  0.0000, -0.0200, -0.0100, -0.5200,  0.0000],\n",
      "        [ 0.0500, -0.0400,  0.0900,  0.1700, -0.0500,  0.0000],\n",
      "        [-0.0100,  0.0000, -0.0100,  0.0000, -0.5100, -0.0200],\n",
      "        [ 0.0100, -0.4900,  0.0300,  0.0000,  0.0100,  0.1100],\n",
      "        [-0.0300, -0.0500,  0.0000, -0.0700, -0.5300,  0.0500],\n",
      "        [-0.0600, -0.0400,  0.0000, -0.0900, -0.5600, -0.0700],\n",
      "        [-0.0400, -0.0400, -0.0200,  0.0000, -0.5400, -0.0500]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dynamic2_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_default.csv\")\n",
    "train_default_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_default_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_default_dataloader = DataLoader(train_default_dataset, batch_size=64, shuffle=False)\n",
    "test_default_dataloader = DataLoader(test_default_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.8800, 0.8200, 0.4300, 0.1300, 0.7900, 0.1500, 0.4600, 0.7800, 0.8000,\n",
      "         0.4100, 0.2700, 0.0900],\n",
      "        [0.4500, 0.9100, 0.7200, 0.0900, 0.9700, 0.0700, 0.0300, 0.1500, 1.0000,\n",
      "         0.4300, 0.1400, 0.1300],\n",
      "        [0.9700, 0.9000, 0.8800, 0.9100, 0.9500, 0.9900, 0.9300, 0.7800, 0.8800,\n",
      "         0.9200, 0.1600, 0.1000],\n",
      "        [0.9200, 0.8700, 0.8100, 0.9100, 0.8500, 0.7700, 0.7200, 0.8900, 0.9600,\n",
      "         0.7800, 0.2400, 0.2800],\n",
      "        [0.9800, 0.7600, 0.8900, 0.7200, 0.9200, 0.7900, 0.9200, 0.8500, 0.8500,\n",
      "         0.8200, 0.2700, 0.1200],\n",
      "        [0.7000, 0.6200, 0.2500, 0.6000, 0.2400, 0.6400, 0.6900, 0.2700, 0.6400,\n",
      "         0.4700, 0.0400, 0.0200],\n",
      "        [0.7700, 0.9900, 0.2700, 0.4100, 0.5900, 0.0400, 0.5100, 0.3700, 0.4200,\n",
      "         0.3700, 0.2200, 0.2400],\n",
      "        [0.9600, 0.4700, 0.2600, 0.9000, 0.4800, 0.2400, 0.1700, 0.5400, 0.6200,\n",
      "         0.8000, 0.2800, 0.1000],\n",
      "        [0.6000, 0.3800, 0.4900, 0.6700, 0.6400, 0.7400, 0.1500, 0.2500, 0.1700,\n",
      "         0.4100, 0.2200, 0.0700],\n",
      "        [0.5900, 0.9900, 0.5900, 0.1400, 0.3700, 0.6600, 0.3300, 0.8700, 0.5900,\n",
      "         0.3100, 0.1800, 0.2700],\n",
      "        [0.9000, 0.7200, 0.5400, 0.4500, 0.2600, 0.0300, 0.4100, 0.8800, 0.8100,\n",
      "         0.2700, 0.0800, 0.0900],\n",
      "        [0.4600, 0.7000, 0.5100, 0.7000, 0.4100, 0.5400, 0.3700, 0.4600, 0.6700,\n",
      "         0.0600, 0.1100, 0.2700],\n",
      "        [0.8600, 0.8500, 0.0900, 0.9500, 0.2500, 0.1300, 0.5800, 0.4300, 0.8600,\n",
      "         0.5300, 0.0500, 0.0700],\n",
      "        [0.9100, 0.6600, 0.3300, 0.5600, 0.7500, 0.9100, 0.8900, 0.4400, 0.1600,\n",
      "         0.0300, 0.0800, 0.1600],\n",
      "        [0.6900, 0.2800, 0.6700, 0.8200, 0.4600, 0.7600, 0.8100, 0.6400, 0.2000,\n",
      "         0.7000, 0.1300, 0.0800],\n",
      "        [0.0900, 0.3700, 0.3100, 0.7800, 0.6700, 0.8600, 0.3700, 0.3800, 0.1100,\n",
      "         0.9000, 0.1700, 0.2800],\n",
      "        [0.3100, 0.7200, 0.9800, 0.2200, 0.4700, 0.8100, 0.6400, 0.0900, 0.4100,\n",
      "         0.5100, 0.0700, 0.1500],\n",
      "        [0.8300, 0.8400, 0.8200, 0.9600, 0.8500, 0.9200, 0.7400, 0.7000, 0.7900,\n",
      "         0.8600, 0.1900, 0.1000],\n",
      "        [1.0000, 0.3800, 0.3300, 0.6500, 0.1300, 0.3100, 0.9000, 0.4500, 0.5400,\n",
      "         0.8400, 0.2200, 0.2800],\n",
      "        [0.3100, 0.6600, 0.0200, 0.3300, 0.3700, 0.6900, 0.6300, 0.4000, 0.6700,\n",
      "         0.4400, 0.0200, 0.1200],\n",
      "        [0.7500, 0.7000, 0.7700, 0.9000, 0.8200, 0.7200, 0.8600, 0.7800, 0.9600,\n",
      "         0.7600, 0.0700, 0.2200],\n",
      "        [0.9800, 0.7400, 0.9200, 0.9200, 0.8600, 0.8000, 0.9000, 0.7400, 0.7700,\n",
      "         0.7100, 0.3000, 0.2900],\n",
      "        [0.8700, 0.7600, 0.9600, 0.8700, 0.8700, 0.7400, 0.9200, 0.9000, 0.7600,\n",
      "         0.9400, 0.2100, 0.1800],\n",
      "        [0.8400, 0.8300, 0.9400, 0.8100, 0.9400, 0.7100, 0.8300, 0.8500, 0.7600,\n",
      "         0.8500, 0.1400, 0.0600],\n",
      "        [0.5200, 0.8600, 0.7100, 0.8300, 0.7500, 0.4700, 0.5800, 0.4000, 0.7500,\n",
      "         0.7200, 0.0800, 0.1300],\n",
      "        [0.8800, 0.9000, 0.7500, 0.8400, 0.7700, 0.7100, 1.0000, 0.7300, 0.7200,\n",
      "         0.7000, 0.1800, 0.2500],\n",
      "        [0.3800, 0.9900, 0.7100, 0.1300, 0.9900, 0.9400, 0.7200, 0.7900, 0.5700,\n",
      "         0.0200, 0.1000, 0.2300],\n",
      "        [0.8800, 0.0800, 0.6700, 0.1900, 0.2700, 0.5100, 0.4200, 0.7000, 0.2800,\n",
      "         0.3000, 0.2700, 0.0900],\n",
      "        [0.0600, 0.8200, 0.8900, 0.4700, 0.5300, 0.5300, 0.8300, 0.5000, 0.8900,\n",
      "         0.8800, 0.0400, 0.2100],\n",
      "        [0.0900, 0.0300, 0.3100, 0.6600, 0.3800, 0.6600, 0.5000, 0.8200, 0.7300,\n",
      "         0.3300, 0.1100, 0.2900],\n",
      "        [0.2600, 0.4900, 0.9100, 0.0400, 0.3600, 0.4800, 0.6400, 0.9500, 0.8600,\n",
      "         0.5600, 0.1400, 0.1100],\n",
      "        [0.7900, 0.0600, 0.4900, 0.0700, 0.3200, 0.8000, 0.9600, 0.0700, 0.3100,\n",
      "         0.9100, 0.0500, 0.0000],\n",
      "        [0.9400, 0.9500, 0.8200, 0.7700, 0.8500, 0.8800, 0.7000, 0.8600, 0.8700,\n",
      "         0.9800, 0.2000, 0.2100],\n",
      "        [0.8100, 0.7400, 0.8100, 0.9200, 0.7500, 0.8400, 0.7300, 0.9700, 0.8400,\n",
      "         0.9300, 0.2200, 0.1900],\n",
      "        [0.0100, 0.6400, 0.3700, 0.3500, 0.2100, 0.0100, 0.2100, 0.9200, 0.3600,\n",
      "         0.3500, 0.1000, 0.2400],\n",
      "        [0.8200, 0.7000, 0.7300, 0.8300, 0.7900, 0.8800, 0.7100, 0.9000, 0.9500,\n",
      "         0.8300, 0.1700, 0.1200],\n",
      "        [0.8800, 0.8600, 0.7300, 0.8100, 0.8100, 0.9800, 0.7600, 0.8200, 0.7400,\n",
      "         0.7300, 0.0700, 0.1400],\n",
      "        [0.9800, 0.9300, 0.9400, 0.8000, 0.7700, 0.8900, 0.8400, 0.7200, 0.9600,\n",
      "         0.9900, 0.2200, 0.1800],\n",
      "        [0.7200, 0.9300, 0.9100, 0.9000, 0.9100, 0.9600, 0.8300, 0.9200, 0.8900,\n",
      "         0.8100, 0.2800, 0.1600],\n",
      "        [0.6200, 0.1600, 0.6400, 0.2600, 0.0800, 0.1100, 0.3400, 0.5800, 0.5600,\n",
      "         0.2400, 0.1500, 0.0200],\n",
      "        [0.7700, 0.4700, 0.2800, 0.0300, 0.9800, 0.8700, 0.2000, 0.6200, 0.7100,\n",
      "         0.6300, 0.0000, 0.1300],\n",
      "        [0.9400, 0.1300, 0.9900, 0.0600, 0.2000, 0.2400, 0.3800, 0.4600, 0.9300,\n",
      "         0.8300, 0.1400, 0.1100],\n",
      "        [0.3700, 0.1300, 0.3400, 0.6500, 0.6000, 0.6700, 0.4800, 0.7200, 0.4100,\n",
      "         0.9700, 0.0000, 0.0600],\n",
      "        [0.8900, 0.7100, 0.9700, 0.8800, 0.9000, 0.9300, 0.8000, 0.7600, 0.7300,\n",
      "         0.7700, 0.0000, 0.2300],\n",
      "        [0.7400, 0.8100, 0.7200, 0.4500, 0.5100, 0.9800, 0.8900, 0.0700, 0.3900,\n",
      "         0.2900, 0.2900, 0.0600],\n",
      "        [0.8600, 0.9400, 0.8500, 0.8100, 0.7500, 0.7500, 0.9000, 0.7500, 0.9400,\n",
      "         0.8200, 0.1900, 0.1000],\n",
      "        [0.8500, 0.9100, 0.9500, 0.6000, 0.7700, 0.5700, 0.6400, 0.8200, 0.0600,\n",
      "         0.3200, 0.2500, 0.1500],\n",
      "        [0.0300, 0.7000, 0.4900, 0.8000, 0.2000, 0.6800, 0.2500, 0.2000, 0.3200,\n",
      "         0.4000, 0.0100, 0.1500],\n",
      "        [0.7000, 0.7500, 0.7500, 0.7400, 0.7200, 0.8800, 0.7600, 0.8300, 0.9100,\n",
      "         0.9700, 0.1900, 0.1700],\n",
      "        [0.9000, 0.8300, 0.8200, 0.8600, 0.7900, 0.8100, 0.7000, 0.7800, 0.9600,\n",
      "         0.7200, 0.1300, 0.2600],\n",
      "        [0.9200, 0.6800, 0.2800, 0.5000, 0.9800, 0.1200, 0.1900, 0.6600, 0.5000,\n",
      "         0.4900, 0.2600, 0.2600],\n",
      "        [0.2000, 0.9300, 0.5000, 0.2400, 1.0000, 0.6100, 0.1300, 0.7900, 0.6200,\n",
      "         0.5700, 0.1200, 0.1900],\n",
      "        [0.9700, 0.8900, 0.7700, 0.7500, 0.8900, 0.8700, 0.9100, 0.8900, 0.9400,\n",
      "         0.9000, 0.0500, 0.0700],\n",
      "        [0.3200, 0.9200, 0.8100, 0.3700, 0.2400, 0.6300, 0.1100, 0.5000, 0.9600,\n",
      "         0.2400, 0.2900, 0.1600],\n",
      "        [0.4600, 0.9900, 0.7100, 0.5500, 0.0900, 0.9300, 0.6400, 0.4500, 0.9300,\n",
      "         0.8300, 0.0200, 0.2700],\n",
      "        [0.5800, 0.0500, 0.6100, 0.1500, 0.5000, 0.4500, 0.7400, 0.8800, 0.5600,\n",
      "         0.7800, 0.2500, 0.2000],\n",
      "        [0.5400, 0.9500, 0.5800, 0.3300, 0.4500, 0.6600, 0.2500, 0.9700, 0.8200,\n",
      "         0.0300, 0.2100, 0.0600],\n",
      "        [0.5700, 0.7800, 0.6600, 0.6900, 0.8000, 0.4200, 0.4000, 0.7000, 0.4800,\n",
      "         0.5000, 0.2000, 0.2900],\n",
      "        [0.6000, 0.0300, 0.9200, 0.1200, 0.2600, 0.9600, 0.7800, 0.7400, 0.4200,\n",
      "         0.1500, 0.1200, 0.0800],\n",
      "        [0.8800, 0.7300, 0.7000, 0.3800, 0.2200, 0.6200, 0.0100, 0.6800, 0.6600,\n",
      "         0.5000, 0.1700, 0.3000],\n",
      "        [0.8600, 0.7200, 0.1300, 0.7500, 0.2900, 0.0400, 0.7600, 0.5800, 0.3100,\n",
      "         0.1800, 0.1900, 0.0400],\n",
      "        [0.8600, 0.9200, 0.9800, 0.8000, 0.9500, 0.7300, 0.8100, 0.8900, 0.8600,\n",
      "         0.9800, 0.1100, 0.1000],\n",
      "        [1.0000, 0.1600, 0.0400, 0.6800, 0.5300, 0.8400, 0.1900, 0.1600, 0.3300,\n",
      "         0.4800, 0.2300, 0.0600],\n",
      "        [0.3700, 0.9000, 0.4300, 0.3600, 0.9900, 0.5800, 0.7200, 0.4600, 0.2100,\n",
      "         0.0800, 0.1300, 0.2900]])\n",
      "labels: tensor([[0.3950, 0.1500, 0.5400, 0.5300, 0.2000, 0.3950],\n",
      "        [0.2850, 0.3200, 0.4600, 0.4800, 0.7750, 0.2850],\n",
      "        [0.1000, 0.0650, 0.1050, 0.0300, 0.1450, 0.1000],\n",
      "        [0.1300, 0.1050, 0.1400, 0.1900, 0.1950, 0.1300],\n",
      "        [0.1650, 0.1300, 0.1950, 0.1450, 0.1150, 0.1650],\n",
      "        [0.4450, 0.3100, 0.5450, 0.5300, 0.4900, 0.4150],\n",
      "        [0.6050, 0.1200, 0.4300, 0.4550, 0.3300, 0.3750],\n",
      "        [0.2900, 0.2850, 0.2300, 0.4500, 0.4550, 0.1000],\n",
      "        [0.7100, 0.3650, 0.2750, 0.1650, 0.6550, 0.5650],\n",
      "        [0.5500, 0.2100, 0.4100, 0.2600, 0.4000, 0.3250],\n",
      "        [0.4600, 0.1050, 0.4200, 0.7700, 0.2700, 0.3750],\n",
      "        [0.6350, 0.2300, 0.2050, 0.3350, 0.3950, 0.4450],\n",
      "        [0.3050, 0.0850, 0.4800, 0.7500, 0.4350, 0.2450],\n",
      "        [0.9050, 0.0950, 0.4350, 0.1700, 0.2150, 0.7850],\n",
      "        [0.5500, 0.4100, 0.1500, 0.2850, 0.1700, 0.4450],\n",
      "        [0.4950, 0.5450, 0.4550, 0.2350, 0.4000, 0.4950],\n",
      "        [0.5400, 0.3750, 0.4000, 0.2500, 0.5250, 0.4300],\n",
      "        [0.1750, 0.1650, 0.1100, 0.1150, 0.1350, 0.0300],\n",
      "        [0.3100, 0.3100, 0.2600, 0.5300, 0.3250, 0.3100],\n",
      "        [0.4450, 0.4450, 0.7550, 0.4000, 0.4150, 0.3750],\n",
      "        [0.1400, 0.1300, 0.1650, 0.0850, 0.0350, 0.1400],\n",
      "        [0.2600, 0.1400, 0.0800, 0.1700, 0.1800, 0.2600],\n",
      "        [0.1500, 0.1850, 0.0850, 0.1950, 0.0900, 0.1500],\n",
      "        [0.1950, 0.0650, 0.1250, 0.1750, 0.0600, 0.0950],\n",
      "        [0.2650, 0.2050, 0.1250, 0.2850, 0.4050, 0.1600],\n",
      "        [0.2900, 0.1100, 0.2050, 0.0450, 0.1350, 0.0750],\n",
      "        [0.7050, 0.3150, 0.4150, 0.0350, 0.2450, 0.5400],\n",
      "        [0.7100, 0.5200, 0.3900, 0.4300, 0.2600, 0.5300],\n",
      "        [0.1150, 0.5600, 0.1950, 0.3450, 0.2100, 0.1150],\n",
      "        [0.4700, 0.7400, 0.3150, 0.2800, 0.3400, 0.2700],\n",
      "        [0.2900, 0.5000, 0.5250, 0.4550, 0.2050, 0.1650],\n",
      "        [0.3900, 0.5500, 0.6950, 0.4150, 0.4850, 0.3650],\n",
      "        [0.0750, 0.0550, 0.2050, 0.1350, 0.2200, 0.0750],\n",
      "        [0.1150, 0.2250, 0.1350, 0.2050, 0.1500, 0.1150],\n",
      "        [0.6450, 0.5050, 0.4700, 0.7200, 0.4350, 0.4750],\n",
      "        [0.1100, 0.0950, 0.0750, 0.0200, 0.1950, 0.1100],\n",
      "        [0.2650, 0.0250, 0.1250, 0.1050, 0.1050, 0.1600],\n",
      "        [0.0250, 0.0450, 0.1300, 0.1700, 0.2200, 0.0250],\n",
      "        [0.1500, 0.1750, 0.0950, 0.0650, 0.1250, 0.1500],\n",
      "        [0.6000, 0.5250, 0.4650, 0.8200, 0.4550, 0.5150],\n",
      "        [0.3300, 0.3150, 0.7800, 0.0100, 0.5250, 0.2650],\n",
      "        [0.1200, 0.4650, 0.4750, 0.6550, 0.4550, 0.1200],\n",
      "        [0.3100, 0.7200, 0.4750, 0.3350, 0.3700, 0.3100],\n",
      "        [0.2500, 0.0850, 0.0750, 0.0850, 0.1050, 0.1350],\n",
      "        [0.6600, 0.2250, 0.4150, 0.2550, 0.5200, 0.4850],\n",
      "        [0.1200, 0.1000, 0.1700, 0.1050, 0.1750, 0.1200],\n",
      "        [0.8100, 0.1200, 0.2250, 0.3300, 0.0700, 0.6100],\n",
      "        [0.6400, 0.5550, 0.2750, 0.4800, 0.6950, 0.5600],\n",
      "        [0.0600, 0.0950, 0.0750, 0.2000, 0.0250, 0.0600],\n",
      "        [0.1600, 0.1350, 0.1600, 0.2000, 0.2600, 0.1600],\n",
      "        [0.5050, 0.2000, 0.3500, 0.4500, 0.3150, 0.2450],\n",
      "        [0.4050, 0.4350, 0.4750, 0.1950, 0.3850, 0.2500],\n",
      "        [0.0800, 0.0700, 0.1800, 0.0600, 0.0400, 0.0200],\n",
      "        [0.4000, 0.3800, 0.4100, 0.3400, 0.4700, 0.4000],\n",
      "        [0.1200, 0.2750, 0.2250, 0.4900, 0.3100, 0.1200],\n",
      "        [0.3300, 0.4600, 0.3950, 0.3000, 0.1900, 0.1050],\n",
      "        [0.5750, 0.2550, 0.4100, 0.3100, 0.3900, 0.5750],\n",
      "        [0.5100, 0.3250, 0.0800, 0.1450, 0.2050, 0.2650],\n",
      "        [0.7150, 0.5850, 0.4800, 0.3900, 0.1400, 0.6150],\n",
      "        [0.4200, 0.1950, 0.2250, 0.3450, 0.4200, 0.1850],\n",
      "        [0.7550, 0.2100, 0.4450, 0.7200, 0.2150, 0.6400],\n",
      "        [0.0800, 0.1100, 0.1100, 0.1600, 0.0450, 0.0800],\n",
      "        [0.5950, 0.4200, 0.4950, 0.1700, 0.6800, 0.4500],\n",
      "        [0.8550, 0.3650, 0.3950, 0.2150, 0.2000, 0.6450]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_default_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_default2.csv\")\n",
    "train_default2_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_default2_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_default2_dataloader = DataLoader(train_default2_dataset, batch_size=64, shuffle=False)\n",
    "test_default2_dataloader = DataLoader(test_default2_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.1400, 0.9600, 0.2000, 0.3200, 0.9600, 0.1300, 0.4600, 0.4600, 0.4400,\n",
      "         0.7200, 0.1900, 0.2600],\n",
      "        [0.8300, 0.7600, 0.7400, 0.9400, 0.8200, 0.8600, 0.9400, 0.7700, 0.7500,\n",
      "         0.8100, 0.2800, 0.1400],\n",
      "        [0.8100, 0.9400, 0.9000, 0.8400, 0.8600, 0.9800, 0.8100, 0.9900, 0.7500,\n",
      "         0.7800, 0.0600, 0.1500],\n",
      "        [0.7100, 0.7100, 0.8600, 0.9100, 0.7800, 0.7600, 0.8500, 0.8200, 0.9700,\n",
      "         0.9000, 0.2000, 0.1100],\n",
      "        [0.9800, 0.8600, 0.7700, 0.8900, 0.7500, 0.9800, 0.7000, 0.8100, 0.7900,\n",
      "         0.8200, 0.1500, 0.1900],\n",
      "        [0.8300, 0.3800, 0.1300, 0.2200, 0.9200, 0.8100, 0.7800, 0.4700, 0.8700,\n",
      "         0.5300, 0.0400, 0.2100],\n",
      "        [0.4600, 0.2400, 0.9800, 0.6300, 0.4600, 0.3300, 0.8300, 0.1100, 0.4600,\n",
      "         0.6700, 0.2800, 0.0200],\n",
      "        [0.8000, 0.4000, 0.1800, 0.9200, 0.9000, 0.0500, 0.8400, 0.8400, 0.3400,\n",
      "         0.3400, 0.1600, 0.2400],\n",
      "        [0.9700, 0.9200, 0.8000, 0.8300, 0.9100, 0.9600, 0.9100, 0.9400, 0.9200,\n",
      "         0.7400, 0.1000, 0.2400],\n",
      "        [0.7700, 0.1100, 0.2400, 0.2800, 0.7900, 0.2800, 0.9800, 0.4000, 0.3800,\n",
      "         0.5300, 0.2700, 0.2900],\n",
      "        [0.9600, 0.9800, 0.9200, 0.8600, 0.8400, 0.9900, 0.8600, 0.7200, 0.7400,\n",
      "         0.8100, 0.1200, 0.1700],\n",
      "        [0.7900, 0.4000, 0.7800, 0.5400, 0.6300, 0.0300, 0.9300, 0.5000, 0.1800,\n",
      "         0.4000, 0.0800, 0.1700],\n",
      "        [0.8400, 0.7100, 0.7200, 0.7700, 0.8200, 0.8100, 0.8200, 0.9300, 0.8200,\n",
      "         0.7100, 0.0500, 0.2400],\n",
      "        [0.7000, 0.7100, 0.8700, 0.7400, 0.9100, 0.8300, 0.8400, 0.9700, 0.9500,\n",
      "         0.8000, 0.2800, 0.1400],\n",
      "        [0.6700, 0.4000, 0.0900, 0.3700, 0.0500, 0.4100, 0.2000, 0.8400, 0.0500,\n",
      "         0.9200, 0.1000, 0.0700],\n",
      "        [0.4700, 0.3100, 0.4000, 0.3600, 0.0500, 0.5600, 0.7000, 0.2700, 0.6400,\n",
      "         0.6400, 0.1000, 0.1800],\n",
      "        [0.8100, 0.8200, 0.9000, 0.7500, 0.8000, 0.9800, 0.9500, 0.8600, 0.9800,\n",
      "         0.9300, 0.1500, 0.2300],\n",
      "        [0.9300, 0.3800, 0.4100, 0.6200, 0.5100, 0.5300, 0.3900, 0.8100, 0.5100,\n",
      "         0.1200, 0.0700, 0.2700],\n",
      "        [0.9700, 0.4900, 0.9300, 0.1900, 0.1500, 0.6200, 0.1300, 0.3800, 0.7800,\n",
      "         0.4700, 0.2300, 0.0500],\n",
      "        [0.9700, 0.7300, 0.7800, 0.8900, 0.8400, 0.7100, 0.9800, 0.7800, 0.9500,\n",
      "         0.8600, 0.2300, 0.0700],\n",
      "        [0.1300, 0.2200, 0.5800, 0.6000, 0.1900, 0.2400, 0.6700, 0.2200, 0.9800,\n",
      "         0.9400, 0.1200, 0.0400],\n",
      "        [0.7000, 0.9600, 0.8100, 0.8300, 0.7900, 0.8700, 0.8500, 0.7100, 0.9900,\n",
      "         0.8700, 0.1500, 0.0000],\n",
      "        [0.8300, 0.9500, 0.9400, 0.9300, 0.8600, 0.8600, 0.8900, 0.9700, 0.9500,\n",
      "         0.7200, 0.0700, 0.2800],\n",
      "        [0.6000, 0.4900, 0.0800, 0.8900, 0.0200, 0.5400, 0.6700, 0.4500, 0.4100,\n",
      "         0.5600, 0.1000, 0.0200],\n",
      "        [0.7400, 0.7100, 0.7100, 0.8100, 0.9300, 0.7400, 0.7700, 0.7300, 0.7400,\n",
      "         0.7600, 0.0100, 0.1600],\n",
      "        [0.4800, 0.9500, 0.5900, 0.9400, 0.5500, 0.5500, 0.1400, 0.1700, 0.1300,\n",
      "         0.5000, 0.2300, 0.2200],\n",
      "        [0.8600, 0.1300, 0.4100, 0.4000, 0.7000, 0.8000, 0.8400, 0.2100, 0.1200,\n",
      "         0.2300, 0.0100, 0.1800],\n",
      "        [0.9400, 0.9000, 0.7500, 0.7600, 0.8100, 0.9500, 0.9400, 0.7300, 0.8900,\n",
      "         0.8700, 0.0600, 0.1900],\n",
      "        [0.7400, 0.4300, 0.9200, 0.3400, 0.8800, 0.3800, 0.9600, 0.0300, 0.9100,\n",
      "         0.6200, 0.0100, 0.0000],\n",
      "        [0.7200, 0.2500, 0.0400, 0.8300, 0.1200, 0.2200, 0.3600, 0.6000, 0.7600,\n",
      "         0.1900, 0.0000, 0.1300],\n",
      "        [0.8800, 0.7500, 0.7800, 0.8100, 0.9400, 0.9900, 0.9800, 0.7200, 0.9200,\n",
      "         0.9800, 0.1700, 0.2200],\n",
      "        [0.6600, 0.5700, 0.2000, 0.8300, 0.3000, 0.6100, 0.8400, 0.7100, 0.0400,\n",
      "         0.8500, 0.2500, 0.1700],\n",
      "        [0.7500, 0.2500, 0.3200, 0.8900, 0.5600, 0.7200, 0.6700, 0.7900, 1.0000,\n",
      "         0.7100, 0.3000, 0.0500],\n",
      "        [0.5700, 0.2700, 0.9500, 0.8200, 0.0800, 0.9400, 0.2000, 0.7600, 0.0900,\n",
      "         0.2800, 0.1500, 0.0600],\n",
      "        [0.5000, 0.2000, 0.1300, 0.5200, 0.4400, 0.9400, 0.9700, 0.5600, 0.5400,\n",
      "         0.8100, 0.2600, 0.2200],\n",
      "        [0.9600, 0.9400, 0.9800, 0.7100, 0.7700, 0.9300, 0.8700, 0.7100, 0.9600,\n",
      "         0.8900, 0.1100, 0.2300],\n",
      "        [0.2100, 0.7400, 0.6200, 0.2600, 0.7800, 0.7600, 0.4300, 0.8500, 0.3600,\n",
      "         0.7200, 0.0100, 0.1300],\n",
      "        [0.6300, 0.7600, 0.5700, 0.1800, 0.2700, 0.2000, 0.3100, 0.6300, 1.0000,\n",
      "         0.5000, 0.1300, 0.1500],\n",
      "        [0.1500, 0.9400, 0.4100, 0.8100, 0.1200, 0.8400, 0.8900, 0.8800, 0.6900,\n",
      "         0.2100, 0.1800, 0.0300],\n",
      "        [0.1800, 0.8200, 0.7500, 0.4300, 0.1000, 0.8900, 0.6600, 0.2200, 0.4400,\n",
      "         0.7400, 0.1100, 0.1300],\n",
      "        [0.1700, 0.3400, 0.4100, 0.8300, 0.1100, 0.0700, 0.7800, 0.4000, 0.9200,\n",
      "         0.6900, 0.2900, 0.0700],\n",
      "        [0.8100, 0.9200, 0.1200, 0.8600, 0.3300, 0.5400, 0.8700, 0.7900, 0.8100,\n",
      "         0.9600, 0.1800, 0.1500],\n",
      "        [0.8100, 0.8300, 0.9700, 0.9400, 0.9300, 0.7100, 0.8800, 0.7300, 0.9000,\n",
      "         0.8900, 0.0600, 0.0800],\n",
      "        [0.8000, 0.7200, 0.9300, 0.9500, 0.7300, 0.9000, 0.7200, 0.8300, 0.7200,\n",
      "         0.8000, 0.0500, 0.2800],\n",
      "        [0.9600, 0.2500, 0.4800, 0.5400, 0.9500, 0.9200, 0.3700, 0.8200, 0.8700,\n",
      "         1.0000, 0.1800, 0.1600],\n",
      "        [0.8100, 0.7400, 0.9100, 0.8400, 0.7000, 0.9100, 0.8000, 0.9400, 0.7300,\n",
      "         0.8100, 0.0300, 0.1000],\n",
      "        [0.9800, 0.9000, 0.9700, 0.7700, 0.8100, 0.8600, 0.9500, 0.9000, 0.7700,\n",
      "         0.9400, 0.2400, 0.2800],\n",
      "        [0.5200, 0.7600, 0.3900, 0.6400, 0.8400, 0.3500, 0.2200, 0.9900, 0.7200,\n",
      "         0.0900, 0.1700, 0.1200],\n",
      "        [0.9800, 0.2500, 0.8400, 1.0000, 0.4400, 0.8500, 0.4900, 0.3200, 0.9900,\n",
      "         0.9400, 0.0500, 0.0500],\n",
      "        [0.8800, 0.0700, 0.5300, 0.0900, 0.9400, 0.9500, 0.8300, 0.1100, 0.9600,\n",
      "         0.9500, 0.3000, 0.0600],\n",
      "        [0.9900, 0.7700, 0.9200, 0.7400, 0.9500, 0.9100, 0.8300, 0.7200, 0.9100,\n",
      "         0.8900, 0.0200, 0.1400],\n",
      "        [0.6900, 0.0600, 0.5600, 0.8100, 0.4800, 0.1200, 0.4400, 0.7000, 0.6800,\n",
      "         0.8800, 0.0500, 0.2400],\n",
      "        [0.6500, 0.3400, 0.3500, 0.6900, 0.9200, 0.9300, 0.4500, 0.7400, 0.8700,\n",
      "         0.1700, 0.2900, 0.0100],\n",
      "        [0.9800, 0.7400, 0.8400, 0.7300, 0.9700, 0.9100, 0.7300, 0.7500, 0.7900,\n",
      "         0.9100, 0.1600, 0.0800],\n",
      "        [0.9500, 0.7900, 0.9400, 0.7700, 0.9500, 0.9800, 0.8000, 0.7100, 0.9900,\n",
      "         0.9600, 0.2500, 0.1900],\n",
      "        [0.8500, 0.8400, 0.9800, 0.3000, 0.7700, 0.8600, 0.3000, 0.9300, 0.5800,\n",
      "         0.4800, 0.2200, 0.0400],\n",
      "        [0.8500, 0.7200, 0.9600, 0.3600, 0.0200, 0.1200, 0.1400, 0.8300, 0.9100,\n",
      "         0.0700, 0.2600, 0.2700],\n",
      "        [0.4500, 0.8100, 0.8100, 0.2100, 0.1100, 0.9700, 0.2500, 0.1400, 0.5600,\n",
      "         0.2100, 0.0300, 0.1300],\n",
      "        [0.9600, 0.3500, 0.5700, 0.4300, 0.7300, 0.3800, 0.3700, 0.7600, 0.1800,\n",
      "         1.0000, 0.1600, 0.1200],\n",
      "        [0.8400, 0.7100, 0.9700, 0.9300, 0.8000, 0.8000, 0.7600, 0.8700, 0.7500,\n",
      "         0.9700, 0.1100, 0.1200],\n",
      "        [0.4700, 0.2100, 0.6100, 0.4400, 0.1000, 0.3900, 0.3200, 0.8600, 0.7200,\n",
      "         0.5700, 0.0100, 0.2300],\n",
      "        [0.0800, 0.7400, 0.5300, 0.9600, 0.3200, 0.2300, 0.9600, 0.3300, 0.3100,\n",
      "         0.2000, 0.0300, 0.0600],\n",
      "        [0.5700, 0.5300, 0.0200, 0.5400, 0.7500, 0.6200, 0.0800, 0.0400, 0.1500,\n",
      "         0.9700, 0.1300, 0.1900],\n",
      "        [0.4100, 0.3900, 0.4500, 0.6300, 0.6700, 0.8100, 0.5100, 0.5100, 0.6800,\n",
      "         0.5500, 0.2300, 0.2800]])\n",
      "labels: tensor([[-1.1300, -1.3700, -0.9600, -1.3700, -1.1300, -1.4300],\n",
      "        [-0.9100, -0.9300, -1.0400, -0.9600, -1.0400, -0.9100],\n",
      "        [-0.8700, -1.0300, -1.1100, -1.0700, -1.0800, -0.9900],\n",
      "        [-1.0900, -0.9900, -1.0300, -1.0900, -0.9800, -1.0900],\n",
      "        [-0.9300, -1.1000, -1.0100, -1.1000, -1.1100, -0.9300],\n",
      "        [-1.2200, -1.2300, -0.7500, -1.2600, -1.1800, -1.2800],\n",
      "        [-0.9900, -1.0400, -1.2900, -1.0400, -1.1400, -1.0500],\n",
      "        [-0.7800, -1.4100, -1.3600, -1.3400, -1.2800, -0.9900],\n",
      "        [-1.0200, -1.0600, -0.9300, -1.0600, -1.0300, -1.0200],\n",
      "        [-0.8500, -1.0800, -0.8400, -1.1100, -1.2900, -1.1700],\n",
      "        [-0.9500, -1.1100, -1.0500, -1.1300, -1.0900, -1.0900],\n",
      "        [-0.7200, -1.2100, -1.2100, -0.9900, -1.2600, -0.8700],\n",
      "        [-0.9000, -1.0500, -0.8600, -0.9100, -1.0300, -1.0500],\n",
      "        [-1.0800, -1.0700, -0.9900, -1.0400, -1.0900, -1.0800],\n",
      "        [-1.2700, -1.1200, -0.7700, -0.8000, -1.2500, -1.3300],\n",
      "        [-0.9100, -0.8300, -0.8100, -1.0100, -1.0800, -1.1400],\n",
      "        [-1.0900, -0.9300, -1.0100, -1.0900, -1.0500, -1.0900],\n",
      "        [-0.8200, -1.3200, -1.2200, -1.1300, -1.1100, -0.8300],\n",
      "        [-1.1800, -1.3700, -1.3300, -1.0400, -0.8000, -1.1800],\n",
      "        [-1.0500, -1.0800, -1.0000, -0.9400, -1.0900, -1.0500],\n",
      "        [-1.4100, -0.6700, -1.1300, -0.7100, -1.2100, -1.4100],\n",
      "        [-1.1100, -1.0600, -1.1000, -1.0600, -1.1500, -1.1100],\n",
      "        [-1.0600, -1.0500, -1.0400, -0.9600, -1.0700, -1.0600],\n",
      "        [-0.8700, -1.0200, -1.2100, -0.8500, -1.1000, -0.8800],\n",
      "        [-0.8500, -0.9600, -1.0900, -1.0500, -0.9900, -1.0400],\n",
      "        [-0.8500, -1.3100, -1.2900, -1.1800, -0.6700, -1.0500],\n",
      "        [-0.6100, -1.2300, -0.9600, -1.4000, -1.2100, -0.7700],\n",
      "        [-1.0000, -1.0500, -1.0500, -1.0600, -1.1100, -1.0000],\n",
      "        [-1.1200, -0.9500, -1.1500, -1.1100, -1.1800, -1.1300],\n",
      "        [-1.1600, -1.1000, -1.3800, -0.7400, -1.1300, -1.1400],\n",
      "        [-1.1100, -1.0100, -0.9500, -1.1200, -1.1100, -1.1100],\n",
      "        [-1.1600, -1.2700, -1.3200, -1.1000, -1.1500, -1.1600],\n",
      "        [-1.3200, -1.0600, -1.2100, -1.1800, -1.3100, -1.3200],\n",
      "        [-0.7200, -1.1600, -1.3800, -1.4300, -1.2600, -0.7600],\n",
      "        [-1.1900, -1.1100, -1.0700, -1.3100, -1.3300, -1.1900],\n",
      "        [-1.0800, -1.0900, -1.1100, -1.0600, -1.1000, -1.0800],\n",
      "        [-1.0100, -1.1700, -0.8900, -1.1900, -1.2900, -1.1500],\n",
      "        [-1.3500, -1.3000, -1.0400, -0.7200, -1.1300, -1.3500],\n",
      "        [-1.0900, -1.3500, -1.2400, -1.2500, -1.2900, -1.2900],\n",
      "        [-1.1000, -1.3200, -1.2400, -1.2600, -1.1300, -1.2400],\n",
      "        [-1.3400, -0.8600, -1.3400, -0.7700, -1.2000, -1.3400],\n",
      "        [-1.2900, -1.2500, -1.2000, -0.9900, -1.2100, -1.2900],\n",
      "        [-1.0100, -1.0200, -1.0800, -1.0800, -1.0300, -1.1000],\n",
      "        [-0.9100, -1.1100, -1.0600, -1.0200, -0.9300, -0.9100],\n",
      "        [-1.3800, -1.3400, -1.0400, -1.3300, -1.3400, -1.3800],\n",
      "        [-0.9100, -0.9300, -1.0500, -1.0100, -1.0400, -1.0200],\n",
      "        [-1.0500, -1.0900, -1.0800, -0.9700, -1.0500, -1.0500],\n",
      "        [-1.0900, -1.2800, -1.1400, -1.2200, -1.3600, -1.2700],\n",
      "        [-1.3900, -1.3800, -1.4000, -1.3000, -0.9400, -1.3900],\n",
      "        [-1.4200, -1.3300, -1.2400, -1.4000, -1.2800, -1.4200],\n",
      "        [-1.0000, -1.0800, -1.0300, -1.0400, -0.9600, -1.0000],\n",
      "        [-1.2400, -1.0600, -1.1600, -0.8500, -1.3400, -1.2400],\n",
      "        [-1.2300, -1.3200, -1.0300, -1.2900, -1.0900, -1.2300],\n",
      "        [-1.0400, -1.1100, -1.1200, -1.1000, -1.0000, -1.1400],\n",
      "        [-1.1300, -1.0800, -1.0700, -1.1100, -0.9400, -1.1300],\n",
      "        [-0.9300, -1.2000, -1.3300, -1.3500, -1.2700, -1.1300],\n",
      "        [-1.4200, -1.3500, -1.4700, -0.8300, -1.3300, -1.4200],\n",
      "        [-0.9800, -1.3900, -1.2500, -1.3900, -0.7000, -0.9900],\n",
      "        [-1.3700, -1.3300, -1.1000, -1.2800, -1.2500, -1.3700],\n",
      "        [-1.0900, -1.0600, -1.0900, -1.0500, -1.1200, -1.0900],\n",
      "        [-1.0300, -0.7600, -0.9800, -0.9200, -1.1700, -1.1300],\n",
      "        [-0.7300, -1.2200, -1.3700, -0.7600, -1.4200, -0.7500],\n",
      "        [-1.3900, -1.1700, -1.1400, -1.3300, -0.5900, -1.3900],\n",
      "        [-0.8600, -0.8100, -1.1400, -0.9900, -1.0000, -1.1600]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_default2_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic_time.csv\")\n",
    "train_dynamic_time_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_dynamic_time_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_dynamic_time_dataloader = DataLoader(train_dynamic_time_dataset, batch_size=64, shuffle=False)\n",
    "test_dynamic_time_dataloader = DataLoader(test_dynamic_time_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 12]) torch.Size([64, 6])\n",
      "input1: tensor([[0.1400, 0.9600, 0.2000, 0.3200, 0.9600, 0.1300, 0.4600, 0.4600, 0.4400,\n",
      "         0.7200, 0.1900, 0.2600],\n",
      "        [0.8300, 0.7600, 0.7400, 0.9400, 0.8200, 0.8600, 0.9400, 0.7700, 0.7500,\n",
      "         0.8100, 0.2800, 0.1400],\n",
      "        [0.8100, 0.9400, 0.9000, 0.8400, 0.8600, 0.9800, 0.8100, 0.9900, 0.7500,\n",
      "         0.7800, 0.0600, 0.1500],\n",
      "        [0.7100, 0.7100, 0.8600, 0.9100, 0.7800, 0.7600, 0.8500, 0.8200, 0.9700,\n",
      "         0.9000, 0.2000, 0.1100],\n",
      "        [0.9800, 0.8600, 0.7700, 0.8900, 0.7500, 0.9800, 0.7000, 0.8100, 0.7900,\n",
      "         0.8200, 0.1500, 0.1900],\n",
      "        [0.8300, 0.3800, 0.1300, 0.2200, 0.9200, 0.8100, 0.7800, 0.4700, 0.8700,\n",
      "         0.5300, 0.0400, 0.2100],\n",
      "        [0.4600, 0.2400, 0.9800, 0.6300, 0.4600, 0.3300, 0.8300, 0.1100, 0.4600,\n",
      "         0.6700, 0.2800, 0.0200],\n",
      "        [0.8000, 0.4000, 0.1800, 0.9200, 0.9000, 0.0500, 0.8400, 0.8400, 0.3400,\n",
      "         0.3400, 0.1600, 0.2400],\n",
      "        [0.9700, 0.9200, 0.8000, 0.8300, 0.9100, 0.9600, 0.9100, 0.9400, 0.9200,\n",
      "         0.7400, 0.1000, 0.2400],\n",
      "        [0.7700, 0.1100, 0.2400, 0.2800, 0.7900, 0.2800, 0.9800, 0.4000, 0.3800,\n",
      "         0.5300, 0.2700, 0.2900],\n",
      "        [0.9600, 0.9800, 0.9200, 0.8600, 0.8400, 0.9900, 0.8600, 0.7200, 0.7400,\n",
      "         0.8100, 0.1200, 0.1700],\n",
      "        [0.7900, 0.4000, 0.7800, 0.5400, 0.6300, 0.0300, 0.9300, 0.5000, 0.1800,\n",
      "         0.4000, 0.0800, 0.1700],\n",
      "        [0.8400, 0.7100, 0.7200, 0.7700, 0.8200, 0.8100, 0.8200, 0.9300, 0.8200,\n",
      "         0.7100, 0.0500, 0.2400],\n",
      "        [0.7000, 0.7100, 0.8700, 0.7400, 0.9100, 0.8300, 0.8400, 0.9700, 0.9500,\n",
      "         0.8000, 0.2800, 0.1400],\n",
      "        [0.6700, 0.4000, 0.0900, 0.3700, 0.0500, 0.4100, 0.2000, 0.8400, 0.0500,\n",
      "         0.9200, 0.1000, 0.0700],\n",
      "        [0.4700, 0.3100, 0.4000, 0.3600, 0.0500, 0.5600, 0.7000, 0.2700, 0.6400,\n",
      "         0.6400, 0.1000, 0.1800],\n",
      "        [0.8100, 0.8200, 0.9000, 0.7500, 0.8000, 0.9800, 0.9500, 0.8600, 0.9800,\n",
      "         0.9300, 0.1500, 0.2300],\n",
      "        [0.9300, 0.3800, 0.4100, 0.6200, 0.5100, 0.5300, 0.3900, 0.8100, 0.5100,\n",
      "         0.1200, 0.0700, 0.2700],\n",
      "        [0.9700, 0.4900, 0.9300, 0.1900, 0.1500, 0.6200, 0.1300, 0.3800, 0.7800,\n",
      "         0.4700, 0.2300, 0.0500],\n",
      "        [0.9700, 0.7300, 0.7800, 0.8900, 0.8400, 0.7100, 0.9800, 0.7800, 0.9500,\n",
      "         0.8600, 0.2300, 0.0700],\n",
      "        [0.1300, 0.2200, 0.5800, 0.6000, 0.1900, 0.2400, 0.6700, 0.2200, 0.9800,\n",
      "         0.9400, 0.1200, 0.0400],\n",
      "        [0.7000, 0.9600, 0.8100, 0.8300, 0.7900, 0.8700, 0.8500, 0.7100, 0.9900,\n",
      "         0.8700, 0.1500, 0.0000],\n",
      "        [0.8300, 0.9500, 0.9400, 0.9300, 0.8600, 0.8600, 0.8900, 0.9700, 0.9500,\n",
      "         0.7200, 0.0700, 0.2800],\n",
      "        [0.6000, 0.4900, 0.0800, 0.8900, 0.0200, 0.5400, 0.6700, 0.4500, 0.4100,\n",
      "         0.5600, 0.1000, 0.0200],\n",
      "        [0.7400, 0.7100, 0.7100, 0.8100, 0.9300, 0.7400, 0.7700, 0.7300, 0.7400,\n",
      "         0.7600, 0.0100, 0.1600],\n",
      "        [0.4800, 0.9500, 0.5900, 0.9400, 0.5500, 0.5500, 0.1400, 0.1700, 0.1300,\n",
      "         0.5000, 0.2300, 0.2200],\n",
      "        [0.8600, 0.1300, 0.4100, 0.4000, 0.7000, 0.8000, 0.8400, 0.2100, 0.1200,\n",
      "         0.2300, 0.0100, 0.1800],\n",
      "        [0.9400, 0.9000, 0.7500, 0.7600, 0.8100, 0.9500, 0.9400, 0.7300, 0.8900,\n",
      "         0.8700, 0.0600, 0.1900],\n",
      "        [0.7400, 0.4300, 0.9200, 0.3400, 0.8800, 0.3800, 0.9600, 0.0300, 0.9100,\n",
      "         0.6200, 0.0100, 0.0000],\n",
      "        [0.7200, 0.2500, 0.0400, 0.8300, 0.1200, 0.2200, 0.3600, 0.6000, 0.7600,\n",
      "         0.1900, 0.0000, 0.1300],\n",
      "        [0.8800, 0.7500, 0.7800, 0.8100, 0.9400, 0.9900, 0.9800, 0.7200, 0.9200,\n",
      "         0.9800, 0.1700, 0.2200],\n",
      "        [0.6600, 0.5700, 0.2000, 0.8300, 0.3000, 0.6100, 0.8400, 0.7100, 0.0400,\n",
      "         0.8500, 0.2500, 0.1700],\n",
      "        [0.7500, 0.2500, 0.3200, 0.8900, 0.5600, 0.7200, 0.6700, 0.7900, 1.0000,\n",
      "         0.7100, 0.3000, 0.0500],\n",
      "        [0.5700, 0.2700, 0.9500, 0.8200, 0.0800, 0.9400, 0.2000, 0.7600, 0.0900,\n",
      "         0.2800, 0.1500, 0.0600],\n",
      "        [0.5000, 0.2000, 0.1300, 0.5200, 0.4400, 0.9400, 0.9700, 0.5600, 0.5400,\n",
      "         0.8100, 0.2600, 0.2200],\n",
      "        [0.9600, 0.9400, 0.9800, 0.7100, 0.7700, 0.9300, 0.8700, 0.7100, 0.9600,\n",
      "         0.8900, 0.1100, 0.2300],\n",
      "        [0.2100, 0.7400, 0.6200, 0.2600, 0.7800, 0.7600, 0.4300, 0.8500, 0.3600,\n",
      "         0.7200, 0.0100, 0.1300],\n",
      "        [0.6300, 0.7600, 0.5700, 0.1800, 0.2700, 0.2000, 0.3100, 0.6300, 1.0000,\n",
      "         0.5000, 0.1300, 0.1500],\n",
      "        [0.1500, 0.9400, 0.4100, 0.8100, 0.1200, 0.8400, 0.8900, 0.8800, 0.6900,\n",
      "         0.2100, 0.1800, 0.0300],\n",
      "        [0.1800, 0.8200, 0.7500, 0.4300, 0.1000, 0.8900, 0.6600, 0.2200, 0.4400,\n",
      "         0.7400, 0.1100, 0.1300],\n",
      "        [0.1700, 0.3400, 0.4100, 0.8300, 0.1100, 0.0700, 0.7800, 0.4000, 0.9200,\n",
      "         0.6900, 0.2900, 0.0700],\n",
      "        [0.8100, 0.9200, 0.1200, 0.8600, 0.3300, 0.5400, 0.8700, 0.7900, 0.8100,\n",
      "         0.9600, 0.1800, 0.1500],\n",
      "        [0.8100, 0.8300, 0.9700, 0.9400, 0.9300, 0.7100, 0.8800, 0.7300, 0.9000,\n",
      "         0.8900, 0.0600, 0.0800],\n",
      "        [0.8000, 0.7200, 0.9300, 0.9500, 0.7300, 0.9000, 0.7200, 0.8300, 0.7200,\n",
      "         0.8000, 0.0500, 0.2800],\n",
      "        [0.9600, 0.2500, 0.4800, 0.5400, 0.9500, 0.9200, 0.3700, 0.8200, 0.8700,\n",
      "         1.0000, 0.1800, 0.1600],\n",
      "        [0.8100, 0.7400, 0.9100, 0.8400, 0.7000, 0.9100, 0.8000, 0.9400, 0.7300,\n",
      "         0.8100, 0.0300, 0.1000],\n",
      "        [0.9800, 0.9000, 0.9700, 0.7700, 0.8100, 0.8600, 0.9500, 0.9000, 0.7700,\n",
      "         0.9400, 0.2400, 0.2800],\n",
      "        [0.5200, 0.7600, 0.3900, 0.6400, 0.8400, 0.3500, 0.2200, 0.9900, 0.7200,\n",
      "         0.0900, 0.1700, 0.1200],\n",
      "        [0.9800, 0.2500, 0.8400, 1.0000, 0.4400, 0.8500, 0.4900, 0.3200, 0.9900,\n",
      "         0.9400, 0.0500, 0.0500],\n",
      "        [0.8800, 0.0700, 0.5300, 0.0900, 0.9400, 0.9500, 0.8300, 0.1100, 0.9600,\n",
      "         0.9500, 0.3000, 0.0600],\n",
      "        [0.9900, 0.7700, 0.9200, 0.7400, 0.9500, 0.9100, 0.8300, 0.7200, 0.9100,\n",
      "         0.8900, 0.0200, 0.1400],\n",
      "        [0.6900, 0.0600, 0.5600, 0.8100, 0.4800, 0.1200, 0.4400, 0.7000, 0.6800,\n",
      "         0.8800, 0.0500, 0.2400],\n",
      "        [0.6500, 0.3400, 0.3500, 0.6900, 0.9200, 0.9300, 0.4500, 0.7400, 0.8700,\n",
      "         0.1700, 0.2900, 0.0100],\n",
      "        [0.9800, 0.7400, 0.8400, 0.7300, 0.9700, 0.9100, 0.7300, 0.7500, 0.7900,\n",
      "         0.9100, 0.1600, 0.0800],\n",
      "        [0.9500, 0.7900, 0.9400, 0.7700, 0.9500, 0.9800, 0.8000, 0.7100, 0.9900,\n",
      "         0.9600, 0.2500, 0.1900],\n",
      "        [0.8500, 0.8400, 0.9800, 0.3000, 0.7700, 0.8600, 0.3000, 0.9300, 0.5800,\n",
      "         0.4800, 0.2200, 0.0400],\n",
      "        [0.8500, 0.7200, 0.9600, 0.3600, 0.0200, 0.1200, 0.1400, 0.8300, 0.9100,\n",
      "         0.0700, 0.2600, 0.2700],\n",
      "        [0.4500, 0.8100, 0.8100, 0.2100, 0.1100, 0.9700, 0.2500, 0.1400, 0.5600,\n",
      "         0.2100, 0.0300, 0.1300],\n",
      "        [0.9600, 0.3500, 0.5700, 0.4300, 0.7300, 0.3800, 0.3700, 0.7600, 0.1800,\n",
      "         1.0000, 0.1600, 0.1200],\n",
      "        [0.8400, 0.7100, 0.9700, 0.9300, 0.8000, 0.8000, 0.7600, 0.8700, 0.7500,\n",
      "         0.9700, 0.1100, 0.1200],\n",
      "        [0.4700, 0.2100, 0.6100, 0.4400, 0.1000, 0.3900, 0.3200, 0.8600, 0.7200,\n",
      "         0.5700, 0.0100, 0.2300],\n",
      "        [0.0800, 0.7400, 0.5300, 0.9600, 0.3200, 0.2300, 0.9600, 0.3300, 0.3100,\n",
      "         0.2000, 0.0300, 0.0600],\n",
      "        [0.5700, 0.5300, 0.0200, 0.5400, 0.7500, 0.6200, 0.0800, 0.0400, 0.1500,\n",
      "         0.9700, 0.1300, 0.1900],\n",
      "        [0.4100, 0.3900, 0.4500, 0.6300, 0.6700, 0.8100, 0.5100, 0.5100, 0.6800,\n",
      "         0.5500, 0.2300, 0.2800]])\n",
      "labels: tensor([[-1.1300, -1.3700, -0.9600, -1.3700, -1.1300, -1.4300],\n",
      "        [-0.9100, -0.9300, -1.0400, -0.9600, -1.0400, -0.9100],\n",
      "        [-0.8700, -1.0300, -1.1100, -1.0700, -1.0800, -0.9900],\n",
      "        [-1.0900, -0.9900, -1.0300, -1.0900, -0.9800, -1.0900],\n",
      "        [-0.9300, -1.1000, -1.0100, -1.1000, -1.1100, -0.9300],\n",
      "        [-1.2200, -1.2300, -0.7500, -1.2600, -1.1800, -1.2800],\n",
      "        [-0.9900, -1.0400, -1.2900, -1.0400, -1.1400, -1.0500],\n",
      "        [-0.7800, -1.4100, -1.3600, -1.3400, -1.2800, -0.9900],\n",
      "        [-1.0200, -1.0600, -0.9300, -1.0600, -1.0300, -1.0200],\n",
      "        [-0.8500, -1.0800, -0.8400, -1.1100, -1.2900, -1.1700],\n",
      "        [-0.9500, -1.1100, -1.0500, -1.1300, -1.0900, -1.0900],\n",
      "        [-0.7200, -1.2100, -1.2100, -0.9900, -1.2600, -0.8700],\n",
      "        [-0.9000, -1.0500, -0.8600, -0.9100, -1.0300, -1.0500],\n",
      "        [-1.0800, -1.0700, -0.9900, -1.0400, -1.0900, -1.0800],\n",
      "        [-1.2700, -1.1200, -0.7700, -0.8000, -1.2500, -1.3300],\n",
      "        [-0.9100, -0.8300, -0.8100, -1.0100, -1.0800, -1.1400],\n",
      "        [-1.0900, -0.9300, -1.0100, -1.0900, -1.0500, -1.0900],\n",
      "        [-0.8200, -1.3200, -1.2200, -1.1300, -1.1100, -0.8300],\n",
      "        [-1.1800, -1.3700, -1.3300, -1.0400, -0.8000, -1.1800],\n",
      "        [-1.0500, -1.0800, -1.0000, -0.9400, -1.0900, -1.0500],\n",
      "        [-1.4100, -0.6700, -1.1300, -0.7100, -1.2100, -1.4100],\n",
      "        [-1.1100, -1.0600, -1.1000, -1.0600, -1.1500, -1.1100],\n",
      "        [-1.0600, -1.0500, -1.0400, -0.9600, -1.0700, -1.0600],\n",
      "        [-0.8700, -1.0200, -1.2100, -0.8500, -1.1000, -0.8800],\n",
      "        [-0.8500, -0.9600, -1.0900, -1.0500, -0.9900, -1.0400],\n",
      "        [-0.8500, -1.3100, -1.2900, -1.1800, -0.6700, -1.0500],\n",
      "        [-0.6100, -1.2300, -0.9600, -1.4000, -1.2100, -0.7700],\n",
      "        [-1.0000, -1.0500, -1.0500, -1.0600, -1.1100, -1.0000],\n",
      "        [-1.1200, -0.9500, -1.1500, -1.1100, -1.1800, -1.1300],\n",
      "        [-1.1600, -1.1000, -1.3800, -0.7400, -1.1300, -1.1400],\n",
      "        [-1.1100, -1.0100, -0.9500, -1.1200, -1.1100, -1.1100],\n",
      "        [-1.1600, -1.2700, -1.3200, -1.1000, -1.1500, -1.1600],\n",
      "        [-1.3200, -1.0600, -1.2100, -1.1800, -1.3100, -1.3200],\n",
      "        [-0.7200, -1.1600, -1.3800, -1.4300, -1.2600, -0.7600],\n",
      "        [-1.1900, -1.1100, -1.0700, -1.3100, -1.3300, -1.1900],\n",
      "        [-1.0800, -1.0900, -1.1100, -1.0600, -1.1000, -1.0800],\n",
      "        [-1.0100, -1.1700, -0.8900, -1.1900, -1.2900, -1.1500],\n",
      "        [-1.3500, -1.3000, -1.0400, -0.7200, -1.1300, -1.3500],\n",
      "        [-1.0900, -1.3500, -1.2400, -1.2500, -1.2900, -1.2900],\n",
      "        [-1.1000, -1.3200, -1.2400, -1.2600, -1.1300, -1.2400],\n",
      "        [-1.3400, -0.8600, -1.3400, -0.7700, -1.2000, -1.3400],\n",
      "        [-1.2900, -1.2500, -1.2000, -0.9900, -1.2100, -1.2900],\n",
      "        [-1.0100, -1.0200, -1.0800, -1.0800, -1.0300, -1.1000],\n",
      "        [-0.9100, -1.1100, -1.0600, -1.0200, -0.9300, -0.9100],\n",
      "        [-1.3800, -1.3400, -1.0400, -1.3300, -1.3400, -1.3800],\n",
      "        [-0.9100, -0.9300, -1.0500, -1.0100, -1.0400, -1.0200],\n",
      "        [-1.0500, -1.0900, -1.0800, -0.9700, -1.0500, -1.0500],\n",
      "        [-1.0900, -1.2800, -1.1400, -1.2200, -1.3600, -1.2700],\n",
      "        [-1.3900, -1.3800, -1.4000, -1.3000, -0.9400, -1.3900],\n",
      "        [-1.4200, -1.3300, -1.2400, -1.4000, -1.2800, -1.4200],\n",
      "        [-1.0000, -1.0800, -1.0300, -1.0400, -0.9600, -1.0000],\n",
      "        [-1.2400, -1.0600, -1.1600, -0.8500, -1.3400, -1.2400],\n",
      "        [-1.2300, -1.3200, -1.0300, -1.2900, -1.0900, -1.2300],\n",
      "        [-1.0400, -1.1100, -1.1200, -1.1000, -1.0000, -1.1400],\n",
      "        [-1.1300, -1.0800, -1.0700, -1.1100, -0.9400, -1.1300],\n",
      "        [-0.9300, -1.2000, -1.3300, -1.3500, -1.2700, -1.1300],\n",
      "        [-1.4200, -1.3500, -1.4700, -0.8300, -1.3300, -1.4200],\n",
      "        [-0.9800, -1.3900, -1.2500, -1.3900, -0.7000, -0.9900],\n",
      "        [-1.3700, -1.3300, -1.1000, -1.2800, -1.2500, -1.3700],\n",
      "        [-1.0900, -1.0600, -1.0900, -1.0500, -1.1200, -1.0900],\n",
      "        [-1.0300, -0.7600, -0.9800, -0.9200, -1.1700, -1.1300],\n",
      "        [-0.7300, -1.2200, -1.3700, -0.7600, -1.4200, -0.7500],\n",
      "        [-1.3900, -1.1700, -1.1400, -1.3300, -0.5900, -1.3900],\n",
      "        [-0.8600, -0.8100, -1.1400, -0.9900, -1.0000, -1.1600]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_default2_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic2_time.csv\")\n",
    "train_dynamic2_time_dataset = Pr_Dataset(data_path, train=True)\n",
    "test_dynamic2_time_dataset = Pr_Dataset(data_path, train=False)\n",
    "train_dynamic2_time_dataloader = DataLoader(train_dynamic2_time_dataset, batch_size=64, shuffle=False)\n",
    "test_dynamic2_time_dataloader = DataLoader(test_dynamic2_time_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 13]) torch.Size([64, 6])\n",
      "input1: tensor([[ 0.5900,  0.0200,  0.9000,  0.6100,  0.6900,  0.9900,  0.3300,  0.7600,\n",
      "          0.7500,  0.0100,  0.0700,  0.2100, -0.1900],\n",
      "        [ 0.7700,  0.4800,  0.7200,  0.2900,  0.2700,  0.2400,  0.8800,  0.7200,\n",
      "          0.0600,  0.7800,  0.1700,  0.1700, -2.1000],\n",
      "        [ 0.2600,  0.7100,  0.0700,  0.6700,  0.3100,  0.8800,  0.1000,  0.6400,\n",
      "          0.5800,  0.2000,  0.1400,  0.0800, -1.4800],\n",
      "        [ 0.7500,  0.7000,  0.7100,  0.9200,  0.7100,  0.9400,  0.8400,  0.8300,\n",
      "          0.7400,  0.8100,  0.2800,  0.2000, -1.3800],\n",
      "        [ 0.5800,  0.2700,  0.4000,  0.2100,  0.7700,  1.0000,  0.8600,  0.5500,\n",
      "          0.1900,  0.7100,  0.2800,  0.2000, -1.2200],\n",
      "        [ 0.6800,  0.6900,  0.9000,  0.3300,  0.2400,  0.3100,  0.0600,  0.8200,\n",
      "          0.6700,  0.6000,  0.2800,  0.0700, -0.9800],\n",
      "        [ 0.5400,  0.9500,  0.6900,  0.3200,  0.1700,  0.4500,  0.8500,  0.3100,\n",
      "          0.7500,  0.9200,  0.2900,  0.2100, -1.4200],\n",
      "        [ 0.9700,  0.8000,  0.8100,  0.9500,  0.8900,  0.9200,  0.9600,  0.8700,\n",
      "          0.7500,  0.9400,  0.0300,  0.1200, -2.2500],\n",
      "        [ 0.6500,  1.0000,  0.3700,  0.8300,  0.2600,  0.3300,  0.7200,  0.0700,\n",
      "          0.9000,  0.5400,  0.1600,  0.1000, -1.4700],\n",
      "        [ 0.7500,  0.8200,  0.9800,  0.7200,  0.8500,  0.7200,  0.7600,  0.7800,\n",
      "          0.9500,  0.9300,  0.1800,  0.2700, -0.4900],\n",
      "        [ 0.4400,  0.0600,  0.9100,  0.4200,  0.3500,  0.3200,  0.7200,  0.5600,\n",
      "          0.6600,  0.5100,  0.1500,  0.0800, -0.0500],\n",
      "        [ 0.9600,  0.7300,  0.7500,  0.8700,  0.8900,  0.9300,  0.9500,  0.8200,\n",
      "          0.9500,  0.8300,  0.0300,  0.0300, -1.0300],\n",
      "        [ 0.9300,  0.3900,  0.2100,  0.7800,  0.5900,  0.9500,  0.0800,  0.8100,\n",
      "          0.8700,  0.3200,  0.1000,  0.0400, -2.3300],\n",
      "        [ 0.5700,  0.6000,  0.3600,  0.9200,  0.0400,  0.8900,  0.3400,  0.0200,\n",
      "          0.6900,  0.9900,  0.0300,  0.2200, -0.5200],\n",
      "        [ 0.9300,  0.3600,  0.7300,  0.0900,  0.2600,  0.2100,  0.4900,  0.5300,\n",
      "          0.0200,  0.2800,  0.1100,  0.1600, -1.0000],\n",
      "        [ 0.8900,  0.3600,  0.2900,  0.1200,  0.4200,  0.7800,  0.8300,  0.2900,\n",
      "          0.4800,  0.5100,  0.1900,  0.2400, -2.9600],\n",
      "        [ 0.1500,  0.0500,  0.1300,  0.5600,  0.2700,  0.7400,  0.3900,  0.0800,\n",
      "          0.3800,  0.7200,  0.0700,  0.1000, -3.1600],\n",
      "        [ 0.3700,  0.5300,  0.5500,  0.4100,  0.3800,  0.3000,  0.6800,  0.2100,\n",
      "          0.1600,  0.3700,  0.2400,  0.0700, -1.1900],\n",
      "        [ 0.6500,  0.3400,  0.0600,  0.9100,  0.2000,  0.5400,  0.5000,  0.0900,\n",
      "          0.4900,  0.7600,  0.1200,  0.0900, -0.6700],\n",
      "        [ 0.8800,  0.6400,  0.3400,  0.6900,  0.4800,  0.0400,  0.3900,  0.8200,\n",
      "          0.2000,  0.7700,  0.1200,  0.0800, -0.3300],\n",
      "        [ 0.8300,  0.8100,  0.8000,  1.0000,  0.9600,  0.8700,  0.7200,  0.7600,\n",
      "          0.8300,  0.9900,  0.0100,  0.1700, -1.5000],\n",
      "        [ 0.5700,  0.3300,  0.7800,  0.6600,  0.4600,  0.0800,  0.4500,  0.3400,\n",
      "          0.9600,  0.8700,  0.2100,  0.2800, -0.9200],\n",
      "        [ 0.1500,  0.8200,  0.0100,  0.0200,  0.4400,  0.9700,  0.6900,  0.1800,\n",
      "          0.9000,  0.8900,  0.0700,  0.0300, -0.8800],\n",
      "        [ 0.7100,  0.9600,  0.8600,  0.8600,  0.8300,  0.9000,  0.7300,  0.9800,\n",
      "          0.7000,  0.7200,  0.1200,  0.1100, -0.6200],\n",
      "        [ 0.8800,  0.8100,  0.7700,  0.8500,  0.7400,  0.8000,  0.8200,  0.7200,\n",
      "          0.9200,  0.9600,  0.1600,  0.1500, -1.1800],\n",
      "        [ 0.4400,  0.4600,  0.3200,  0.0800,  0.0400,  0.2300,  0.3500,  0.0800,\n",
      "          0.7700,  0.8000,  0.0200,  0.0100, -2.2200],\n",
      "        [ 0.5000,  0.5300,  0.8700,  0.9500,  0.4700,  0.6600,  0.7000,  0.8900,\n",
      "          0.1300,  0.3700,  0.1400,  0.0200, -2.3500],\n",
      "        [ 0.5200,  0.5800,  0.7800,  0.0100,  0.3600,  0.6000,  0.8500,  0.4600,\n",
      "          0.7700,  0.7900,  0.0200,  0.1200, -1.4700],\n",
      "        [ 0.8100,  0.8100,  0.9400,  0.9800,  0.7300,  0.9200,  0.9000,  0.7400,\n",
      "          0.8900,  0.7900,  0.2000,  0.0800, -1.6000],\n",
      "        [ 0.6300,  0.7200,  0.9600,  0.4700,  0.7500,  0.9700,  0.4400,  0.6400,\n",
      "          0.6800,  0.6100,  0.2800,  0.2800, -1.1200],\n",
      "        [ 0.8500,  1.0000,  0.9600,  0.9100,  0.9400,  0.8800,  0.8700,  0.9200,\n",
      "          0.9800,  0.8400,  0.2200,  0.1800, -4.0800],\n",
      "        [ 0.5700,  0.9700,  0.5500,  0.8100,  0.5100,  0.8400,  0.2900,  0.5600,\n",
      "          0.9300,  0.8500,  0.1800,  0.1900, -0.6000],\n",
      "        [ 0.0300,  1.0000,  0.7900,  0.1600,  0.8400,  0.3000,  0.3500,  0.0700,\n",
      "          0.2100,  0.4400,  0.0100,  0.1900, -0.3800],\n",
      "        [ 0.4300,  0.9900,  0.8400,  0.6600,  0.5100,  0.2200,  0.7200,  0.8000,\n",
      "          0.6900,  0.7100,  0.0200,  0.2500, -1.2600],\n",
      "        [ 0.0100,  0.8200,  0.4100,  0.3900,  0.6500,  0.3900,  0.9800,  0.3200,\n",
      "          0.4800,  0.4400,  0.0600,  0.1900, -3.1600],\n",
      "        [ 0.8900,  0.4700,  0.3000,  0.3200,  0.3100,  0.3800,  0.1800,  0.1200,\n",
      "          0.0200,  0.8000,  0.2300,  0.1600, -0.2700],\n",
      "        [ 0.1500,  0.1700,  0.1200,  0.7100,  0.8200,  0.6200,  0.7800,  0.4900,\n",
      "          0.3900,  0.4700,  0.2800,  0.1800, -0.3700],\n",
      "        [ 0.9000,  0.5000,  0.0600,  0.6000,  0.3700,  0.4100,  0.4500,  0.8500,\n",
      "          0.7600,  0.7400,  0.1800,  0.2000, -1.7300],\n",
      "        [ 0.1200,  0.5600,  0.6500,  0.9900,  0.5400,  1.0000,  0.7600,  0.6800,\n",
      "          0.1600,  0.3100,  0.2600,  0.1700, -1.4100],\n",
      "        [ 0.2100,  0.2800,  0.5600,  0.9500,  0.5000,  0.4500,  0.4800,  0.2000,\n",
      "          0.0600,  0.9500,  0.0800,  0.1700, -1.7000],\n",
      "        [ 0.1100,  0.8700,  0.8900,  0.1200,  0.8900,  0.7600,  0.4900,  0.2500,\n",
      "          0.8600,  0.3500,  0.2000,  0.0200, -2.9400],\n",
      "        [ 0.0500,  0.8000,  0.7000,  0.6200,  0.0800,  0.5400,  0.3300,  0.2100,\n",
      "          0.5500,  0.4000,  0.2800,  0.0500, -0.2800],\n",
      "        [ 0.7500,  0.9900,  0.8400,  0.8300,  0.9000,  0.8100,  0.7500,  0.8100,\n",
      "          0.8700,  0.8500,  0.1200,  0.1700, -0.9200],\n",
      "        [ 0.3100,  0.4600,  0.5200,  0.8900,  0.1200,  0.6300,  0.3500,  0.3100,\n",
      "          0.7500,  0.4500,  0.0600,  0.0500, -1.6500],\n",
      "        [ 0.7200,  0.9600,  0.8500,  0.9100,  0.5400,  0.5100,  0.2000,  0.7400,\n",
      "          0.7700,  0.5600,  0.0800,  0.1200, -1.6300],\n",
      "        [ 0.5600,  0.8200,  0.4200,  0.8300,  0.9300,  0.3200,  0.8100,  0.5000,\n",
      "          0.6500,  0.4700,  0.1700,  0.1200, -0.6300],\n",
      "        [ 0.2900,  0.2600,  0.6300,  0.0700,  0.4400,  0.4100,  0.3800,  0.1500,\n",
      "          0.2100,  0.3700,  0.0600,  0.2500, -0.9500],\n",
      "        [ 0.0400,  0.0600,  0.1100,  0.5200,  0.3400,  0.4400,  0.0600,  1.0000,\n",
      "          0.9200,  0.2500,  0.2500,  0.1500, -0.8900],\n",
      "        [ 0.6200,  0.8900,  0.6300,  0.9700,  0.0300,  0.5900,  0.5900,  0.7000,\n",
      "          0.8600,  0.9400,  0.0800,  0.0100, -0.8200],\n",
      "        [ 0.9200,  0.1300,  0.1800,  0.0500,  0.7400,  0.0900,  0.9800,  0.4900,\n",
      "          0.1600,  0.6700,  0.2400,  0.1100, -1.1300],\n",
      "        [ 0.7600,  0.8900,  0.8000,  0.8500,  0.9600,  0.7800,  0.9400,  0.9900,\n",
      "          0.8700,  0.9100,  0.2300,  0.0400, -1.1000],\n",
      "        [ 0.1000,  0.6000,  0.6000,  0.9300,  0.8400,  0.3000,  0.9800,  0.4200,\n",
      "          0.6600,  0.6500,  0.2500,  0.2700, -2.0700],\n",
      "        [ 0.1800,  0.1900,  0.4600,  0.7200,  0.3200,  0.6000,  0.6900,  0.9800,\n",
      "          0.5400,  0.3700,  0.2400,  0.0300, -1.5500],\n",
      "        [ 0.1000,  0.2700,  0.7200,  0.2500,  0.3700,  0.6100,  0.4600,  0.2600,\n",
      "          0.1900,  0.1000,  0.2700,  0.0300, -0.2600],\n",
      "        [ 0.8000,  0.7000,  0.7200,  0.8800,  0.8200,  0.9500,  0.8100,  0.9600,\n",
      "          0.8700,  0.8900,  0.0400,  0.0700, -0.0600],\n",
      "        [ 0.8800,  0.9500,  1.0000,  0.9000,  0.8700,  0.9000,  0.8600,  0.8900,\n",
      "          0.8600,  0.9000,  0.1100,  0.2000, -1.4600],\n",
      "        [ 0.6000,  0.9100,  0.0300,  0.7700,  0.7000,  0.3700,  0.5400,  0.1600,\n",
      "          0.8400,  0.2500,  0.1000,  0.0100, -0.4100],\n",
      "        [ 0.0400,  0.7000,  0.6000,  0.8600,  0.6300,  1.0000,  0.7700,  1.0000,\n",
      "          0.9600,  0.2300,  0.2100,  0.2700, -0.1600],\n",
      "        [ 0.8400,  0.9300,  0.1400,  0.3700,  0.3000,  0.9200,  0.5700,  0.9600,\n",
      "          0.5500,  0.9500,  0.2500,  0.1400, -0.2400],\n",
      "        [ 0.8500,  0.5400,  0.5200,  0.6200,  0.4900,  0.9200,  0.6900,  0.1400,\n",
      "          0.4700,  0.5600,  0.0800,  0.0400, -0.9900],\n",
      "        [ 0.7500,  0.1300,  0.9900,  0.5500,  0.7900,  0.1500,  0.2100,  0.5100,\n",
      "          0.1900,  0.2700,  0.1100,  0.0700, -0.2700],\n",
      "        [ 0.7500,  0.7800,  0.7900,  0.8400,  0.7900,  0.7400,  0.7800,  0.9500,\n",
      "          0.7400,  0.9800,  0.0400,  0.1400, -0.7300],\n",
      "        [ 0.2900,  0.5500,  0.4700,  0.6700,  0.6800,  0.1600,  0.2800,  0.4700,\n",
      "          0.1500,  0.3600,  0.2400,  0.0200, -0.6000],\n",
      "        [ 0.9700,  0.9900,  0.7900,  0.9800,  0.9500,  0.9800,  0.8700,  0.7500,\n",
      "          0.9600,  0.7800,  0.0600,  0.0300, -1.4700]])\n",
      "labels: tensor([[-9.0000e-02, -1.7000e-01, -6.9000e-01, -1.9000e-01, -9.0000e-02,\n",
      "          2.5000e+02],\n",
      "        [-2.1100e+00, -2.0600e+00, -2.0200e+00, -2.6000e+00, -2.0700e+00,\n",
      "          3.3140e+03],\n",
      "        [-1.4600e+00, -1.4300e+00, -1.4900e+00, -1.4400e+00, -1.4900e+00,\n",
      "          2.7360e+03],\n",
      "        [-1.8800e+00, -1.8800e+00, -1.8800e+00, -1.8800e+00, -1.8800e+00,\n",
      "          2.5960e+03],\n",
      "        [-1.1900e+00, -1.1400e+00, -1.7200e+00, -1.7200e+00, -1.1200e+00,\n",
      "          2.2770e+03],\n",
      "        [-1.0300e+00, -1.4800e+00, -9.1000e-01, -8.5000e-01, -1.0400e+00,\n",
      "          1.9360e+03],\n",
      "        [-1.9200e+00, -1.4100e+00, -1.2600e+00, -1.9200e+00, -1.9200e+00,\n",
      "          2.4490e+03],\n",
      "        [-2.2000e+00, -2.7500e+00, -2.7500e+00, -2.2300e+00, -2.7500e+00,\n",
      "          3.5160e+03],\n",
      "        [-1.9700e+00, -1.4400e+00, -1.4000e+00, -1.4700e+00, -1.9700e+00,\n",
      "          2.6280e+03],\n",
      "        [-9.9000e-01, -9.9000e-01, -9.9000e-01, -9.9000e-01, -9.9000e-01,\n",
      "          1.1900e+03],\n",
      "        [ 1.0000e-02, -5.5000e-01, -1.0000e-02, -8.0000e-02, -7.0000e-02,\n",
      "          2.4000e+01],\n",
      "        [-1.0200e+00, -1.0200e+00, -1.0300e+00, -1.0300e+00, -1.0300e+00,\n",
      "          2.1140e+03],\n",
      "        [-2.8300e+00, -2.2800e+00, -2.3200e+00, -2.2800e+00, -2.3300e+00,\n",
      "          3.5620e+03],\n",
      "        [-5.4000e-01, -1.0200e+00, -1.0200e+00, -3.7000e-01, -1.0200e+00,\n",
      "          9.5100e+02],\n",
      "        [-1.5000e+00, -9.4000e-01, -9.4000e-01, -1.0400e+00, -9.6000e-01,\n",
      "          1.9910e+03],\n",
      "        [-3.4600e+00, -2.8300e+00, -3.4600e+00, -3.4600e+00, -2.9600e+00,\n",
      "          4.0260e+03],\n",
      "        [-3.1100e+00, -3.1700e+00, -3.1900e+00, -3.1400e+00, -3.2100e+00,\n",
      "          4.2360e+03],\n",
      "        [-1.1600e+00, -1.2600e+00, -1.2000e+00, -1.2500e+00, -1.0700e+00,\n",
      "          2.3630e+03],\n",
      "        [-6.7000e-01, -6.2000e-01, -6.2000e-01, -6.3000e-01, -6.7000e-01,\n",
      "          1.4570e+03],\n",
      "        [-3.6000e-01, -2.9000e-01, -2.9000e-01, -3.1000e-01, -2.9000e-01,\n",
      "          7.5800e+02],\n",
      "        [-1.5000e+00, -2.0000e+00, -2.0000e+00, -1.4700e+00, -2.0000e+00,\n",
      "          2.7080e+03],\n",
      "        [-8.4000e-01, -9.4000e-01, -7.7000e-01, -8.2000e-01, -1.4200e+00,\n",
      "          1.7970e+03],\n",
      "        [-8.6000e-01, -8.5000e-01, -8.7000e-01, -8.8000e-01, -9.0000e-01,\n",
      "          1.8470e+03],\n",
      "        [-1.1200e+00, -6.4000e-01, -1.1200e+00, -1.1200e+00, -5.5000e-01,\n",
      "          1.3130e+03],\n",
      "        [-1.6800e+00, -1.1800e+00, -1.1500e+00, -1.1500e+00, -1.6800e+00,\n",
      "          2.2930e+03],\n",
      "        [-2.2300e+00, -2.2200e+00, -2.2200e+00, -2.2200e+00, -2.2400e+00,\n",
      "          3.5060e+03],\n",
      "        [-2.3400e+00, -2.8500e+00, -2.3100e+00, -2.3300e+00, -2.2600e+00,\n",
      "          3.5460e+03],\n",
      "        [-1.4700e+00, -1.3900e+00, -1.4800e+00, -1.4300e+00, -1.5000e+00,\n",
      "          2.7060e+03],\n",
      "        [-2.1000e+00, -2.1000e+00, -1.5400e+00, -2.1000e+00, -2.1000e+00,\n",
      "          2.8000e+03],\n",
      "        [-1.1200e+00, -1.6200e+00, -1.6200e+00, -1.0300e+00, -1.1100e+00,\n",
      "          2.2400e+03],\n",
      "        [-4.5800e+00, -4.5800e+00, -4.5800e+00, -4.5800e+00, -4.5800e+00,\n",
      "          4.8730e+03],\n",
      "        [-1.1000e+00, -6.0000e-01, -1.1000e+00, -4.7000e-01, -1.1000e+00,\n",
      "          1.1510e+03],\n",
      "        [-8.8000e-01, -3.0000e-01, -3.2000e-01, -2.9000e-01, -4.1000e-01,\n",
      "          7.5900e+02],\n",
      "        [-1.7600e+00, -1.2300e+00, -1.1000e+00, -1.7600e+00, -1.3100e+00,\n",
      "          2.2310e+03],\n",
      "        [-3.6600e+00, -3.1500e+00, -3.1100e+00, -3.6600e+00, -3.1600e+00,\n",
      "          4.2690e+03],\n",
      "        [-7.7000e-01, -2.5000e-01, -2.4000e-01, -2.0000e-01, -2.5000e-01,\n",
      "          5.5400e+02],\n",
      "        [-2.3000e-01, -3.0000e-01, -8.7000e-01, -8.7000e-01, -3.2000e-01,\n",
      "          6.1400e+02],\n",
      "        [-2.2300e+00, -1.6500e+00, -1.6400e+00, -2.2300e+00, -1.7600e+00,\n",
      "          2.9170e+03],\n",
      "        [-1.3000e+00, -1.9100e+00, -1.9100e+00, -1.9100e+00, -1.2800e+00,\n",
      "          2.4650e+03],\n",
      "        [-1.6500e+00, -2.2000e+00, -1.6800e+00, -1.6400e+00, -2.2000e+00,\n",
      "          2.9280e+03],\n",
      "        [-2.8300e+00, -3.4400e+00, -3.4400e+00, -2.9500e+00, -3.4400e+00,\n",
      "          4.1190e+03],\n",
      "        [-1.7000e-01, -3.9000e-01, -1.7000e-01, -3.0000e-01, -3.5000e-01,\n",
      "          7.9100e+02],\n",
      "        [-1.3800e+00, -9.0000e-01, -1.3800e+00, -8.6000e-01, -1.3800e+00,\n",
      "          1.9200e+03],\n",
      "        [-1.6400e+00, -1.6700e+00, -1.6300e+00, -1.6400e+00, -1.6600e+00,\n",
      "          2.9130e+03],\n",
      "        [-2.1300e+00, -2.1300e+00, -1.5700e+00, -1.5900e+00, -1.5900e+00,\n",
      "          2.8280e+03],\n",
      "        [-6.1000e-01, -5.8000e-01, -1.1300e+00, -6.5000e-01, -6.1000e-01,\n",
      "          1.4270e+03],\n",
      "        [-9.9000e-01, -8.6000e-01, -1.0400e+00, -8.8000e-01, -1.0200e+00,\n",
      "          2.0590e+03],\n",
      "        [-8.0000e-01, -8.3000e-01, -8.5000e-01, -1.3900e+00, -1.3900e+00,\n",
      "          1.7670e+03],\n",
      "        [-8.0000e-01, -8.0000e-01, -7.6000e-01, -8.0000e-01, -8.1000e-01,\n",
      "          1.7610e+03],\n",
      "        [-1.6300e+00, -1.0500e+00, -1.1300e+00, -1.6300e+00, -1.0500e+00,\n",
      "          2.1570e+03],\n",
      "        [-1.0600e+00, -1.6000e+00, -1.6000e+00, -1.6000e+00, -1.6000e+00,\n",
      "          2.1690e+03],\n",
      "        [-1.9600e+00, -2.5700e+00, -2.5700e+00, -2.5700e+00, -2.0800e+00,\n",
      "          3.3770e+03],\n",
      "        [-1.5000e+00, -1.5200e+00, -1.4800e+00, -2.0500e+00, -1.6000e+00,\n",
      "          2.7470e+03],\n",
      "        [-1.6000e-01, -3.7000e-01, -2.2000e-01, -3.2000e-01, -2.5000e-01,\n",
      "          6.6900e+02],\n",
      "        [-3.0000e-02, -6.0000e-02, -5.6000e-01, -5.6000e-01, -8.0000e-02,\n",
      "          7.9000e+01],\n",
      "        [-1.9600e+00, -1.9600e+00, -1.9600e+00, -1.9600e+00, -1.9600e+00,\n",
      "          2.7050e+03],\n",
      "        [-3.8000e-01, -3.4000e-01, -4.2000e-01, -4.1000e-01, -4.3000e-01,\n",
      "          1.0270e+03],\n",
      "        [-5.0000e-02, -6.6000e-01, -6.6000e-01, -6.6000e-01, -6.6000e-01,\n",
      "          1.4100e+02],\n",
      "        [-7.4000e-01, -5.0000e-02, -7.4000e-01, -7.4000e-01, -7.4000e-01,\n",
      "          1.4200e+02],\n",
      "        [-1.0000e+00, -9.6000e-01, -9.7000e-01, -9.8000e-01, -9.5000e-01,\n",
      "          2.0000e+03],\n",
      "        [-2.4000e-01, -7.7000e-01, -2.5000e-01, -2.2000e-01, -2.0000e-01,\n",
      "          5.3900e+02],\n",
      "        [-7.3000e-01, -7.5000e-01, -7.0000e-01, -1.2300e+00, -1.2300e+00,\n",
      "          1.6490e+03],\n",
      "        [-5.5000e-01, -6.0000e-01, -7.1000e-01, -5.4000e-01, -5.1000e-01,\n",
      "          1.2440e+03],\n",
      "        [-1.9700e+00, -1.9700e+00, -1.9700e+00, -1.4500e+00, -1.9700e+00,\n",
      "          2.6840e+03]])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dynamic2_time_dataloader:\n",
    "    input, labels = batch\n",
    "    print(input.shape, labels.shape)\n",
    "    print(f\"input1: {input}\\nlabels: {labels}\")\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Multi-modal(Untrained + Pretrained)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward : Dynamic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 64):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 32)\n",
    "        self.fc2_2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(64, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x1 = 1 - x1\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0033,  0.0019, -0.0006,  0.0022, -0.0006, -0.0012]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1244, Train Acc: 69.80%, Test Loss: 0.0754, Test Acc: 76.94%\n",
      "Epoch 2: Train Loss: 0.0648, Train Acc: 79.27%, Test Loss: 0.0548, Test Acc: 81.17%\n",
      "Epoch 3: Train Loss: 0.0525, Train Acc: 81.37%, Test Loss: 0.0488, Test Acc: 82.25%\n",
      "Epoch 4: Train Loss: 0.0478, Train Acc: 82.12%, Test Loss: 0.0462, Test Acc: 82.70%\n",
      "Epoch 5: Train Loss: 0.0454, Train Acc: 82.56%, Test Loss: 0.0428, Test Acc: 83.31%\n",
      "Epoch 6: Train Loss: 0.0439, Train Acc: 82.76%, Test Loss: 0.0413, Test Acc: 83.89%\n",
      "Epoch 7: Train Loss: 0.0428, Train Acc: 83.01%, Test Loss: 0.0416, Test Acc: 84.06%\n",
      "Epoch 8: Train Loss: 0.0421, Train Acc: 83.15%, Test Loss: 0.0416, Test Acc: 83.86%\n",
      "Epoch 9: Train Loss: 0.0413, Train Acc: 83.31%, Test Loss: 0.0406, Test Acc: 84.00%\n",
      "Epoch 10: Train Loss: 0.0408, Train Acc: 83.39%, Test Loss: 0.0409, Test Acc: 84.09%\n",
      "Epoch 11: Train Loss: 0.0403, Train Acc: 83.48%, Test Loss: 0.0384, Test Acc: 84.32%\n",
      "Epoch 12: Train Loss: 0.0399, Train Acc: 83.47%, Test Loss: 0.0428, Test Acc: 84.01%\n",
      "Epoch 13: Train Loss: 0.0396, Train Acc: 83.57%, Test Loss: 0.0400, Test Acc: 84.18%\n",
      "Epoch 14: Train Loss: 0.0394, Train Acc: 83.65%, Test Loss: 0.0403, Test Acc: 84.36%\n",
      "Epoch 15: Train Loss: 0.0392, Train Acc: 83.66%, Test Loss: 0.0396, Test Acc: 84.02%\n",
      "Epoch 16: Train Loss: 0.0388, Train Acc: 83.74%, Test Loss: 0.0378, Test Acc: 84.43%\n",
      "Epoch 17: Train Loss: 0.0387, Train Acc: 83.77%, Test Loss: 0.0392, Test Acc: 84.27%\n",
      "Epoch 18: Train Loss: 0.0385, Train Acc: 83.80%, Test Loss: 0.0368, Test Acc: 85.06%\n",
      "Epoch 19: Train Loss: 0.0381, Train Acc: 83.89%, Test Loss: 0.0353, Test Acc: 84.87%\n",
      "Epoch 20: Train Loss: 0.0378, Train Acc: 83.94%, Test Loss: 0.0370, Test Acc: 84.63%\n",
      "Epoch 21: Train Loss: 0.0378, Train Acc: 83.98%, Test Loss: 0.0392, Test Acc: 84.51%\n",
      "Epoch 22: Train Loss: 0.0378, Train Acc: 83.97%, Test Loss: 0.0360, Test Acc: 84.61%\n",
      "Epoch 23: Train Loss: 0.0373, Train Acc: 84.02%, Test Loss: 0.0382, Test Acc: 84.53%\n",
      "Epoch 24: Train Loss: 0.0374, Train Acc: 84.04%, Test Loss: 0.0376, Test Acc: 84.43%\n",
      "Epoch 25: Train Loss: 0.0371, Train Acc: 84.12%, Test Loss: 0.0401, Test Acc: 84.16%\n",
      "Epoch 26: Train Loss: 0.0371, Train Acc: 84.18%, Test Loss: 0.0373, Test Acc: 84.60%\n",
      "Epoch 27: Train Loss: 0.0368, Train Acc: 84.24%, Test Loss: 0.0371, Test Acc: 84.57%\n",
      "Epoch 28: Train Loss: 0.0367, Train Acc: 84.24%, Test Loss: 0.0393, Test Acc: 84.11%\n",
      "Epoch 29: Train Loss: 0.0368, Train Acc: 84.27%, Test Loss: 0.0409, Test Acc: 84.21%\n",
      "Epoch 30: Train Loss: 0.0365, Train Acc: 84.28%, Test Loss: 0.0377, Test Acc: 84.78%\n",
      "Epoch 31: Train Loss: 0.0365, Train Acc: 84.36%, Test Loss: 0.0344, Test Acc: 84.84%\n",
      "Epoch 32: Train Loss: 0.0362, Train Acc: 84.42%, Test Loss: 0.0362, Test Acc: 84.88%\n",
      "Epoch 33: Train Loss: 0.0360, Train Acc: 84.43%, Test Loss: 0.0328, Test Acc: 85.15%\n",
      "Epoch 34: Train Loss: 0.0363, Train Acc: 84.46%, Test Loss: 0.0378, Test Acc: 84.60%\n",
      "Epoch 35: Train Loss: 0.0361, Train Acc: 84.51%, Test Loss: 0.0380, Test Acc: 84.37%\n",
      "Epoch 36: Train Loss: 0.0362, Train Acc: 84.49%, Test Loss: 0.0392, Test Acc: 84.89%\n",
      "Epoch 37: Train Loss: 0.0360, Train Acc: 84.50%, Test Loss: 0.0340, Test Acc: 84.91%\n",
      "Epoch 38: Train Loss: 0.0360, Train Acc: 84.59%, Test Loss: 0.0384, Test Acc: 84.29%\n",
      "Epoch 39: Train Loss: 0.0361, Train Acc: 84.59%, Test Loss: 0.0372, Test Acc: 84.89%\n",
      "Epoch 40: Train Loss: 0.0359, Train Acc: 84.56%, Test Loss: 0.0382, Test Acc: 84.20%\n",
      "Epoch 41: Train Loss: 0.0358, Train Acc: 84.61%, Test Loss: 0.0358, Test Acc: 84.65%\n",
      "Epoch 42: Train Loss: 0.0356, Train Acc: 84.65%, Test Loss: 0.0387, Test Acc: 84.52%\n",
      "Epoch 43: Train Loss: 0.0357, Train Acc: 84.63%, Test Loss: 0.0418, Test Acc: 84.07%\n",
      "Epoch 44: Train Loss: 0.0356, Train Acc: 84.67%, Test Loss: 0.0374, Test Acc: 84.44%\n",
      "Epoch 45: Train Loss: 0.0353, Train Acc: 84.68%, Test Loss: 0.0360, Test Acc: 85.07%\n",
      "Epoch 46: Train Loss: 0.0356, Train Acc: 84.72%, Test Loss: 0.0414, Test Acc: 83.90%\n",
      "Epoch 47: Train Loss: 0.0356, Train Acc: 84.69%, Test Loss: 0.0355, Test Acc: 85.07%\n",
      "Epoch 48: Train Loss: 0.0355, Train Acc: 84.74%, Test Loss: 0.0385, Test Acc: 84.60%\n",
      "Epoch 49: Train Loss: 0.0354, Train Acc: 84.76%, Test Loss: 0.0353, Test Acc: 85.10%\n",
      "Epoch 50: Train Loss: 0.0354, Train Acc: 84.79%, Test Loss: 0.0354, Test Acc: 85.30%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_dynamic_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_dynamic_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())\n",
    "rl_model.policy.mlp_extractor.value_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward : Dynamic2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 32)\n",
    "        self.fc2_2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(64, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x1 = 1 - x1\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_dynamic2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.5743e-03, -3.8203e-03,  2.3502e-04,  3.0332e-03, -8.3489e-05,\n",
       "         -1.0153e-03]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0134, Train Acc: 53.44%, Test Loss: 0.0064, Test Acc: 62.23%\n",
      "Epoch 2: Train Loss: 0.0056, Train Acc: 65.52%, Test Loss: 0.0045, Test Acc: 70.04%\n",
      "Epoch 3: Train Loss: 0.0044, Train Acc: 70.84%, Test Loss: 0.0038, Test Acc: 72.97%\n",
      "Epoch 4: Train Loss: 0.0039, Train Acc: 73.56%, Test Loss: 0.0033, Test Acc: 75.83%\n",
      "Epoch 5: Train Loss: 0.0037, Train Acc: 74.73%, Test Loss: 0.0032, Test Acc: 75.93%\n",
      "Epoch 6: Train Loss: 0.0035, Train Acc: 75.32%, Test Loss: 0.0031, Test Acc: 76.58%\n",
      "Epoch 7: Train Loss: 0.0034, Train Acc: 75.74%, Test Loss: 0.0030, Test Acc: 77.09%\n",
      "Epoch 8: Train Loss: 0.0033, Train Acc: 75.78%, Test Loss: 0.0028, Test Acc: 76.58%\n",
      "Epoch 9: Train Loss: 0.0032, Train Acc: 75.93%, Test Loss: 0.0031, Test Acc: 77.78%\n",
      "Epoch 10: Train Loss: 0.0032, Train Acc: 76.18%, Test Loss: 0.0030, Test Acc: 77.70%\n",
      "Epoch 11: Train Loss: 0.0031, Train Acc: 76.15%, Test Loss: 0.0030, Test Acc: 78.52%\n",
      "Epoch 12: Train Loss: 0.0030, Train Acc: 76.40%, Test Loss: 0.0026, Test Acc: 74.49%\n",
      "Epoch 13: Train Loss: 0.0030, Train Acc: 76.29%, Test Loss: 0.0032, Test Acc: 77.27%\n",
      "Epoch 14: Train Loss: 0.0030, Train Acc: 76.47%, Test Loss: 0.0029, Test Acc: 77.87%\n",
      "Epoch 15: Train Loss: 0.0029, Train Acc: 76.63%, Test Loss: 0.0026, Test Acc: 79.91%\n",
      "Epoch 16: Train Loss: 0.0029, Train Acc: 76.59%, Test Loss: 0.0031, Test Acc: 77.23%\n",
      "Epoch 17: Train Loss: 0.0028, Train Acc: 76.60%, Test Loss: 0.0033, Test Acc: 77.18%\n",
      "Epoch 18: Train Loss: 0.0028, Train Acc: 76.62%, Test Loss: 0.0030, Test Acc: 73.90%\n",
      "Epoch 19: Train Loss: 0.0028, Train Acc: 76.75%, Test Loss: 0.0028, Test Acc: 76.62%\n",
      "Epoch 20: Train Loss: 0.0027, Train Acc: 76.77%, Test Loss: 0.0032, Test Acc: 77.21%\n",
      "Epoch 21: Train Loss: 0.0027, Train Acc: 76.87%, Test Loss: 0.0029, Test Acc: 78.56%\n",
      "Epoch 22: Train Loss: 0.0027, Train Acc: 76.60%, Test Loss: 0.0029, Test Acc: 79.50%\n",
      "Epoch 23: Train Loss: 0.0027, Train Acc: 76.75%, Test Loss: 0.0028, Test Acc: 79.79%\n",
      "Epoch 24: Train Loss: 0.0026, Train Acc: 76.79%, Test Loss: 0.0033, Test Acc: 78.69%\n",
      "Epoch 25: Train Loss: 0.0026, Train Acc: 76.85%, Test Loss: 0.0036, Test Acc: 78.43%\n",
      "Epoch 26: Train Loss: 0.0026, Train Acc: 77.07%, Test Loss: 0.0031, Test Acc: 78.55%\n",
      "Epoch 27: Train Loss: 0.0026, Train Acc: 77.13%, Test Loss: 0.0024, Test Acc: 78.47%\n",
      "Epoch 28: Train Loss: 0.0026, Train Acc: 77.03%, Test Loss: 0.0020, Test Acc: 79.42%\n",
      "Epoch 29: Train Loss: 0.0025, Train Acc: 77.07%, Test Loss: 0.0022, Test Acc: 77.17%\n",
      "Epoch 30: Train Loss: 0.0025, Train Acc: 77.08%, Test Loss: 0.0026, Test Acc: 78.26%\n",
      "Epoch 31: Train Loss: 0.0025, Train Acc: 77.07%, Test Loss: 0.0023, Test Acc: 79.93%\n",
      "Epoch 32: Train Loss: 0.0025, Train Acc: 77.26%, Test Loss: 0.0024, Test Acc: 79.16%\n",
      "Epoch 33: Train Loss: 0.0025, Train Acc: 77.21%, Test Loss: 0.0029, Test Acc: 75.55%\n",
      "Epoch 34: Train Loss: 0.0024, Train Acc: 77.28%, Test Loss: 0.0020, Test Acc: 78.60%\n",
      "Epoch 35: Train Loss: 0.0024, Train Acc: 77.23%, Test Loss: 0.0020, Test Acc: 79.99%\n",
      "Epoch 36: Train Loss: 0.0024, Train Acc: 77.21%, Test Loss: 0.0023, Test Acc: 78.67%\n",
      "Epoch 37: Train Loss: 0.0024, Train Acc: 77.21%, Test Loss: 0.0025, Test Acc: 77.96%\n",
      "Epoch 38: Train Loss: 0.0024, Train Acc: 77.37%, Test Loss: 0.0023, Test Acc: 80.09%\n",
      "Epoch 39: Train Loss: 0.0024, Train Acc: 77.21%, Test Loss: 0.0030, Test Acc: 76.45%\n",
      "Epoch 40: Train Loss: 0.0024, Train Acc: 77.06%, Test Loss: 0.0030, Test Acc: 74.07%\n",
      "Epoch 41: Train Loss: 0.0024, Train Acc: 77.12%, Test Loss: 0.0023, Test Acc: 79.70%\n",
      "Epoch 42: Train Loss: 0.0024, Train Acc: 76.99%, Test Loss: 0.0020, Test Acc: 77.34%\n",
      "Epoch 43: Train Loss: 0.0024, Train Acc: 76.97%, Test Loss: 0.0032, Test Acc: 76.10%\n",
      "Epoch 44: Train Loss: 0.0023, Train Acc: 77.08%, Test Loss: 0.0031, Test Acc: 76.64%\n",
      "Epoch 45: Train Loss: 0.0023, Train Acc: 77.16%, Test Loss: 0.0027, Test Acc: 76.64%\n",
      "Epoch 46: Train Loss: 0.0024, Train Acc: 77.03%, Test Loss: 0.0025, Test Acc: 78.37%\n",
      "Epoch 47: Train Loss: 0.0023, Train Acc: 77.15%, Test Loss: 0.0024, Test Acc: 75.78%\n",
      "Epoch 48: Train Loss: 0.0023, Train Acc: 77.01%, Test Loss: 0.0021, Test Acc: 77.78%\n",
      "Epoch 49: Train Loss: 0.0023, Train Acc: 76.96%, Test Loss: 0.0022, Test Acc: 77.80%\n",
      "Epoch 50: Train Loss: 0.0023, Train Acc: 77.08%, Test Loss: 0.0020, Test Acc: 77.83%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_dynamic2_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_dynamic2_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())\n",
    "rl_model.policy.mlp_extractor.value_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_dynamic2'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward : Default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 32)\n",
    "        self.fc2_2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(64, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x1 = 1 - x1\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_default.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_default'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0024,  0.0048,  0.0019, -0.0059,  0.0035, -0.0003]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0031, Train Acc: 78.57%, Test Loss: 0.0017, Test Acc: 80.98%\n",
      "Epoch 2: Train Loss: 0.0015, Train Acc: 81.19%, Test Loss: 0.0015, Test Acc: 79.64%\n",
      "Epoch 3: Train Loss: 0.0013, Train Acc: 81.78%, Test Loss: 0.0014, Test Acc: 80.38%\n",
      "Epoch 4: Train Loss: 0.0012, Train Acc: 82.46%, Test Loss: 0.0012, Test Acc: 81.64%\n",
      "Epoch 5: Train Loss: 0.0011, Train Acc: 83.20%, Test Loss: 0.0011, Test Acc: 81.96%\n",
      "Epoch 6: Train Loss: 0.0009, Train Acc: 83.93%, Test Loss: 0.0009, Test Acc: 83.82%\n",
      "Epoch 7: Train Loss: 0.0008, Train Acc: 84.96%, Test Loss: 0.0008, Test Acc: 84.63%\n",
      "Epoch 8: Train Loss: 0.0007, Train Acc: 85.79%, Test Loss: 0.0007, Test Acc: 85.10%\n",
      "Epoch 9: Train Loss: 0.0006, Train Acc: 86.56%, Test Loss: 0.0006, Test Acc: 85.55%\n",
      "Epoch 10: Train Loss: 0.0005, Train Acc: 87.28%, Test Loss: 0.0006, Test Acc: 85.70%\n",
      "Epoch 11: Train Loss: 0.0005, Train Acc: 87.98%, Test Loss: 0.0005, Test Acc: 86.29%\n",
      "Epoch 12: Train Loss: 0.0004, Train Acc: 88.71%, Test Loss: 0.0005, Test Acc: 86.76%\n",
      "Epoch 13: Train Loss: 0.0004, Train Acc: 89.37%, Test Loss: 0.0004, Test Acc: 87.61%\n",
      "Epoch 14: Train Loss: 0.0003, Train Acc: 89.85%, Test Loss: 0.0004, Test Acc: 88.56%\n",
      "Epoch 15: Train Loss: 0.0003, Train Acc: 90.24%, Test Loss: 0.0004, Test Acc: 89.40%\n",
      "Epoch 16: Train Loss: 0.0003, Train Acc: 90.50%, Test Loss: 0.0004, Test Acc: 89.46%\n",
      "Epoch 17: Train Loss: 0.0003, Train Acc: 90.68%, Test Loss: 0.0003, Test Acc: 89.57%\n",
      "Epoch 18: Train Loss: 0.0003, Train Acc: 90.80%, Test Loss: 0.0003, Test Acc: 89.77%\n",
      "Epoch 19: Train Loss: 0.0003, Train Acc: 90.93%, Test Loss: 0.0003, Test Acc: 90.28%\n",
      "Epoch 20: Train Loss: 0.0003, Train Acc: 91.09%, Test Loss: 0.0003, Test Acc: 92.50%\n",
      "Epoch 21: Train Loss: 0.0003, Train Acc: 91.16%, Test Loss: 0.0003, Test Acc: 90.86%\n",
      "Epoch 22: Train Loss: 0.0003, Train Acc: 91.24%, Test Loss: 0.0003, Test Acc: 90.56%\n",
      "Epoch 23: Train Loss: 0.0003, Train Acc: 91.33%, Test Loss: 0.0003, Test Acc: 91.61%\n",
      "Epoch 24: Train Loss: 0.0003, Train Acc: 91.40%, Test Loss: 0.0003, Test Acc: 91.36%\n",
      "Epoch 25: Train Loss: 0.0003, Train Acc: 91.42%, Test Loss: 0.0003, Test Acc: 91.00%\n",
      "Epoch 26: Train Loss: 0.0003, Train Acc: 91.52%, Test Loss: 0.0003, Test Acc: 89.85%\n",
      "Epoch 27: Train Loss: 0.0003, Train Acc: 91.61%, Test Loss: 0.0003, Test Acc: 90.39%\n",
      "Epoch 28: Train Loss: 0.0003, Train Acc: 91.64%, Test Loss: 0.0003, Test Acc: 91.35%\n",
      "Epoch 29: Train Loss: 0.0003, Train Acc: 91.67%, Test Loss: 0.0003, Test Acc: 90.66%\n",
      "Epoch 30: Train Loss: 0.0003, Train Acc: 91.73%, Test Loss: 0.0003, Test Acc: 91.16%\n",
      "Epoch 31: Train Loss: 0.0003, Train Acc: 91.71%, Test Loss: 0.0003, Test Acc: 91.43%\n",
      "Epoch 32: Train Loss: 0.0002, Train Acc: 91.74%, Test Loss: 0.0003, Test Acc: 90.89%\n",
      "Epoch 33: Train Loss: 0.0002, Train Acc: 91.75%, Test Loss: 0.0003, Test Acc: 90.52%\n",
      "Epoch 34: Train Loss: 0.0002, Train Acc: 91.85%, Test Loss: 0.0003, Test Acc: 91.71%\n",
      "Epoch 35: Train Loss: 0.0002, Train Acc: 91.81%, Test Loss: 0.0003, Test Acc: 91.37%\n",
      "Epoch 36: Train Loss: 0.0002, Train Acc: 91.88%, Test Loss: 0.0002, Test Acc: 91.03%\n",
      "Epoch 37: Train Loss: 0.0002, Train Acc: 91.93%, Test Loss: 0.0003, Test Acc: 91.30%\n",
      "Epoch 38: Train Loss: 0.0002, Train Acc: 91.84%, Test Loss: 0.0003, Test Acc: 91.57%\n",
      "Epoch 39: Train Loss: 0.0002, Train Acc: 91.98%, Test Loss: 0.0003, Test Acc: 90.92%\n",
      "Epoch 40: Train Loss: 0.0002, Train Acc: 92.04%, Test Loss: 0.0002, Test Acc: 90.17%\n",
      "Epoch 41: Train Loss: 0.0002, Train Acc: 91.98%, Test Loss: 0.0002, Test Acc: 91.92%\n",
      "Epoch 42: Train Loss: 0.0002, Train Acc: 91.88%, Test Loss: 0.0002, Test Acc: 90.53%\n",
      "Epoch 43: Train Loss: 0.0002, Train Acc: 92.01%, Test Loss: 0.0002, Test Acc: 91.83%\n",
      "Epoch 44: Train Loss: 0.0002, Train Acc: 91.97%, Test Loss: 0.0002, Test Acc: 91.74%\n",
      "Epoch 45: Train Loss: 0.0002, Train Acc: 91.99%, Test Loss: 0.0003, Test Acc: 92.04%\n",
      "Epoch 46: Train Loss: 0.0002, Train Acc: 92.01%, Test Loss: 0.0002, Test Acc: 91.59%\n",
      "Epoch 47: Train Loss: 0.0002, Train Acc: 92.08%, Test Loss: 0.0002, Test Acc: 92.04%\n",
      "Epoch 48: Train Loss: 0.0002, Train Acc: 92.06%, Test Loss: 0.0002, Test Acc: 92.26%\n",
      "Epoch 49: Train Loss: 0.0002, Train Acc: 92.12%, Test Loss: 0.0002, Test Acc: 91.79%\n",
      "Epoch 50: Train Loss: 0.0002, Train Acc: 92.11%, Test Loss: 0.0002, Test Acc: 91.53%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_default_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_default_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())\n",
    "rl_model.policy.mlp_extractor.value_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_default'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reward : Default2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(15, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 32)\n",
    "        self.fc2_2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(64, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x1 = 1 - x1\n",
    "        x2 = x[:, 10:]\n",
    "\n",
    "        is_allocatable = ((x1[:, ::2] - x2[:, 0].unsqueeze(1)) >= 0) & ((x1[:, 1::2] - x2[:, 1].unsqueeze(1)) >= 0)\n",
    "        is_allocatable = is_allocatable.float()\n",
    "\n",
    "        x1 = torch.cat((x1, is_allocatable), dim=1)\n",
    "\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n",
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[64, 32], vf=[64, 32])],\n",
    "    \n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_default.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_MM_net(\n",
       "    (fc1_1): Linear(in_features=15, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=64, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=32, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=32, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_ut_default2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_MM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_MM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_mm_action_model = PPO_MM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0068, -0.0070,  0.0013,  0.0072, -0.0095, -0.0023]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_mm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_mm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0406, Train Acc: 40.95%, Test Loss: 0.0132, Test Acc: 59.29%\n",
      "Epoch 2: Train Loss: 0.0075, Train Acc: 70.77%, Test Loss: 0.0050, Test Acc: 76.50%\n",
      "Epoch 3: Train Loss: 0.0041, Train Acc: 78.24%, Test Loss: 0.0032, Test Acc: 81.58%\n",
      "Epoch 4: Train Loss: 0.0029, Train Acc: 81.57%, Test Loss: 0.0024, Test Acc: 83.87%\n",
      "Epoch 5: Train Loss: 0.0023, Train Acc: 83.11%, Test Loss: 0.0020, Test Acc: 83.81%\n",
      "Epoch 6: Train Loss: 0.0020, Train Acc: 83.99%, Test Loss: 0.0018, Test Acc: 84.11%\n",
      "Epoch 7: Train Loss: 0.0017, Train Acc: 84.61%, Test Loss: 0.0018, Test Acc: 83.98%\n",
      "Epoch 8: Train Loss: 0.0015, Train Acc: 85.14%, Test Loss: 0.0018, Test Acc: 84.61%\n",
      "Epoch 9: Train Loss: 0.0013, Train Acc: 85.80%, Test Loss: 0.0016, Test Acc: 85.69%\n",
      "Epoch 10: Train Loss: 0.0012, Train Acc: 86.53%, Test Loss: 0.0014, Test Acc: 86.75%\n",
      "Epoch 11: Train Loss: 0.0011, Train Acc: 87.22%, Test Loss: 0.0013, Test Acc: 87.68%\n",
      "Epoch 12: Train Loss: 0.0010, Train Acc: 87.83%, Test Loss: 0.0012, Test Acc: 88.78%\n",
      "Epoch 13: Train Loss: 0.0009, Train Acc: 88.49%, Test Loss: 0.0011, Test Acc: 89.58%\n",
      "Epoch 14: Train Loss: 0.0009, Train Acc: 88.97%, Test Loss: 0.0011, Test Acc: 89.76%\n",
      "Epoch 15: Train Loss: 0.0008, Train Acc: 89.42%, Test Loss: 0.0010, Test Acc: 89.86%\n",
      "Epoch 16: Train Loss: 0.0008, Train Acc: 89.65%, Test Loss: 0.0009, Test Acc: 89.84%\n",
      "Epoch 17: Train Loss: 0.0008, Train Acc: 89.91%, Test Loss: 0.0009, Test Acc: 89.67%\n",
      "Epoch 18: Train Loss: 0.0007, Train Acc: 90.10%, Test Loss: 0.0009, Test Acc: 89.60%\n",
      "Epoch 19: Train Loss: 0.0007, Train Acc: 90.29%, Test Loss: 0.0009, Test Acc: 89.34%\n",
      "Epoch 20: Train Loss: 0.0007, Train Acc: 90.44%, Test Loss: 0.0009, Test Acc: 89.26%\n",
      "Epoch 21: Train Loss: 0.0007, Train Acc: 90.61%, Test Loss: 0.0008, Test Acc: 89.23%\n",
      "Epoch 22: Train Loss: 0.0007, Train Acc: 90.78%, Test Loss: 0.0008, Test Acc: 89.25%\n",
      "Epoch 23: Train Loss: 0.0007, Train Acc: 90.88%, Test Loss: 0.0008, Test Acc: 89.34%\n",
      "Epoch 24: Train Loss: 0.0007, Train Acc: 90.95%, Test Loss: 0.0008, Test Acc: 89.40%\n",
      "Epoch 25: Train Loss: 0.0006, Train Acc: 91.05%, Test Loss: 0.0008, Test Acc: 89.38%\n",
      "Epoch 26: Train Loss: 0.0006, Train Acc: 91.12%, Test Loss: 0.0007, Test Acc: 89.39%\n",
      "Epoch 27: Train Loss: 0.0006, Train Acc: 91.21%, Test Loss: 0.0007, Test Acc: 89.41%\n",
      "Epoch 28: Train Loss: 0.0006, Train Acc: 91.26%, Test Loss: 0.0007, Test Acc: 89.47%\n",
      "Epoch 29: Train Loss: 0.0006, Train Acc: 91.33%, Test Loss: 0.0007, Test Acc: 89.51%\n",
      "Epoch 30: Train Loss: 0.0006, Train Acc: 91.37%, Test Loss: 0.0007, Test Acc: 89.60%\n",
      "Epoch 31: Train Loss: 0.0006, Train Acc: 91.40%, Test Loss: 0.0007, Test Acc: 89.66%\n",
      "Epoch 32: Train Loss: 0.0006, Train Acc: 91.42%, Test Loss: 0.0007, Test Acc: 89.76%\n",
      "Epoch 33: Train Loss: 0.0006, Train Acc: 91.45%, Test Loss: 0.0006, Test Acc: 89.76%\n",
      "Epoch 34: Train Loss: 0.0006, Train Acc: 91.51%, Test Loss: 0.0006, Test Acc: 89.75%\n",
      "Epoch 35: Train Loss: 0.0006, Train Acc: 91.57%, Test Loss: 0.0006, Test Acc: 89.85%\n",
      "Epoch 36: Train Loss: 0.0006, Train Acc: 91.63%, Test Loss: 0.0006, Test Acc: 89.85%\n",
      "Epoch 37: Train Loss: 0.0006, Train Acc: 91.66%, Test Loss: 0.0006, Test Acc: 89.88%\n",
      "Epoch 38: Train Loss: 0.0006, Train Acc: 91.67%, Test Loss: 0.0006, Test Acc: 89.96%\n",
      "Epoch 39: Train Loss: 0.0006, Train Acc: 91.70%, Test Loss: 0.0006, Test Acc: 90.03%\n",
      "Epoch 40: Train Loss: 0.0006, Train Acc: 91.76%, Test Loss: 0.0006, Test Acc: 90.07%\n",
      "Epoch 41: Train Loss: 0.0006, Train Acc: 91.84%, Test Loss: 0.0006, Test Acc: 90.07%\n",
      "Epoch 42: Train Loss: 0.0006, Train Acc: 91.89%, Test Loss: 0.0006, Test Acc: 90.12%\n",
      "Epoch 43: Train Loss: 0.0005, Train Acc: 91.90%, Test Loss: 0.0006, Test Acc: 90.18%\n",
      "Epoch 44: Train Loss: 0.0005, Train Acc: 91.94%, Test Loss: 0.0006, Test Acc: 90.28%\n",
      "Epoch 45: Train Loss: 0.0005, Train Acc: 91.96%, Test Loss: 0.0006, Test Acc: 90.37%\n",
      "Epoch 46: Train Loss: 0.0005, Train Acc: 91.99%, Test Loss: 0.0006, Test Acc: 90.40%\n",
      "Epoch 47: Train Loss: 0.0005, Train Acc: 92.02%, Test Loss: 0.0006, Test Acc: 90.53%\n",
      "Epoch 48: Train Loss: 0.0005, Train Acc: 92.05%, Test Loss: 0.0006, Test Acc: 90.53%\n",
      "Epoch 49: Train Loss: 0.0005, Train Acc: 92.08%, Test Loss: 0.0006, Test Acc: 90.59%\n",
      "Epoch 50: Train Loss: 0.0005, Train Acc: 92.12%, Test Loss: 0.0006, Test Acc: 90.63%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_mm_action_model, train_default2_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_mm_action_model, test_default2_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_mm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_mm_action_model.action_net.state_dict())\n",
    "rl_model.policy.mlp_extractor.value_net.load_state_dict(ppo_mm_action_model.policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_mm_pr_default2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, 64)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)     # (batch_size, 16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_SM_net(\n",
       "    (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=32, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_SM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_SM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_sm_action_model = PPO_SM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_sm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1135, Train Acc: 72.45%, Test Loss: 0.0739, Test Acc: 78.37%\n",
      "Epoch 2: Train Loss: 0.0673, Train Acc: 79.63%, Test Loss: 0.0590, Test Acc: 80.87%\n",
      "Epoch 3: Train Loss: 0.0604, Train Acc: 80.79%, Test Loss: 0.0546, Test Acc: 82.42%\n",
      "Epoch 4: Train Loss: 0.0569, Train Acc: 81.36%, Test Loss: 0.0507, Test Acc: 83.22%\n",
      "Epoch 5: Train Loss: 0.0543, Train Acc: 81.86%, Test Loss: 0.0516, Test Acc: 83.17%\n",
      "Epoch 6: Train Loss: 0.0524, Train Acc: 82.22%, Test Loss: 0.0481, Test Acc: 83.45%\n",
      "Epoch 7: Train Loss: 0.0509, Train Acc: 82.46%, Test Loss: 0.0474, Test Acc: 83.20%\n",
      "Epoch 8: Train Loss: 0.0495, Train Acc: 82.74%, Test Loss: 0.0453, Test Acc: 83.70%\n",
      "Epoch 9: Train Loss: 0.0485, Train Acc: 82.84%, Test Loss: 0.0473, Test Acc: 83.28%\n",
      "Epoch 10: Train Loss: 0.0477, Train Acc: 82.96%, Test Loss: 0.0464, Test Acc: 83.35%\n",
      "Epoch 11: Train Loss: 0.0469, Train Acc: 83.06%, Test Loss: 0.0469, Test Acc: 83.19%\n",
      "Epoch 12: Train Loss: 0.0461, Train Acc: 83.23%, Test Loss: 0.0498, Test Acc: 82.67%\n",
      "Epoch 13: Train Loss: 0.0450, Train Acc: 83.38%, Test Loss: 0.0451, Test Acc: 83.54%\n",
      "Epoch 14: Train Loss: 0.0446, Train Acc: 83.43%, Test Loss: 0.0436, Test Acc: 83.85%\n",
      "Epoch 15: Train Loss: 0.0441, Train Acc: 83.50%, Test Loss: 0.0444, Test Acc: 83.66%\n",
      "Epoch 16: Train Loss: 0.0432, Train Acc: 83.70%, Test Loss: 0.0494, Test Acc: 83.21%\n",
      "Epoch 17: Train Loss: 0.0428, Train Acc: 83.78%, Test Loss: 0.0430, Test Acc: 83.88%\n",
      "Epoch 18: Train Loss: 0.0423, Train Acc: 83.83%, Test Loss: 0.0565, Test Acc: 82.26%\n",
      "Epoch 19: Train Loss: 0.0419, Train Acc: 83.99%, Test Loss: 0.0497, Test Acc: 83.29%\n",
      "Epoch 20: Train Loss: 0.0413, Train Acc: 84.10%, Test Loss: 0.0465, Test Acc: 83.89%\n",
      "Epoch 21: Train Loss: 0.0413, Train Acc: 84.15%, Test Loss: 0.0451, Test Acc: 83.91%\n",
      "Epoch 22: Train Loss: 0.0409, Train Acc: 84.23%, Test Loss: 0.0478, Test Acc: 83.22%\n",
      "Epoch 23: Train Loss: 0.0407, Train Acc: 84.29%, Test Loss: 0.0428, Test Acc: 84.05%\n",
      "Epoch 24: Train Loss: 0.0402, Train Acc: 84.34%, Test Loss: 0.0410, Test Acc: 84.04%\n",
      "Epoch 25: Train Loss: 0.0398, Train Acc: 84.45%, Test Loss: 0.0401, Test Acc: 85.06%\n",
      "Epoch 26: Train Loss: 0.0398, Train Acc: 84.48%, Test Loss: 0.0434, Test Acc: 84.22%\n",
      "Epoch 27: Train Loss: 0.0393, Train Acc: 84.56%, Test Loss: 0.0469, Test Acc: 83.92%\n",
      "Epoch 28: Train Loss: 0.0397, Train Acc: 84.51%, Test Loss: 0.0387, Test Acc: 85.00%\n",
      "Epoch 29: Train Loss: 0.0391, Train Acc: 84.62%, Test Loss: 0.0387, Test Acc: 85.11%\n",
      "Epoch 30: Train Loss: 0.0388, Train Acc: 84.67%, Test Loss: 0.0448, Test Acc: 84.25%\n",
      "Epoch 31: Train Loss: 0.0387, Train Acc: 84.68%, Test Loss: 0.0378, Test Acc: 85.28%\n",
      "Epoch 32: Train Loss: 0.0387, Train Acc: 84.72%, Test Loss: 0.0422, Test Acc: 84.14%\n",
      "Epoch 33: Train Loss: 0.0383, Train Acc: 84.81%, Test Loss: 0.0438, Test Acc: 84.10%\n",
      "Epoch 34: Train Loss: 0.0383, Train Acc: 84.85%, Test Loss: 0.0398, Test Acc: 84.63%\n",
      "Epoch 35: Train Loss: 0.0382, Train Acc: 84.83%, Test Loss: 0.0390, Test Acc: 85.02%\n",
      "Epoch 36: Train Loss: 0.0381, Train Acc: 84.88%, Test Loss: 0.0394, Test Acc: 84.92%\n",
      "Epoch 37: Train Loss: 0.0378, Train Acc: 85.00%, Test Loss: 0.0463, Test Acc: 83.68%\n",
      "Epoch 38: Train Loss: 0.0376, Train Acc: 85.05%, Test Loss: 0.0443, Test Acc: 84.00%\n",
      "Epoch 39: Train Loss: 0.0375, Train Acc: 85.07%, Test Loss: 0.0394, Test Acc: 84.82%\n",
      "Epoch 40: Train Loss: 0.0376, Train Acc: 85.01%, Test Loss: 0.0374, Test Acc: 85.18%\n",
      "Epoch 41: Train Loss: 0.0375, Train Acc: 85.08%, Test Loss: 0.0473, Test Acc: 83.73%\n",
      "Epoch 42: Train Loss: 0.0371, Train Acc: 85.14%, Test Loss: 0.0397, Test Acc: 84.76%\n",
      "Epoch 43: Train Loss: 0.0371, Train Acc: 85.17%, Test Loss: 0.0421, Test Acc: 84.06%\n",
      "Epoch 44: Train Loss: 0.0370, Train Acc: 85.11%, Test Loss: 0.0432, Test Acc: 83.97%\n",
      "Epoch 45: Train Loss: 0.0368, Train Acc: 85.20%, Test Loss: 0.0397, Test Acc: 84.89%\n",
      "Epoch 46: Train Loss: 0.0368, Train Acc: 85.23%, Test Loss: 0.0500, Test Acc: 83.72%\n",
      "Epoch 47: Train Loss: 0.0370, Train Acc: 85.18%, Test Loss: 0.0430, Test Acc: 84.45%\n",
      "Epoch 48: Train Loss: 0.0369, Train Acc: 85.19%, Test Loss: 0.0375, Test Acc: 85.49%\n",
      "Epoch 49: Train Loss: 0.0365, Train Acc: 85.28%, Test Loss: 0.0407, Test Acc: 84.72%\n",
      "Epoch 50: Train Loss: 0.0367, Train Acc: 85.27%, Test Loss: 0.0363, Test Acc: 85.07%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_sm_action_model, train_dynamic_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_sm_action_model, test_dynamic_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_sm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_sm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_sm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Single-modal Default2 (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, 64)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)     # (batch_size, 16)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_default2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_SM_net(\n",
       "    (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "    (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "    (fc3): Linear(in_features=32, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save untrained model\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_ut_default2'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_SM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_SM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_sm_action_model = PPO_SM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_sm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1135, Train Acc: 72.45%, Test Loss: 0.0739, Test Acc: 78.37%\n",
      "Epoch 2: Train Loss: 0.0673, Train Acc: 79.63%, Test Loss: 0.0590, Test Acc: 80.87%\n",
      "Epoch 3: Train Loss: 0.0604, Train Acc: 80.79%, Test Loss: 0.0546, Test Acc: 82.42%\n",
      "Epoch 4: Train Loss: 0.0569, Train Acc: 81.36%, Test Loss: 0.0507, Test Acc: 83.22%\n",
      "Epoch 5: Train Loss: 0.0543, Train Acc: 81.86%, Test Loss: 0.0516, Test Acc: 83.17%\n",
      "Epoch 6: Train Loss: 0.0524, Train Acc: 82.22%, Test Loss: 0.0481, Test Acc: 83.45%\n",
      "Epoch 7: Train Loss: 0.0509, Train Acc: 82.46%, Test Loss: 0.0474, Test Acc: 83.20%\n",
      "Epoch 8: Train Loss: 0.0495, Train Acc: 82.74%, Test Loss: 0.0453, Test Acc: 83.70%\n",
      "Epoch 9: Train Loss: 0.0485, Train Acc: 82.84%, Test Loss: 0.0473, Test Acc: 83.28%\n",
      "Epoch 10: Train Loss: 0.0477, Train Acc: 82.96%, Test Loss: 0.0464, Test Acc: 83.35%\n",
      "Epoch 11: Train Loss: 0.0469, Train Acc: 83.06%, Test Loss: 0.0469, Test Acc: 83.19%\n",
      "Epoch 12: Train Loss: 0.0461, Train Acc: 83.23%, Test Loss: 0.0498, Test Acc: 82.67%\n",
      "Epoch 13: Train Loss: 0.0450, Train Acc: 83.38%, Test Loss: 0.0451, Test Acc: 83.54%\n",
      "Epoch 14: Train Loss: 0.0446, Train Acc: 83.43%, Test Loss: 0.0436, Test Acc: 83.85%\n",
      "Epoch 15: Train Loss: 0.0441, Train Acc: 83.50%, Test Loss: 0.0444, Test Acc: 83.66%\n",
      "Epoch 16: Train Loss: 0.0432, Train Acc: 83.70%, Test Loss: 0.0494, Test Acc: 83.21%\n",
      "Epoch 17: Train Loss: 0.0428, Train Acc: 83.78%, Test Loss: 0.0430, Test Acc: 83.88%\n",
      "Epoch 18: Train Loss: 0.0423, Train Acc: 83.83%, Test Loss: 0.0565, Test Acc: 82.26%\n",
      "Epoch 19: Train Loss: 0.0419, Train Acc: 83.99%, Test Loss: 0.0497, Test Acc: 83.29%\n",
      "Epoch 20: Train Loss: 0.0413, Train Acc: 84.10%, Test Loss: 0.0465, Test Acc: 83.89%\n",
      "Epoch 21: Train Loss: 0.0413, Train Acc: 84.15%, Test Loss: 0.0451, Test Acc: 83.91%\n",
      "Epoch 22: Train Loss: 0.0409, Train Acc: 84.23%, Test Loss: 0.0478, Test Acc: 83.22%\n",
      "Epoch 23: Train Loss: 0.0407, Train Acc: 84.29%, Test Loss: 0.0428, Test Acc: 84.05%\n",
      "Epoch 24: Train Loss: 0.0402, Train Acc: 84.34%, Test Loss: 0.0410, Test Acc: 84.04%\n",
      "Epoch 25: Train Loss: 0.0398, Train Acc: 84.45%, Test Loss: 0.0401, Test Acc: 85.06%\n",
      "Epoch 26: Train Loss: 0.0398, Train Acc: 84.48%, Test Loss: 0.0434, Test Acc: 84.22%\n",
      "Epoch 27: Train Loss: 0.0393, Train Acc: 84.56%, Test Loss: 0.0469, Test Acc: 83.92%\n",
      "Epoch 28: Train Loss: 0.0397, Train Acc: 84.51%, Test Loss: 0.0387, Test Acc: 85.00%\n",
      "Epoch 29: Train Loss: 0.0391, Train Acc: 84.62%, Test Loss: 0.0387, Test Acc: 85.11%\n",
      "Epoch 30: Train Loss: 0.0388, Train Acc: 84.67%, Test Loss: 0.0448, Test Acc: 84.25%\n",
      "Epoch 31: Train Loss: 0.0387, Train Acc: 84.68%, Test Loss: 0.0378, Test Acc: 85.28%\n",
      "Epoch 32: Train Loss: 0.0387, Train Acc: 84.72%, Test Loss: 0.0422, Test Acc: 84.14%\n",
      "Epoch 33: Train Loss: 0.0383, Train Acc: 84.81%, Test Loss: 0.0438, Test Acc: 84.10%\n",
      "Epoch 34: Train Loss: 0.0383, Train Acc: 84.85%, Test Loss: 0.0398, Test Acc: 84.63%\n",
      "Epoch 35: Train Loss: 0.0382, Train Acc: 84.83%, Test Loss: 0.0390, Test Acc: 85.02%\n",
      "Epoch 36: Train Loss: 0.0381, Train Acc: 84.88%, Test Loss: 0.0394, Test Acc: 84.92%\n",
      "Epoch 37: Train Loss: 0.0378, Train Acc: 85.00%, Test Loss: 0.0463, Test Acc: 83.68%\n",
      "Epoch 38: Train Loss: 0.0376, Train Acc: 85.05%, Test Loss: 0.0443, Test Acc: 84.00%\n",
      "Epoch 39: Train Loss: 0.0375, Train Acc: 85.07%, Test Loss: 0.0394, Test Acc: 84.82%\n",
      "Epoch 40: Train Loss: 0.0376, Train Acc: 85.01%, Test Loss: 0.0374, Test Acc: 85.18%\n",
      "Epoch 41: Train Loss: 0.0375, Train Acc: 85.08%, Test Loss: 0.0473, Test Acc: 83.73%\n",
      "Epoch 42: Train Loss: 0.0371, Train Acc: 85.14%, Test Loss: 0.0397, Test Acc: 84.76%\n",
      "Epoch 43: Train Loss: 0.0371, Train Acc: 85.17%, Test Loss: 0.0421, Test Acc: 84.06%\n",
      "Epoch 44: Train Loss: 0.0370, Train Acc: 85.11%, Test Loss: 0.0432, Test Acc: 83.97%\n",
      "Epoch 45: Train Loss: 0.0368, Train Acc: 85.20%, Test Loss: 0.0397, Test Acc: 84.89%\n",
      "Epoch 46: Train Loss: 0.0368, Train Acc: 85.23%, Test Loss: 0.0500, Test Acc: 83.72%\n",
      "Epoch 47: Train Loss: 0.0370, Train Acc: 85.18%, Test Loss: 0.0430, Test Acc: 84.45%\n",
      "Epoch 48: Train Loss: 0.0369, Train Acc: 85.19%, Test Loss: 0.0375, Test Acc: 85.49%\n",
      "Epoch 49: Train Loss: 0.0365, Train Acc: 85.28%, Test Loss: 0.0407, Test Acc: 84.72%\n",
      "Epoch 50: Train Loss: 0.0367, Train Acc: 85.27%, Test Loss: 0.0363, Test Acc: 85.07%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_sm_action_model, train_default2_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_sm_action_model, test_default2_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_sm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_sm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_sm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Multi-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_MM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_MM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc3 = nn.Linear(16, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x = torch.cat((x1, x2), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_MM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_MM_net(\n",
       "      (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "      (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "      (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "      (fc3): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0019, -0.1223,  0.0332, -0.0266, -0.0474, -0.0251]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_MM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_MM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_mm_model = DQN_MM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_mm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1929, Train Acc: 61.92%, Test Loss: 0.1382, Test Acc: 73.71%\n",
      "Epoch 2: Train Loss: 0.1290, Train Acc: 74.93%, Test Loss: 0.1304, Test Acc: 74.60%\n",
      "Epoch 3: Train Loss: 0.1254, Train Acc: 75.37%, Test Loss: 0.1292, Test Acc: 74.84%\n",
      "Epoch 4: Train Loss: 0.1240, Train Acc: 75.56%, Test Loss: 0.1261, Test Acc: 75.12%\n",
      "Epoch 5: Train Loss: 0.1226, Train Acc: 75.73%, Test Loss: 0.1239, Test Acc: 75.25%\n",
      "Epoch 6: Train Loss: 0.1212, Train Acc: 75.81%, Test Loss: 0.1223, Test Acc: 75.21%\n",
      "Epoch 7: Train Loss: 0.1200, Train Acc: 75.88%, Test Loss: 0.1216, Test Acc: 75.15%\n",
      "Epoch 8: Train Loss: 0.1191, Train Acc: 75.99%, Test Loss: 0.1207, Test Acc: 75.11%\n",
      "Epoch 9: Train Loss: 0.1184, Train Acc: 76.04%, Test Loss: 0.1200, Test Acc: 75.21%\n",
      "Epoch 10: Train Loss: 0.1179, Train Acc: 76.13%, Test Loss: 0.1190, Test Acc: 75.27%\n",
      "Epoch 11: Train Loss: 0.1175, Train Acc: 76.18%, Test Loss: 0.1189, Test Acc: 75.39%\n",
      "Epoch 12: Train Loss: 0.1172, Train Acc: 76.22%, Test Loss: 0.1182, Test Acc: 75.43%\n",
      "Epoch 13: Train Loss: 0.1169, Train Acc: 76.23%, Test Loss: 0.1182, Test Acc: 75.36%\n",
      "Epoch 14: Train Loss: 0.1167, Train Acc: 76.28%, Test Loss: 0.1177, Test Acc: 75.58%\n",
      "Epoch 15: Train Loss: 0.1165, Train Acc: 76.30%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 16: Train Loss: 0.1163, Train Acc: 76.34%, Test Loss: 0.1167, Test Acc: 75.63%\n",
      "Epoch 17: Train Loss: 0.1161, Train Acc: 76.35%, Test Loss: 0.1168, Test Acc: 75.69%\n",
      "Epoch 18: Train Loss: 0.1160, Train Acc: 76.33%, Test Loss: 0.1167, Test Acc: 75.55%\n",
      "Epoch 19: Train Loss: 0.1159, Train Acc: 76.36%, Test Loss: 0.1167, Test Acc: 75.64%\n",
      "Epoch 20: Train Loss: 0.1158, Train Acc: 76.33%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 21: Train Loss: 0.1156, Train Acc: 76.35%, Test Loss: 0.1166, Test Acc: 75.57%\n",
      "Epoch 22: Train Loss: 0.1152, Train Acc: 76.41%, Test Loss: 0.1150, Test Acc: 75.92%\n",
      "Epoch 23: Train Loss: 0.1128, Train Acc: 76.72%, Test Loss: 0.1135, Test Acc: 76.14%\n",
      "Epoch 24: Train Loss: 0.1115, Train Acc: 76.92%, Test Loss: 0.1135, Test Acc: 76.12%\n",
      "Epoch 25: Train Loss: 0.1112, Train Acc: 76.98%, Test Loss: 0.1133, Test Acc: 76.23%\n",
      "Epoch 26: Train Loss: 0.1108, Train Acc: 77.00%, Test Loss: 0.1127, Test Acc: 76.39%\n",
      "Epoch 27: Train Loss: 0.1105, Train Acc: 77.07%, Test Loss: 0.1120, Test Acc: 76.35%\n",
      "Epoch 28: Train Loss: 0.1103, Train Acc: 77.07%, Test Loss: 0.1109, Test Acc: 76.57%\n",
      "Epoch 29: Train Loss: 0.1100, Train Acc: 77.09%, Test Loss: 0.1109, Test Acc: 76.51%\n",
      "Epoch 30: Train Loss: 0.1097, Train Acc: 77.11%, Test Loss: 0.1108, Test Acc: 76.45%\n",
      "Epoch 31: Train Loss: 0.1094, Train Acc: 77.10%, Test Loss: 0.1096, Test Acc: 76.66%\n",
      "Epoch 32: Train Loss: 0.1092, Train Acc: 77.12%, Test Loss: 0.1093, Test Acc: 76.56%\n",
      "Epoch 33: Train Loss: 0.1066, Train Acc: 77.18%, Test Loss: 0.1025, Test Acc: 77.42%\n",
      "Epoch 34: Train Loss: 0.1005, Train Acc: 77.90%, Test Loss: 0.0993, Test Acc: 77.65%\n",
      "Epoch 35: Train Loss: 0.0979, Train Acc: 78.25%, Test Loss: 0.0961, Test Acc: 78.25%\n",
      "Epoch 36: Train Loss: 0.0934, Train Acc: 78.84%, Test Loss: 0.0913, Test Acc: 78.82%\n",
      "Epoch 37: Train Loss: 0.0862, Train Acc: 79.80%, Test Loss: 0.0841, Test Acc: 79.70%\n",
      "Epoch 38: Train Loss: 0.0812, Train Acc: 80.52%, Test Loss: 0.0805, Test Acc: 80.22%\n",
      "Epoch 39: Train Loss: 0.0791, Train Acc: 80.73%, Test Loss: 0.0779, Test Acc: 80.58%\n",
      "Epoch 40: Train Loss: 0.0778, Train Acc: 80.86%, Test Loss: 0.0775, Test Acc: 80.64%\n",
      "Epoch 41: Train Loss: 0.0767, Train Acc: 80.97%, Test Loss: 0.0758, Test Acc: 81.15%\n",
      "Epoch 42: Train Loss: 0.0757, Train Acc: 81.09%, Test Loss: 0.0743, Test Acc: 81.30%\n",
      "Epoch 43: Train Loss: 0.0747, Train Acc: 81.21%, Test Loss: 0.0731, Test Acc: 81.43%\n",
      "Epoch 44: Train Loss: 0.0739, Train Acc: 81.34%, Test Loss: 0.0726, Test Acc: 81.61%\n",
      "Epoch 45: Train Loss: 0.0733, Train Acc: 81.46%, Test Loss: 0.0722, Test Acc: 81.69%\n",
      "Epoch 46: Train Loss: 0.0726, Train Acc: 81.58%, Test Loss: 0.0721, Test Acc: 81.74%\n",
      "Epoch 47: Train Loss: 0.0719, Train Acc: 81.67%, Test Loss: 0.0715, Test Acc: 81.75%\n",
      "Epoch 48: Train Loss: 0.0711, Train Acc: 81.73%, Test Loss: 0.0695, Test Acc: 81.96%\n",
      "Epoch 49: Train Loss: 0.0704, Train Acc: 81.79%, Test Loss: 0.0718, Test Acc: 81.53%\n",
      "Epoch 50: Train Loss: 0.0698, Train Acc: 81.80%, Test Loss: 0.0701, Test Acc: 81.77%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_mm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_mm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_mm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_mm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_mm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5014, -0.9858, -1.0000, -1.0103, -1.0104, -0.9910]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5059, -0.9256, -0.9021, -1.0459, -1.0538,  0.6572]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net(sample2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Single-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_SM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_SM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(12, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc2 = nn.Linear(16, 32)\n",
    "        self.fc3 = nn.Linear(32, 64)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_SM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_SM_net(\n",
       "      (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (fc3): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_SM_net(\n",
       "      (fc1): Linear(in_features=12, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (fc3): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=80, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0595, -0.0151,  0.1065,  0.0312, -0.1239, -0.0574]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_SM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_SM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_sm_model = DQN_SM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_sm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.1545, Train Acc: 64.78%, Test Loss: 0.1164, Test Acc: 69.93%\n",
      "Epoch 2: Train Loss: 0.1101, Train Acc: 71.31%, Test Loss: 0.1038, Test Acc: 71.87%\n",
      "Epoch 3: Train Loss: 0.0932, Train Acc: 74.02%, Test Loss: 0.0791, Test Acc: 76.44%\n",
      "Epoch 4: Train Loss: 0.0737, Train Acc: 77.49%, Test Loss: 0.0699, Test Acc: 78.05%\n",
      "Epoch 5: Train Loss: 0.0659, Train Acc: 78.82%, Test Loss: 0.0622, Test Acc: 79.54%\n",
      "Epoch 6: Train Loss: 0.0617, Train Acc: 79.77%, Test Loss: 0.0558, Test Acc: 80.99%\n",
      "Epoch 7: Train Loss: 0.0563, Train Acc: 80.84%, Test Loss: 0.0500, Test Acc: 82.54%\n",
      "Epoch 8: Train Loss: 0.0530, Train Acc: 81.54%, Test Loss: 0.0462, Test Acc: 83.23%\n",
      "Epoch 9: Train Loss: 0.0511, Train Acc: 81.86%, Test Loss: 0.0494, Test Acc: 82.63%\n",
      "Epoch 10: Train Loss: 0.0498, Train Acc: 82.07%, Test Loss: 0.0526, Test Acc: 81.94%\n",
      "Epoch 11: Train Loss: 0.0486, Train Acc: 82.37%, Test Loss: 0.0501, Test Acc: 82.80%\n",
      "Epoch 12: Train Loss: 0.0476, Train Acc: 82.47%, Test Loss: 0.0478, Test Acc: 82.89%\n",
      "Epoch 13: Train Loss: 0.0470, Train Acc: 82.67%, Test Loss: 0.0450, Test Acc: 83.29%\n",
      "Epoch 14: Train Loss: 0.0463, Train Acc: 82.79%, Test Loss: 0.0472, Test Acc: 83.16%\n",
      "Epoch 15: Train Loss: 0.0461, Train Acc: 82.87%, Test Loss: 0.0485, Test Acc: 82.56%\n",
      "Epoch 16: Train Loss: 0.0459, Train Acc: 82.94%, Test Loss: 0.0473, Test Acc: 82.65%\n",
      "Epoch 17: Train Loss: 0.0454, Train Acc: 83.01%, Test Loss: 0.0423, Test Acc: 83.76%\n",
      "Epoch 18: Train Loss: 0.0448, Train Acc: 83.11%, Test Loss: 0.0441, Test Acc: 83.32%\n",
      "Epoch 19: Train Loss: 0.0447, Train Acc: 83.24%, Test Loss: 0.0484, Test Acc: 83.10%\n",
      "Epoch 20: Train Loss: 0.0443, Train Acc: 83.29%, Test Loss: 0.0426, Test Acc: 83.72%\n",
      "Epoch 21: Train Loss: 0.0438, Train Acc: 83.37%, Test Loss: 0.0416, Test Acc: 83.97%\n",
      "Epoch 22: Train Loss: 0.0436, Train Acc: 83.39%, Test Loss: 0.0433, Test Acc: 83.51%\n",
      "Epoch 23: Train Loss: 0.0433, Train Acc: 83.50%, Test Loss: 0.0473, Test Acc: 83.31%\n",
      "Epoch 24: Train Loss: 0.0429, Train Acc: 83.60%, Test Loss: 0.0411, Test Acc: 84.30%\n",
      "Epoch 25: Train Loss: 0.0424, Train Acc: 83.64%, Test Loss: 0.0431, Test Acc: 83.74%\n",
      "Epoch 26: Train Loss: 0.0422, Train Acc: 83.75%, Test Loss: 0.0432, Test Acc: 84.11%\n",
      "Epoch 27: Train Loss: 0.0416, Train Acc: 83.87%, Test Loss: 0.0437, Test Acc: 83.85%\n",
      "Epoch 28: Train Loss: 0.0411, Train Acc: 83.97%, Test Loss: 0.0502, Test Acc: 83.07%\n",
      "Epoch 29: Train Loss: 0.0410, Train Acc: 84.08%, Test Loss: 0.0407, Test Acc: 84.09%\n",
      "Epoch 30: Train Loss: 0.0408, Train Acc: 84.13%, Test Loss: 0.0386, Test Acc: 85.15%\n",
      "Epoch 31: Train Loss: 0.0404, Train Acc: 84.26%, Test Loss: 0.0467, Test Acc: 83.80%\n",
      "Epoch 32: Train Loss: 0.0401, Train Acc: 84.32%, Test Loss: 0.0420, Test Acc: 84.46%\n",
      "Epoch 33: Train Loss: 0.0397, Train Acc: 84.43%, Test Loss: 0.0492, Test Acc: 83.54%\n",
      "Epoch 34: Train Loss: 0.0395, Train Acc: 84.58%, Test Loss: 0.0411, Test Acc: 84.78%\n",
      "Epoch 35: Train Loss: 0.0394, Train Acc: 84.66%, Test Loss: 0.0461, Test Acc: 83.97%\n",
      "Epoch 36: Train Loss: 0.0391, Train Acc: 84.72%, Test Loss: 0.0383, Test Acc: 85.42%\n",
      "Epoch 37: Train Loss: 0.0391, Train Acc: 84.76%, Test Loss: 0.0351, Test Acc: 85.71%\n",
      "Epoch 38: Train Loss: 0.0387, Train Acc: 84.89%, Test Loss: 0.0350, Test Acc: 85.98%\n",
      "Epoch 39: Train Loss: 0.0384, Train Acc: 84.96%, Test Loss: 0.0420, Test Acc: 85.02%\n",
      "Epoch 40: Train Loss: 0.0382, Train Acc: 85.02%, Test Loss: 0.0379, Test Acc: 85.61%\n",
      "Epoch 41: Train Loss: 0.0379, Train Acc: 85.07%, Test Loss: 0.0380, Test Acc: 85.26%\n",
      "Epoch 42: Train Loss: 0.0380, Train Acc: 85.05%, Test Loss: 0.0485, Test Acc: 83.73%\n",
      "Epoch 43: Train Loss: 0.0378, Train Acc: 85.12%, Test Loss: 0.0404, Test Acc: 85.10%\n",
      "Epoch 44: Train Loss: 0.0380, Train Acc: 85.07%, Test Loss: 0.0374, Test Acc: 85.36%\n",
      "Epoch 45: Train Loss: 0.0374, Train Acc: 85.22%, Test Loss: 0.0397, Test Acc: 84.98%\n",
      "Epoch 46: Train Loss: 0.0376, Train Acc: 85.20%, Test Loss: 0.0375, Test Acc: 85.54%\n",
      "Epoch 47: Train Loss: 0.0372, Train Acc: 85.26%, Test Loss: 0.0373, Test Acc: 85.71%\n",
      "Epoch 48: Train Loss: 0.0371, Train Acc: 85.35%, Test Loss: 0.0368, Test Acc: 85.38%\n",
      "Epoch 49: Train Loss: 0.0369, Train Acc: 85.31%, Test Loss: 0.0401, Test Acc: 84.99%\n",
      "Epoch 50: Train Loss: 0.0367, Train Acc: 85.36%, Test Loss: 0.0380, Test Acc: 85.33%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_sm_model, train_dynamic_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_sm_model, test_dynamic_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_sm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_sm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_sm_pr_dynamic'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Tripple-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dynamic Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_TM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_TM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc1_3 = nn.Linear(5, 16)  # Node difference (CPU, Memory)\n",
    "        # self.fc1_4 = nn.Linear(5, 16) # If the node can deploy the pod\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc2_3 = nn.Linear(16, 8)\n",
    "        # self.fc2_4 = nn.Linear(16, 8)\n",
    "\n",
    "        self.fc3 = nn.Linear(24, 16)\n",
    "        # self.fc3 = nn.Linear(32, 16)    # Concatenated vector\n",
    "        self.fc4 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x3 = x1[:, ::2] - x1[:, 1::2] # Takes the difference of x1's odd and even columns\n",
    "\n",
    "        # # Duplicte x2 5 times horizontally\n",
    "        # x2_ = x2.repeat(1, 5).view(-1, 10)\n",
    "        # x4_ = (1-x1) - x2_ >= 0\n",
    "        # x4 = x4_[:, ::2] * x4_[:, 1::2]\n",
    "        # # Convert boolean to float\n",
    "        # x4 = x4.type(torch.FloatTensor)\n",
    "        # # print(f\"x4 : {x4}\")\n",
    "\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x3 = F.relu(self.fc1_3(x3))\n",
    "        # x4 = F.relu(self.fc1_4(x4))\n",
    "\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x3 = F.relu(self.fc2_3(x3))\n",
    "        # x4 = F.relu(self.fc2_4(x4))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        # x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_TM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_TM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=24, out_features=16, bias=True)\n",
       "    (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (policy_net): Sequential()\n",
       "    (value_net): Sequential()\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_tm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0567, -0.0286, -0.0869, -0.0466, -0.0994, -0.0755]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_TM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_TM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tm_model = DQN_TM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Predict 6 vectors (6 actions' scores)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_tm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0233, Train Acc: 59.48%, Test Loss: 0.0135, Test Acc: 69.50%\n",
      "Epoch 2: Train Loss: 0.0136, Train Acc: 69.64%, Test Loss: 0.0126, Test Acc: 71.00%\n",
      "Epoch 3: Train Loss: 0.0115, Train Acc: 72.31%, Test Loss: 0.0093, Test Acc: 74.22%\n",
      "Epoch 4: Train Loss: 0.0082, Train Acc: 76.54%, Test Loss: 0.0068, Test Acc: 78.27%\n",
      "Epoch 5: Train Loss: 0.0061, Train Acc: 78.97%, Test Loss: 0.0052, Test Acc: 79.49%\n",
      "Epoch 6: Train Loss: 0.0053, Train Acc: 80.24%, Test Loss: 0.0045, Test Acc: 81.49%\n",
      "Epoch 7: Train Loss: 0.0051, Train Acc: 80.89%, Test Loss: 0.0050, Test Acc: 80.77%\n",
      "Epoch 8: Train Loss: 0.0049, Train Acc: 81.43%, Test Loss: 0.0051, Test Acc: 81.64%\n",
      "Epoch 9: Train Loss: 0.0048, Train Acc: 81.90%, Test Loss: 0.0053, Test Acc: 81.67%\n",
      "Epoch 10: Train Loss: 0.0047, Train Acc: 82.21%, Test Loss: 0.0044, Test Acc: 82.39%\n",
      "Epoch 11: Train Loss: 0.0046, Train Acc: 82.51%, Test Loss: 0.0044, Test Acc: 82.67%\n",
      "Epoch 12: Train Loss: 0.0045, Train Acc: 82.83%, Test Loss: 0.0044, Test Acc: 83.53%\n",
      "Epoch 13: Train Loss: 0.0044, Train Acc: 83.10%, Test Loss: 0.0046, Test Acc: 83.33%\n",
      "Epoch 14: Train Loss: 0.0044, Train Acc: 83.26%, Test Loss: 0.0039, Test Acc: 83.51%\n",
      "Epoch 15: Train Loss: 0.0043, Train Acc: 83.40%, Test Loss: 0.0044, Test Acc: 83.43%\n",
      "Epoch 16: Train Loss: 0.0042, Train Acc: 83.50%, Test Loss: 0.0043, Test Acc: 84.16%\n",
      "Epoch 17: Train Loss: 0.0042, Train Acc: 83.60%, Test Loss: 0.0038, Test Acc: 83.70%\n",
      "Epoch 18: Train Loss: 0.0042, Train Acc: 83.69%, Test Loss: 0.0038, Test Acc: 84.19%\n",
      "Epoch 19: Train Loss: 0.0041, Train Acc: 83.76%, Test Loss: 0.0047, Test Acc: 81.87%\n",
      "Epoch 20: Train Loss: 0.0041, Train Acc: 83.80%, Test Loss: 0.0041, Test Acc: 83.58%\n",
      "Epoch 21: Train Loss: 0.0041, Train Acc: 83.89%, Test Loss: 0.0041, Test Acc: 83.99%\n",
      "Epoch 22: Train Loss: 0.0040, Train Acc: 83.96%, Test Loss: 0.0037, Test Acc: 83.98%\n",
      "Epoch 23: Train Loss: 0.0040, Train Acc: 84.00%, Test Loss: 0.0039, Test Acc: 84.11%\n",
      "Epoch 24: Train Loss: 0.0040, Train Acc: 84.10%, Test Loss: 0.0037, Test Acc: 84.70%\n",
      "Epoch 25: Train Loss: 0.0039, Train Acc: 84.19%, Test Loss: 0.0041, Test Acc: 84.26%\n",
      "Epoch 26: Train Loss: 0.0039, Train Acc: 84.20%, Test Loss: 0.0037, Test Acc: 84.61%\n",
      "Epoch 27: Train Loss: 0.0039, Train Acc: 84.22%, Test Loss: 0.0043, Test Acc: 83.78%\n",
      "Epoch 28: Train Loss: 0.0039, Train Acc: 84.25%, Test Loss: 0.0042, Test Acc: 83.92%\n",
      "Epoch 29: Train Loss: 0.0039, Train Acc: 84.28%, Test Loss: 0.0037, Test Acc: 84.28%\n",
      "Epoch 30: Train Loss: 0.0038, Train Acc: 84.32%, Test Loss: 0.0041, Test Acc: 83.64%\n",
      "Epoch 31: Train Loss: 0.0038, Train Acc: 84.35%, Test Loss: 0.0037, Test Acc: 84.70%\n",
      "Epoch 32: Train Loss: 0.0038, Train Acc: 84.43%, Test Loss: 0.0041, Test Acc: 83.79%\n",
      "Epoch 33: Train Loss: 0.0038, Train Acc: 84.46%, Test Loss: 0.0039, Test Acc: 83.78%\n",
      "Epoch 34: Train Loss: 0.0038, Train Acc: 84.47%, Test Loss: 0.0043, Test Acc: 84.07%\n",
      "Epoch 35: Train Loss: 0.0038, Train Acc: 84.50%, Test Loss: 0.0047, Test Acc: 82.88%\n",
      "Epoch 36: Train Loss: 0.0037, Train Acc: 84.49%, Test Loss: 0.0038, Test Acc: 84.20%\n",
      "Epoch 37: Train Loss: 0.0037, Train Acc: 84.52%, Test Loss: 0.0036, Test Acc: 84.37%\n",
      "Epoch 38: Train Loss: 0.0037, Train Acc: 84.51%, Test Loss: 0.0040, Test Acc: 83.57%\n",
      "Epoch 39: Train Loss: 0.0037, Train Acc: 84.55%, Test Loss: 0.0041, Test Acc: 83.11%\n",
      "Epoch 40: Train Loss: 0.0036, Train Acc: 84.61%, Test Loss: 0.0038, Test Acc: 83.50%\n",
      "Epoch 41: Train Loss: 0.0036, Train Acc: 84.56%, Test Loss: 0.0042, Test Acc: 83.76%\n",
      "Epoch 42: Train Loss: 0.0036, Train Acc: 84.58%, Test Loss: 0.0037, Test Acc: 83.72%\n",
      "Epoch 43: Train Loss: 0.0036, Train Acc: 84.60%, Test Loss: 0.0041, Test Acc: 84.68%\n",
      "Epoch 44: Train Loss: 0.0036, Train Acc: 84.62%, Test Loss: 0.0037, Test Acc: 83.57%\n",
      "Epoch 45: Train Loss: 0.0036, Train Acc: 84.63%, Test Loss: 0.0039, Test Acc: 84.46%\n",
      "Epoch 46: Train Loss: 0.0036, Train Acc: 84.65%, Test Loss: 0.0036, Test Acc: 84.34%\n",
      "Epoch 47: Train Loss: 0.0036, Train Acc: 84.70%, Test Loss: 0.0040, Test Acc: 83.91%\n",
      "Epoch 48: Train Loss: 0.0036, Train Acc: 84.71%, Test Loss: 0.0034, Test Acc: 85.34%\n",
      "Epoch 49: Train Loss: 0.0036, Train Acc: 84.74%, Test Loss: 0.0038, Test Acc: 85.24%\n",
      "Epoch 50: Train Loss: 0.0035, Train Acc: 84.74%, Test Loss: 0.0040, Test Acc: 84.37%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_tm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_tm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_tm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_tm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_qm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0045, -0.4972, -0.5014, -0.4907, -0.5101, -0.4970]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.3976, -0.8768, -0.8936, -0.8247, -0.9237,  0.0061]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2225, -0.7509, -0.7294, -0.0045, -0.7570, -0.7192]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DRS Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_TM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[80, 80]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_drs.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_tm_ut_drs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_TM_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_TM_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_tm_model = DQN_TM_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "# Predict 6 vectors (6 actions' scores)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_tm_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_tm_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_tm_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_tm_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_tm_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_qm_pr_drs'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model.policy.q_net_target(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPO Tripple-modal Dynamic (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_TM_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_TM_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1_1 = nn.Linear(10, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc1_2 = nn.Linear(2, 16)   # Pod quota (CPU, Memory)\n",
    "        self.fc1_3 = nn.Linear(5, 16)  # Node difference (CPU, Memory)\n",
    "        self.fc2_1 = nn.Linear(16, 8)\n",
    "        self.fc2_2 = nn.Linear(16, 8)\n",
    "        self.fc2_3 = nn.Linear(16, 8)\n",
    "\n",
    "        self.fc3 = nn.Linear(24, 32)\n",
    "        self.fc4 = nn.Linear(32, 64)    # Last layer of FE_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = x[:, :10]\n",
    "        x2 = x[:, 10:]\n",
    "        x3 = x1[:, ::2] - x1[:, 1::2] # Takes the difference of x1's odd and even columns\n",
    "\n",
    "        # (node.spec[\"cpu_pool\"] - node.status[\"cpu_util\"] - pod.spec[\"cpu_req\"]) / node.spec[\"cpu_pool\"]\n",
    "        # (node.spec[\"mem_pool\"] - node.status[\"mem_util\"] - pod.spec[\"mem_req\"]) / node.spec[\"mem_pool\"]\n",
    "        # x1 = 1 - x1 - x2.repeat(1, 5)\n",
    "\n",
    "        x1 = F.relu(self.fc1_1(x1))  \n",
    "        x2 = F.relu(self.fc1_2(x2))\n",
    "        x3 = F.relu(self.fc1_3(x3))\n",
    "\n",
    "        x1 = F.relu(self.fc2_1(x1))\n",
    "        x2 = F.relu(self.fc2_2(x2))\n",
    "        x3 = F.relu(self.fc2_3(x3))\n",
    "\n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_TM_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[dict(pi=[80, 80], vf=[80, 80])]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.PPO('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_TM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=24, out_features=32, bias=True)\n",
       "    (fc4): Linear(in_features=32, out_features=64, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_tm_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class PPO_TM_Action_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(PPO_TM_Action_net, self).__init__()\n",
    "        self.features_extractor = original_model.policy.features_extractor\n",
    "        self.policy_net = original_model.policy.mlp_extractor.policy_net\n",
    "        self.action_net = original_model.policy.action_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features_extractor(x)\n",
    "        x = self.policy_net(x)\n",
    "        x = self.action_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_tm_action_model = PPO_TM_Action_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0037,  0.0031, -0.0085, -0.0003,  0.0036,  0.0068]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppo_tm_action_model(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(ppo_tm_action_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0154, Train Acc: 52.86%, Test Loss: 0.0077, Test Acc: 64.14%\n",
      "Epoch 2: Train Loss: 0.0072, Train Acc: 65.15%, Test Loss: 0.0061, Test Acc: 68.13%\n",
      "Epoch 3: Train Loss: 0.0061, Train Acc: 67.44%, Test Loss: 0.0053, Test Acc: 70.34%\n",
      "Epoch 4: Train Loss: 0.0057, Train Acc: 68.88%, Test Loss: 0.0045, Test Acc: 71.38%\n",
      "Epoch 5: Train Loss: 0.0055, Train Acc: 70.05%, Test Loss: 0.0044, Test Acc: 72.52%\n",
      "Epoch 6: Train Loss: 0.0053, Train Acc: 71.02%, Test Loss: 0.0043, Test Acc: 72.39%\n",
      "Epoch 7: Train Loss: 0.0052, Train Acc: 71.77%, Test Loss: 0.0044, Test Acc: 74.11%\n",
      "Epoch 8: Train Loss: 0.0050, Train Acc: 72.15%, Test Loss: 0.0044, Test Acc: 74.13%\n",
      "Epoch 9: Train Loss: 0.0050, Train Acc: 72.21%, Test Loss: 0.0041, Test Acc: 74.64%\n",
      "Epoch 10: Train Loss: 0.0048, Train Acc: 72.66%, Test Loss: 0.0055, Test Acc: 72.28%\n",
      "Epoch 11: Train Loss: 0.0048, Train Acc: 72.77%, Test Loss: 0.0042, Test Acc: 74.38%\n",
      "Epoch 12: Train Loss: 0.0048, Train Acc: 72.84%, Test Loss: 0.0050, Test Acc: 73.75%\n",
      "Epoch 13: Train Loss: 0.0047, Train Acc: 73.13%, Test Loss: 0.0040, Test Acc: 75.66%\n",
      "Epoch 14: Train Loss: 0.0046, Train Acc: 73.25%, Test Loss: 0.0046, Test Acc: 75.39%\n",
      "Epoch 15: Train Loss: 0.0045, Train Acc: 73.53%, Test Loss: 0.0040, Test Acc: 75.08%\n",
      "Epoch 16: Train Loss: 0.0045, Train Acc: 73.54%, Test Loss: 0.0038, Test Acc: 75.71%\n",
      "Epoch 17: Train Loss: 0.0045, Train Acc: 73.63%, Test Loss: 0.0044, Test Acc: 74.05%\n",
      "Epoch 18: Train Loss: 0.0044, Train Acc: 73.84%, Test Loss: 0.0046, Test Acc: 75.22%\n",
      "Epoch 19: Train Loss: 0.0043, Train Acc: 74.13%, Test Loss: 0.0033, Test Acc: 76.58%\n",
      "Epoch 20: Train Loss: 0.0043, Train Acc: 73.94%, Test Loss: 0.0046, Test Acc: 73.28%\n",
      "Epoch 21: Train Loss: 0.0043, Train Acc: 74.07%, Test Loss: 0.0039, Test Acc: 75.62%\n",
      "Epoch 22: Train Loss: 0.0041, Train Acc: 74.40%, Test Loss: 0.0045, Test Acc: 73.45%\n",
      "Epoch 23: Train Loss: 0.0041, Train Acc: 74.29%, Test Loss: 0.0038, Test Acc: 76.39%\n",
      "Epoch 24: Train Loss: 0.0041, Train Acc: 74.50%, Test Loss: 0.0053, Test Acc: 74.32%\n",
      "Epoch 25: Train Loss: 0.0041, Train Acc: 74.44%, Test Loss: 0.0041, Test Acc: 76.35%\n",
      "Epoch 26: Train Loss: 0.0040, Train Acc: 74.91%, Test Loss: 0.0035, Test Acc: 77.25%\n",
      "Epoch 27: Train Loss: 0.0040, Train Acc: 74.93%, Test Loss: 0.0039, Test Acc: 77.11%\n",
      "Epoch 28: Train Loss: 0.0039, Train Acc: 74.96%, Test Loss: 0.0038, Test Acc: 75.95%\n",
      "Epoch 29: Train Loss: 0.0039, Train Acc: 75.10%, Test Loss: 0.0042, Test Acc: 74.96%\n",
      "Epoch 30: Train Loss: 0.0040, Train Acc: 74.94%, Test Loss: 0.0043, Test Acc: 76.39%\n",
      "Epoch 31: Train Loss: 0.0039, Train Acc: 75.20%, Test Loss: 0.0044, Test Acc: 76.12%\n",
      "Epoch 32: Train Loss: 0.0038, Train Acc: 75.38%, Test Loss: 0.0043, Test Acc: 77.26%\n",
      "Epoch 33: Train Loss: 0.0038, Train Acc: 75.52%, Test Loss: 0.0028, Test Acc: 76.99%\n",
      "Epoch 34: Train Loss: 0.0038, Train Acc: 75.53%, Test Loss: 0.0038, Test Acc: 77.38%\n",
      "Epoch 35: Train Loss: 0.0038, Train Acc: 75.69%, Test Loss: 0.0044, Test Acc: 76.92%\n",
      "Epoch 36: Train Loss: 0.0037, Train Acc: 75.65%, Test Loss: 0.0034, Test Acc: 77.26%\n",
      "Epoch 37: Train Loss: 0.0037, Train Acc: 75.77%, Test Loss: 0.0043, Test Acc: 76.91%\n",
      "Epoch 38: Train Loss: 0.0037, Train Acc: 75.79%, Test Loss: 0.0031, Test Acc: 76.71%\n",
      "Epoch 39: Train Loss: 0.0037, Train Acc: 75.99%, Test Loss: 0.0038, Test Acc: 77.59%\n",
      "Epoch 40: Train Loss: 0.0036, Train Acc: 76.08%, Test Loss: 0.0032, Test Acc: 77.78%\n",
      "Epoch 41: Train Loss: 0.0036, Train Acc: 76.11%, Test Loss: 0.0033, Test Acc: 76.79%\n",
      "Epoch 42: Train Loss: 0.0036, Train Acc: 76.09%, Test Loss: 0.0033, Test Acc: 78.52%\n",
      "Epoch 43: Train Loss: 0.0036, Train Acc: 76.26%, Test Loss: 0.0046, Test Acc: 76.95%\n",
      "Epoch 44: Train Loss: 0.0035, Train Acc: 76.24%, Test Loss: 0.0051, Test Acc: 75.78%\n",
      "Epoch 45: Train Loss: 0.0035, Train Acc: 76.35%, Test Loss: 0.0040, Test Acc: 72.59%\n",
      "Epoch 46: Train Loss: 0.0035, Train Acc: 76.41%, Test Loss: 0.0040, Test Acc: 74.83%\n",
      "Epoch 47: Train Loss: 0.0035, Train Acc: 76.40%, Test Loss: 0.0036, Test Acc: 75.75%\n",
      "Epoch 48: Train Loss: 0.0035, Train Acc: 76.51%, Test Loss: 0.0041, Test Acc: 76.47%\n",
      "Epoch 49: Train Loss: 0.0035, Train Acc: 76.51%, Test Loss: 0.0028, Test Acc: 79.36%\n",
      "Epoch 50: Train Loss: 0.0035, Train Acc: 76.42%, Test Loss: 0.0030, Test Acc: 77.85%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(ppo_tm_action_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(ppo_tm_action_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.features_extractor.load_state_dict(ppo_tm_action_model.features_extractor.state_dict())\n",
    "rl_model.policy.mlp_extractor.policy_net.load_state_dict(ppo_tm_action_model.policy_net.state_dict())\n",
    "rl_model.policy.action_net.load_state_dict(ppo_tm_action_model.action_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_ppo_tm_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ActorCriticPolicy(\n",
       "  (features_extractor): FE_TM_net(\n",
       "    (fc1_1): Linear(in_features=10, out_features=16, bias=True)\n",
       "    (fc1_2): Linear(in_features=2, out_features=16, bias=True)\n",
       "    (fc1_3): Linear(in_features=5, out_features=16, bias=True)\n",
       "    (fc2_1): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_2): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc2_3): Linear(in_features=16, out_features=8, bias=True)\n",
       "    (fc3): Linear(in_features=24, out_features=16, bias=True)\n",
       "    (fc4): Linear(in_features=16, out_features=16, bias=True)\n",
       "  )\n",
       "  (mlp_extractor): MlpExtractor(\n",
       "    (shared_net): Sequential()\n",
       "    (policy_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "    (value_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=80, bias=True)\n",
       "      (1): Tanh()\n",
       "      (2): Linear(in_features=80, out_features=80, bias=True)\n",
       "      (3): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (action_net): Linear(in_features=80, out_features=6, bias=True)\n",
       "  (value_net): Linear(in_features=80, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1]), None)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.predict(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([5]), None)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.predict(sample2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2]), None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.predict(sample3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Single-modal Dynamic2 (Untrained + Pretrained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_DF_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_DF_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(5, 16) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc2 = nn.Linear(16, 16)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_nodes = 1 - x[:, :10]\n",
    "        x_pod = x[:, 10:]\n",
    "\n",
    "        x = x_nodes - x_pod.repeat(1, 5)\n",
    "        # Average odd and even\n",
    "        x = (x[:, ::2] + x[:, 1::2]) / 2 # 5 vectors -> (cpu_remain + mem_remain) / 2\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_DF_net,\n",
    "    features_extractor_kwargs=dict(features_dim=16),\n",
    "    net_arch=[32, 32]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_DF_net(\n",
       "      (fc1): Linear(in_features=5, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=32, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_DF_net(\n",
       "      (fc1): Linear(in_features=5, out_features=16, bias=True)\n",
       "      (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=16, out_features=32, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=32, out_features=32, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=32, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_df_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0485,  0.0233,  0.1265, -0.0524, -0.0899, -0.0192]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_DF_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_DF_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_df_model = DQN_DF_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_df_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0298, Train Acc: 47.83%, Test Loss: 0.0272, Test Acc: 51.21%\n",
      "Epoch 2: Train Loss: 0.0269, Train Acc: 52.18%, Test Loss: 0.0268, Test Acc: 51.44%\n",
      "Epoch 3: Train Loss: 0.0265, Train Acc: 52.73%, Test Loss: 0.0265, Test Acc: 51.92%\n",
      "Epoch 4: Train Loss: 0.0262, Train Acc: 53.22%, Test Loss: 0.0264, Test Acc: 52.08%\n",
      "Epoch 5: Train Loss: 0.0261, Train Acc: 53.31%, Test Loss: 0.0262, Test Acc: 52.23%\n",
      "Epoch 6: Train Loss: 0.0260, Train Acc: 53.27%, Test Loss: 0.0261, Test Acc: 52.00%\n",
      "Epoch 7: Train Loss: 0.0259, Train Acc: 53.26%, Test Loss: 0.0260, Test Acc: 51.78%\n",
      "Epoch 8: Train Loss: 0.0258, Train Acc: 53.23%, Test Loss: 0.0259, Test Acc: 51.51%\n",
      "Epoch 9: Train Loss: 0.0257, Train Acc: 53.11%, Test Loss: 0.0259, Test Acc: 51.65%\n",
      "Epoch 10: Train Loss: 0.0257, Train Acc: 53.21%, Test Loss: 0.0258, Test Acc: 51.55%\n",
      "Epoch 11: Train Loss: 0.0256, Train Acc: 53.21%, Test Loss: 0.0258, Test Acc: 51.92%\n",
      "Epoch 12: Train Loss: 0.0256, Train Acc: 53.23%, Test Loss: 0.0257, Test Acc: 51.99%\n",
      "Epoch 13: Train Loss: 0.0256, Train Acc: 53.33%, Test Loss: 0.0257, Test Acc: 52.16%\n",
      "Epoch 14: Train Loss: 0.0256, Train Acc: 53.41%, Test Loss: 0.0256, Test Acc: 52.20%\n",
      "Epoch 15: Train Loss: 0.0256, Train Acc: 53.43%, Test Loss: 0.0256, Test Acc: 52.40%\n",
      "Epoch 16: Train Loss: 0.0255, Train Acc: 53.48%, Test Loss: 0.0256, Test Acc: 52.35%\n",
      "Epoch 17: Train Loss: 0.0255, Train Acc: 53.46%, Test Loss: 0.0255, Test Acc: 52.20%\n",
      "Epoch 18: Train Loss: 0.0255, Train Acc: 53.44%, Test Loss: 0.0255, Test Acc: 52.23%\n",
      "Epoch 19: Train Loss: 0.0255, Train Acc: 53.47%, Test Loss: 0.0255, Test Acc: 52.32%\n",
      "Epoch 20: Train Loss: 0.0255, Train Acc: 53.64%, Test Loss: 0.0255, Test Acc: 52.49%\n",
      "Epoch 21: Train Loss: 0.0255, Train Acc: 53.81%, Test Loss: 0.0255, Test Acc: 52.49%\n",
      "Epoch 22: Train Loss: 0.0255, Train Acc: 53.80%, Test Loss: 0.0254, Test Acc: 52.83%\n",
      "Epoch 23: Train Loss: 0.0254, Train Acc: 53.86%, Test Loss: 0.0254, Test Acc: 53.17%\n",
      "Epoch 24: Train Loss: 0.0254, Train Acc: 53.92%, Test Loss: 0.0254, Test Acc: 53.19%\n",
      "Epoch 25: Train Loss: 0.0254, Train Acc: 53.97%, Test Loss: 0.0254, Test Acc: 53.08%\n",
      "Epoch 26: Train Loss: 0.0254, Train Acc: 54.00%, Test Loss: 0.0254, Test Acc: 53.17%\n",
      "Epoch 27: Train Loss: 0.0254, Train Acc: 54.00%, Test Loss: 0.0254, Test Acc: 53.20%\n",
      "Epoch 28: Train Loss: 0.0254, Train Acc: 53.99%, Test Loss: 0.0254, Test Acc: 53.15%\n",
      "Epoch 29: Train Loss: 0.0254, Train Acc: 53.97%, Test Loss: 0.0254, Test Acc: 53.48%\n",
      "Epoch 30: Train Loss: 0.0254, Train Acc: 53.99%, Test Loss: 0.0254, Test Acc: 53.25%\n",
      "Epoch 31: Train Loss: 0.0254, Train Acc: 53.98%, Test Loss: 0.0253, Test Acc: 53.40%\n",
      "Epoch 32: Train Loss: 0.0254, Train Acc: 53.95%, Test Loss: 0.0253, Test Acc: 53.33%\n",
      "Epoch 33: Train Loss: 0.0254, Train Acc: 53.92%, Test Loss: 0.0253, Test Acc: 53.49%\n",
      "Epoch 34: Train Loss: 0.0254, Train Acc: 53.95%, Test Loss: 0.0253, Test Acc: 53.45%\n",
      "Epoch 35: Train Loss: 0.0254, Train Acc: 53.91%, Test Loss: 0.0253, Test Acc: 53.62%\n",
      "Epoch 36: Train Loss: 0.0254, Train Acc: 53.90%, Test Loss: 0.0253, Test Acc: 53.53%\n",
      "Epoch 37: Train Loss: 0.0254, Train Acc: 53.90%, Test Loss: 0.0253, Test Acc: 53.62%\n",
      "Epoch 38: Train Loss: 0.0254, Train Acc: 53.90%, Test Loss: 0.0253, Test Acc: 53.57%\n",
      "Epoch 39: Train Loss: 0.0254, Train Acc: 53.88%, Test Loss: 0.0253, Test Acc: 53.37%\n",
      "Epoch 40: Train Loss: 0.0254, Train Acc: 53.93%, Test Loss: 0.0253, Test Acc: 53.64%\n",
      "Epoch 41: Train Loss: 0.0254, Train Acc: 53.94%, Test Loss: 0.0253, Test Acc: 53.52%\n",
      "Epoch 42: Train Loss: 0.0254, Train Acc: 53.94%, Test Loss: 0.0253, Test Acc: 53.44%\n",
      "Epoch 43: Train Loss: 0.0253, Train Acc: 54.01%, Test Loss: 0.0252, Test Acc: 53.55%\n",
      "Epoch 44: Train Loss: 0.0253, Train Acc: 54.00%, Test Loss: 0.0253, Test Acc: 53.38%\n",
      "Epoch 45: Train Loss: 0.0253, Train Acc: 54.05%, Test Loss: 0.0252, Test Acc: 53.57%\n",
      "Epoch 46: Train Loss: 0.0253, Train Acc: 54.00%, Test Loss: 0.0252, Test Acc: 53.75%\n",
      "Epoch 47: Train Loss: 0.0253, Train Acc: 54.02%, Test Loss: 0.0252, Test Acc: 53.78%\n",
      "Epoch 48: Train Loss: 0.0253, Train Acc: 53.99%, Test Loss: 0.0252, Test Acc: 53.77%\n",
      "Epoch 49: Train Loss: 0.0253, Train Acc: 54.07%, Test Loss: 0.0252, Test Acc: 53.95%\n",
      "Epoch 50: Train Loss: 0.0253, Train Acc: 54.04%, Test Loss: 0.0252, Test Acc: 53.77%\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_df_model, train_pr_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_df_model, test_pr_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_df_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_df_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_df_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN Single-modal Dynamic (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FE_NEW_net(BaseFeaturesExtractor):\n",
    "    def __init__(self, observation_space: spaces.Box, features_dim: int = 16):\n",
    "        super(FE_NEW_net, self).__init__(observation_space, features_dim)\n",
    "        self.fc1 = nn.Linear(15, 32) # 5 Nodes status (CPU, Memory)\n",
    "        self.fc2 = nn.Linear(32, 64)    # Last layer of FE_net\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_nodes = 1 - x[:, :10] # Available CPU and Memory\n",
    "        x_pod = x[:, 10:]  # Pod's CPU and Memory quota\n",
    "\n",
    "        # Scale x's elements to [0, 10] integers (0.1 belongs to 1, 0.9 belongs to 10)\n",
    "        x1 = (x_nodes - x_pod.repeat(1,5)) * 10\n",
    "        x1 = x1.int()\n",
    "\n",
    "        # Returns if the remaining resources of each node can accommodate the pod\n",
    "        x2_1 = x_nodes[:, ::2]\n",
    "        x2_1 = (x2_1 - x_pod[:, 0].unsqueeze(1)) >= 0\n",
    "        x2_2 = x_nodes[:, 1::2]\n",
    "        x2_2 = (x2_2 - x_pod[:, 1].unsqueeze(1)) >= 0\n",
    "        # If x2_1 and x_2_2 are both True, then x2 is True\n",
    "        x2 = x2_1 & x2_2 # Size : (batch_size, 5)\n",
    "        x2 = x2.int()\n",
    "\n",
    "        x = torch.cat((x1, x2), dim=1) # Size : (batch_size, 15)\n",
    "        x = x.float()\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "policy_kwargs = dict(\n",
    "    features_extractor_class=FE_NEW_net,\n",
    "    features_extractor_kwargs=dict(features_dim=64),\n",
    "    net_arch=[64, 32]\n",
    ")\n",
    "\n",
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py', scenario_file='scenario-5l-5m-1000p-10m_unbalanced.csv')\n",
    "\n",
    "rl_model = sb3.DQN('MlpPolicy', env, verbose=1, policy_kwargs=policy_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DQNPolicy(\n",
       "  (q_net): QNetwork(\n",
       "    (features_extractor): FE_NEW_net(\n",
       "      (fc1): Linear(in_features=15, out_features=32, bias=True)\n",
       "      (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=32, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (q_net_target): QNetwork(\n",
       "    (features_extractor): FE_NEW_net(\n",
       "      (fc1): Linear(in_features=15, out_features=32, bias=True)\n",
       "      (fc2): Linear(in_features=32, out_features=64, bias=True)\n",
       "    )\n",
       "    (q_net): Sequential(\n",
       "      (0): Linear(in_features=64, out_features=64, bias=True)\n",
       "      (1): ReLU()\n",
       "      (2): Linear(in_features=64, out_features=32, bias=True)\n",
       "      (3): ReLU()\n",
       "      (4): Linear(in_features=32, out_features=6, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_ut_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_new_ut_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1180, -0.0949, -0.2657,  0.0880, -0.0685,  0.0215]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rl_model.policy.q_net_target(sample1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model containing features_extractor, mlp_extractor.policy_net, action_net\n",
    "class DQN_NEW_net(nn.Module):\n",
    "    def __init__(self, original_model):\n",
    "        super(DQN_NEW_net, self).__init__()\n",
    "        self.q_net = original_model.policy.q_net\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.q_net(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_new_model = DQN_NEW_net(rl_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(dqn_new_model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    for state, target in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(state)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * state.size(0)\n",
    "        pred = output.argmax(dim=1, keepdim=True)\n",
    "        correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    accuracy = 100. * correct / len(train_loader.dataset)\n",
    "    return train_loss, accuracy\n",
    "\n",
    "def test(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for state, target in test_loader:\n",
    "            output = model(state)\n",
    "            test_loss += criterion(output, target).item() * state.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.argmax(dim=1, keepdim=True)).sum().item()\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    return test_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.0701, Train Acc: 75.42%, Test Loss: 0.0573, Test Acc: 78.13%\n",
      "Epoch 2: Train Loss: 0.0551, Train Acc: 78.39%, Test Loss: 0.0546, Test Acc: 78.29%\n",
      "Epoch 3: Train Loss: 0.0531, Train Acc: 78.84%, Test Loss: 0.0533, Test Acc: 78.67%\n",
      "Epoch 4: Train Loss: 0.0524, Train Acc: 79.10%, Test Loss: 0.0524, Test Acc: 79.06%\n",
      "Epoch 5: Train Loss: 0.0518, Train Acc: 79.27%, Test Loss: 0.0519, Test Acc: 79.46%\n",
      "Epoch 6: Train Loss: 0.0513, Train Acc: 79.41%, Test Loss: 0.0517, Test Acc: 79.63%\n",
      "Epoch 7: Train Loss: 0.0509, Train Acc: 79.53%, Test Loss: 0.0516, Test Acc: 79.57%\n",
      "Epoch 8: Train Loss: 0.0505, Train Acc: 79.63%, Test Loss: 0.0513, Test Acc: 79.68%\n",
      "Epoch 9: Train Loss: 0.0502, Train Acc: 79.70%, Test Loss: 0.0508, Test Acc: 79.74%\n",
      "Epoch 10: Train Loss: 0.0499, Train Acc: 79.70%, Test Loss: 0.0505, Test Acc: 79.80%\n",
      "Epoch 11: Train Loss: 0.0497, Train Acc: 79.76%, Test Loss: 0.0502, Test Acc: 79.92%\n",
      "Epoch 12: Train Loss: 0.0495, Train Acc: 79.80%, Test Loss: 0.0500, Test Acc: 79.98%\n",
      "Epoch 13: Train Loss: 0.0493, Train Acc: 79.79%, Test Loss: 0.0502, Test Acc: 79.94%\n",
      "Epoch 14: Train Loss: 0.0492, Train Acc: 79.86%, Test Loss: 0.0505, Test Acc: 79.87%\n",
      "Epoch 15: Train Loss: 0.0491, Train Acc: 79.87%, Test Loss: 0.0504, Test Acc: 79.93%\n",
      "Epoch 16: Train Loss: 0.0490, Train Acc: 79.93%, Test Loss: 0.0504, Test Acc: 79.89%\n",
      "Epoch 17: Train Loss: 0.0489, Train Acc: 79.95%, Test Loss: 0.0503, Test Acc: 79.94%\n",
      "Epoch 18: Train Loss: 0.0488, Train Acc: 79.97%, Test Loss: 0.0503, Test Acc: 79.95%\n",
      "Epoch 19: Train Loss: 0.0488, Train Acc: 79.99%, Test Loss: 0.0494, Test Acc: 80.23%\n",
      "Epoch 20: Train Loss: 0.0487, Train Acc: 80.00%, Test Loss: 0.0502, Test Acc: 79.90%\n",
      "Epoch 21: Train Loss: 0.0487, Train Acc: 80.02%, Test Loss: 0.0499, Test Acc: 80.07%\n",
      "Epoch 22: Train Loss: 0.0486, Train Acc: 80.06%, Test Loss: 0.0494, Test Acc: 80.10%\n",
      "Epoch 23: Train Loss: 0.0485, Train Acc: 80.05%, Test Loss: 0.0494, Test Acc: 80.13%\n",
      "Epoch 24: Train Loss: 0.0485, Train Acc: 80.09%, Test Loss: 0.0490, Test Acc: 80.32%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m test_acc \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfor\u001b[39;00m epoch \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, epochs\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m----> 4\u001b[0m     train_loss, train_acc \u001b[39m=\u001b[39m train(dqn_new_model, train_dynamic_dataloader, criterion, optimizer)\n\u001b[1;32m      5\u001b[0m     test_loss, test_acc \u001b[39m=\u001b[39m test(dqn_new_model, test_dynamic_dataloader, criterion)\n\u001b[1;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mEpoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m: Train Loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Train Acc: \u001b[39m\u001b[39m{\u001b[39;00mtrain_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%, Test Loss: \u001b[39m\u001b[39m{\u001b[39;00mtest_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m, Test Acc: \u001b[39m\u001b[39m{\u001b[39;00mtest_acc\u001b[39m:\u001b[39;00m\u001b[39m.2f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m%\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[38], line 10\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[1;32m      8\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, target)\n\u001b[1;32m      9\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[0;32m---> 10\u001b[0m optimizer\u001b[39m.\u001b[39;49mstep()\n\u001b[1;32m     11\u001b[0m train_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem() \u001b[39m*\u001b[39m state\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m)\n\u001b[1;32m     12\u001b[0m pred \u001b[39m=\u001b[39m output\u001b[39m.\u001b[39margmax(dim\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, keepdim\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/optim/optimizer.py:113\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    111\u001b[0m profile_name \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mOptimizer.step#\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.step\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(obj\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m)\n\u001b[1;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mautograd\u001b[39m.\u001b[39mprofiler\u001b[39m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 113\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/autograd/grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m     25\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     26\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclone():\n\u001b[0;32m---> 27\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/torch/optim/adam.py:149\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[39mif\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m    146\u001b[0m         \u001b[39m# Maintains max of all exp. moving avg. of sq. grad. values\u001b[39;00m\n\u001b[1;32m    147\u001b[0m         state[\u001b[39m'\u001b[39m\u001b[39mmax_exp_avg_sq\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mzeros_like(p, memory_format\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mpreserve_format)\n\u001b[0;32m--> 149\u001b[0m exp_avgs\u001b[39m.\u001b[39;49mappend(state[\u001b[39m'\u001b[39;49m\u001b[39mexp_avg\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    150\u001b[0m exp_avg_sqs\u001b[39m.\u001b[39mappend(state[\u001b[39m'\u001b[39m\u001b[39mexp_avg_sq\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m    152\u001b[0m \u001b[39mif\u001b[39;00m group[\u001b[39m'\u001b[39m\u001b[39mamsgrad\u001b[39m\u001b[39m'\u001b[39m]:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "test_acc = 0\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss, train_acc = train(dqn_new_model, train_dynamic_dataloader, criterion, optimizer)\n",
    "    test_loss, test_acc = test(dqn_new_model, test_dynamic_dataloader, criterion)\n",
    "    print(f'Epoch {epoch}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%, Test Loss: {test_loss:.4f}, Test Acc: {test_acc:.2f}%')\n",
    "    if test_acc > 95:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rl_model.policy.q_net.load_state_dict(dqn_df_model.q_net.state_dict())\n",
    "rl_model.policy.q_net_target.load_state_dict(dqn_df_model.q_net.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_ppo_mm_pr_dynamic\n",
    "rl_model.save(os.path.join(base_path, 'notebook', 'net_arch', 'model_dqn_df_pr_dynamic'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
