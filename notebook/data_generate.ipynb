{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "\n",
    "base_path = os.path.join(os.getcwd(), \"..\")\n",
    "print(f\"Base Path: {base_path}\")\n",
    "sys.path.append(base_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 1-label data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SimKubeEnv-v0')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "# Data consists of state, scheduler decision\n",
    "data_size = 1000000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_1.csv\")\n",
    "with open(data_path, 'w') as f:\n",
    "    for i in range(data_size):\n",
    "        state = list(env.random_state_gen())\n",
    "        action = scheduler.decision(env)\n",
    "        data = state + [action]\n",
    "        f.write(','.join(list(map(str,data))) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 6-confidence data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SimKubeEnv-v0')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data generation\n",
    "# Data consists of state, scheduler decision\n",
    "data_size = 1000000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_5.csv\")\n",
    "with open(data_path, 'w') as f:\n",
    "    for i in range(data_size):\n",
    "        state = list(env.random_state_gen())\n",
    "        confidence = scheduler.model.get_confidence(env)\n",
    "        data = state + confidence.tolist()\n",
    "        f.write(','.join(list(map(str,data))) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. 1-confidence data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from kube_mm_scheduler.model.net3 import Net3\n",
    "\n",
    "net3 = Net3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the score for each action\n",
    "def predict_score(data, model=net3):\n",
    "    model.eval()\n",
    "    if not torch.is_tensor(data):\n",
    "        data = torch.tensor(data, dtype=torch.float32)\n",
    "    data10 = data[:, :-2]\n",
    "    data2 = data[:, -2:]\n",
    "    output = model(data10, data2)\n",
    "    # Round up at the 4th decimal point\n",
    "    output = torch.round(output * 10000) / 10000\n",
    "    # # Softmax\n",
    "    # output = F.softmax(output, dim=1)\n",
    "    # print(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m flt_state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mmap\u001b[39m(\u001b[39mfloat\u001b[39m, node_state\u001b[39m+\u001b[39mpod_quota))\n\u001b[1;32m     14\u001b[0m \u001b[39m# print(flt_state)\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m confidence \u001b[39m=\u001b[39m predict_score([flt_state])\n\u001b[1;32m     17\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m5\u001b[39m):\n\u001b[1;32m     18\u001b[0m     selected_node_state \u001b[39m=\u001b[39m node_state[i\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m:i\u001b[39m*\u001b[39m\u001b[39m2\u001b[39m\u001b[39m+\u001b[39m\u001b[39m2\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 5\u001b[0m, in \u001b[0;36mpredict_score\u001b[0;34m(data, model)\u001b[0m\n\u001b[1;32m      3\u001b[0m model\u001b[39m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mis_tensor(data):\n\u001b[0;32m----> 5\u001b[0m     data \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor(data, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m      6\u001b[0m data10 \u001b[39m=\u001b[39m data[:, :\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[1;32m      7\u001b[0m data2 \u001b[39m=\u001b[39m data[:, \u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Read from data_1.csv and formulate data_3.csv which has additional selected node information\n",
    "src_path = os.path.join(base_path, \"dataset\", \"data_1.csv\")\n",
    "with open(src_path, 'r') as f:\n",
    "    data_path = os.path.join(base_path, \"dataset\", \"data_3.csv\")\n",
    "    with open(data_path, 'w') as g:\n",
    "        for line in f.readlines():\n",
    "            line = line.strip()\n",
    "            line = line.split(',')\n",
    "            node_state = line[:-3]\n",
    "            pod_quota = line[-3:-1]\n",
    "\n",
    "            # Confidence\n",
    "            flt_state = list(map(float, node_state+pod_quota))\n",
    "            # print(flt_state)\n",
    "            confidence = predict_score([flt_state])\n",
    "\n",
    "            for i in range(5):\n",
    "                selected_node_state = node_state[i*2:i*2+2]\n",
    "                new_state = node_state + pod_quota + selected_node_state\n",
    "                conf = confidence[0].tolist()[i+1]\n",
    "                # Takes the 4th decimal point\n",
    "                conf = round(conf, 4)\n",
    "                \n",
    "                new_data = new_state + [conf]\n",
    "                g.write(','.join(list(map(str,new_data))) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 6-reward data generation (Dynamic2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2.py')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 1000000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_5.csv\")\n",
    "\n",
    "with open(data_path, 'w') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 6-reward data generation (Static)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_drs.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 24%|██▍       | 243035/1000000 [1:52:36<5:50:44, 35.97it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m env0 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mduplicate() \u001b[39m# For action 0\u001b[39;00m\n\u001b[1;32m     13\u001b[0m env1 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mduplicate() \u001b[39m# For action 1\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m env2 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mduplicate() \u001b[39m# For action 2\u001b[39;00m\n\u001b[1;32m     15\u001b[0m env3 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mduplicate() \u001b[39m# For action 3\u001b[39;00m\n\u001b[1;32m     16\u001b[0m env4 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mduplicate() \u001b[39m# For action 4\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env.py:62\u001b[0m, in \u001b[0;36mSimKubeEnv.duplicate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mduplicate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     58\u001b[0m     \u001b[39m# Copy the class ifself, but should be separated from the original one\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     \u001b[39m# This is to prevent the original one from being modified\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcopy\u001b[39;00m \u001b[39mimport\u001b[39;00m deepcopy\n\u001b[0;32m---> 62\u001b[0m     new_env \u001b[39m=\u001b[39m SimKubeEnvCopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscenario_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_node, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcpu_pool, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmem_pool, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebug)\n\u001b[1;32m     63\u001b[0m     new_env\u001b[39m.\u001b[39mcluster \u001b[39m=\u001b[39m deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster)\n\u001b[1;32m     64\u001b[0m     new_env\u001b[39m.\u001b[39mtime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env_copy.py:25\u001b[0m, in \u001b[0;36mSimKubeEnvCopy.__init__\u001b[0;34m(self, reward_file, scenario_file, n_node, cpu_pool, mem_pool, debug)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_fn_name \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39msplitext(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward_file)[\u001b[39m0\u001b[39m]\n\u001b[1;32m     24\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscenario_file \u001b[39m=\u001b[39m scenario_file\n\u001b[0;32m---> 25\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstress_gen \u001b[39m=\u001b[39m SimStressGen(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscenario_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebug)\n\u001b[1;32m     27\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_node \u001b[39m=\u001b[39m n_node\n\u001b[1;32m     28\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcpu_pool \u001b[39m=\u001b[39m cpu_pool\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/utils/sim_stress_gen.py:17\u001b[0m, in \u001b[0;36mSimStressGen.__init__\u001b[0;34m(self, scenario_file, debug)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdebug \u001b[39m=\u001b[39m debug\n\u001b[1;32m     16\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscenario_file \u001b[39m=\u001b[39m scenario_file\n\u001b[0;32m---> 17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscenario \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_scenario(scenario_file)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 1000000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_drs.csv\")\n",
    "\n",
    "with open(data_path, 'w') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 6-reward data generation (Default)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 1000000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_default.csv\")\n",
    "\n",
    "with open(data_path, 'w') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 6-reward data generation (Dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 300000/300000 [3:15:14<00:00, 25.61it/s]   \n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 300000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic.csv\")\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. 6-reward data generation (Default2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_default2.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 300000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_default2.csv\")\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. 6-reward data generation (Dynamic2_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic2_time.py')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 300000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic2_time.csv\")\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        time = env.time\n",
    "        # print(f\"State: {state} / Time: {time}\")\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '  ,  ' + str(time) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. 6-reward data generation (Dynamic_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_dynamic_time.py')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 300000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dynamic_time.csv\")\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        time = env.time\n",
    "        # print(f\"State: {state} / Time: {time}\")\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '  ,  ' + str(time) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. 6-reward data generation (step_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_step_3.py')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 479625/1000000 [43:19<47:00, 184.48it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 13\u001b[0m\n\u001b[1;32m      9\u001b[0m state \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(env\u001b[39m.\u001b[39mrandom_state_gen())\n\u001b[1;32m     10\u001b[0m \u001b[39m# print(f\"State: {state} / Time: {time}\")\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[39m# print(f\"State: {state}\")\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m env0 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39;49mduplicate() \u001b[39m# For action 0\u001b[39;00m\n\u001b[1;32m     14\u001b[0m env1 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mduplicate() \u001b[39m# For action 1\u001b[39;00m\n\u001b[1;32m     15\u001b[0m env2 \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mduplicate() \u001b[39m# For action 2\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env.py:80\u001b[0m, in \u001b[0;36mSimKubeEnv.duplicate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mduplicate\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m     76\u001b[0m     \u001b[39m# Copy the class ifself, but should be separated from the original one\u001b[39;00m\n\u001b[1;32m     77\u001b[0m     \u001b[39m# This is to prevent the original one from being modified\u001b[39;00m\n\u001b[1;32m     78\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mcopy\u001b[39;00m \u001b[39mimport\u001b[39;00m deepcopy\n\u001b[0;32m---> 80\u001b[0m     new_env \u001b[39m=\u001b[39m SimKubeEnvCopy(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreward_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mscenario_file, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_node, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcpu_pool, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmem_pool, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdebug)\n\u001b[1;32m     81\u001b[0m     new_env\u001b[39m.\u001b[39mcluster \u001b[39m=\u001b[39m deepcopy(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcluster)\n\u001b[1;32m     82\u001b[0m     new_env\u001b[39m.\u001b[39mtime \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtime\n",
      "File \u001b[0;32m~/Documents/coding/thesis/PROMES_colab/notebook/../kube_sim_gym/envs/sim_kube_env_copy.py:43\u001b[0m, in \u001b[0;36mSimKubeEnvCopy.__init__\u001b[0;34m(self, reward_file, scenario_file, n_node, cpu_pool, mem_pool, debug)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreward \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdone \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobservation_space \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mspaces\u001b[39m.\u001b[39;49mBox(low\u001b[39m=\u001b[39;49m\u001b[39m0.0\u001b[39;49m, high\u001b[39m=\u001b[39;49m\u001b[39m1.0\u001b[39;49m, shape\u001b[39m=\u001b[39;49m(n_node \u001b[39m*\u001b[39;49m \u001b[39m2\u001b[39;49m \u001b[39m+\u001b[39;49m \u001b[39m2\u001b[39;49m,), dtype\u001b[39m=\u001b[39;49mnp\u001b[39m.\u001b[39;49mfloat32)\n\u001b[1;32m     44\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39mspaces\u001b[39m.\u001b[39mDiscrete(n_node \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_map \u001b[39m=\u001b[39m {\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39mstandby\u001b[39m\u001b[39m'\u001b[39m}\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/kube-gym/lib/python3.8/site-packages/gym/spaces/box.py:81\u001b[0m, in \u001b[0;36mBox.__init__\u001b[0;34m(self, low, high, shape, dtype, seed)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[39m# Boolean arrays which indicate the interval type for each coordinate\u001b[39;00m\n\u001b[1;32m     80\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbounded_below \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mnp\u001b[39m.\u001b[39minf \u001b[39m<\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlow\n\u001b[0;32m---> 81\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbounded_above \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49minf \u001b[39m>\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhigh\n\u001b[1;32m     83\u001b[0m \u001b[39msuper\u001b[39m(Box, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype, seed)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "data_size = 1000000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_step3.csv\")\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    for i in tqdm.tqdm(range(data_size)):\n",
    "        env.reset()\n",
    "        state = list(env.random_state_gen())\n",
    "        # print(f\"State: {state} / Time: {time}\")\n",
    "        # print(f\"State: {state}\")\n",
    "\n",
    "        env0 = env.duplicate() # For action 0\n",
    "        env1 = env.duplicate() # For action 1\n",
    "        env2 = env.duplicate() # For action 2\n",
    "        env3 = env.duplicate() # For action 3\n",
    "        env4 = env.duplicate() # For action 4\n",
    "        env5 = env.duplicate() # For action 5\n",
    "\n",
    "        state0, rew0, _, _ = env0.step(0, True)\n",
    "        # print(f\"Action 0\")\n",
    "        # print(f\"State0: {state0}\")\n",
    "        # print(\"====================\")\n",
    "        state1, rew1, _, _ = env1.step(1, True)\n",
    "        # print(f\"Action 1\")\n",
    "        # print(f\"State1: {state1}\") \n",
    "        # print(\"====================\")\n",
    "        state2, rew2, _, _ = env2.step(2, True)\n",
    "        # print(f\"Action 2\")\n",
    "        # print(f\"State2: {state2}\")\n",
    "        # print(\"====================\")\n",
    "        state3, rew3, _, _ = env3.step(3, True)\n",
    "        # print(f\"Action 3\")\n",
    "        # print(f\"State3: {state3}\")\n",
    "        # print(\"====================\")\n",
    "        state4, rew4, _, _ = env4.step(4, True)\n",
    "        # print(f\"Action 4\")\n",
    "        # print(f\"State4: {state4}\")\n",
    "        # print(\"====================\")\n",
    "        state5, rew5, _, _ = env5.step(5, True)\n",
    "        # print(f\"Action 5\")\n",
    "        # print(f\"State5: {state5}\")\n",
    "\n",
    "        reward = [rew0, rew1, rew2, rew3, rew4, rew5]\n",
    "        # Round up at the 4th decimal point\n",
    "        reward = list(map(lambda x: round(x, 4), reward))\n",
    "        \n",
    "        f.write(','.join(list(map(str,state))) + '  ,  ' + ','.join(list(map(str,reward))) + '\\n')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Imitation Transitions generation w/ Default scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_step_3.py')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "epochs = 100\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_expert.csv\")\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    for i in tqdm.tqdm(range(epochs)):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action = scheduler.decision(env)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "            line = state.tolist() + [action] + next_state.tolist() + [int(done)]\n",
    "\n",
    "            line = list(map(lambda x: round(float(x), 2), line))\n",
    "            line = list(map(str, line))\n",
    "            # print(line)\n",
    "            if len(line) == 26:\n",
    "                f.write(','.join(line) + '\\n')\n",
    "\n",
    "            state = next_state"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. DQfD data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_step_3.py')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:04<00:00,  1.25it/s]\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "epochs = 5\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_dqfd.csv\")\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    for i in tqdm.tqdm(range(epochs)):\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        while not done:\n",
    "            action = scheduler.decision(env)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "        \n",
    "            line = state.tolist() + [action, reward]\n",
    "\n",
    "            line = list(map(lambda x: round(float(x), 2), line))\n",
    "            line = list(map(str, line))\n",
    "            # print(line)\n",
    "            if len(line) == 14:\n",
    "                f.write(','.join(line) + '\\n')\n",
    "\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Selective Expert data generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load gym environment\n",
    "import gym\n",
    "from kube_sim_gym import *\n",
    "from kube_sim_gym.envs.sim_kube_env import SimKubeEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kube_hr_scheduler.scheduler.sim_hr_scheduler import SimHrScheduler\n",
    "from kube_hr_scheduler.strategies.model.default import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base Path: /Users/swkim/Documents/coding/thesis/PROMES_colab/notebook/..\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('SimKubeEnv-v0', reward_file='train_step_3.py')\n",
    "scheduler = SimHrScheduler(env, 'default.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Count: 0\n",
      "[1.   1.   1.   1.   1.   1.   1.   1.   1.   1.   0.01 0.19]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gl/n3l9mrsx1jz4xhb0rsxhj8980000gn/T/ipykernel_47952/914017129.py:14: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  state_cos_sim = np.dot(state_a, state_b) / (np.linalg.norm(state_a) * np.linalg.norm(state_b))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.   1.   0.99 0.81 1.   1.   1.   1.   1.   1.   0.16 0.1 ]\n",
      "[1.   1.   0.99 0.81 0.84 0.9  1.   1.   1.   1.   0.3  0.24]\n",
      "[1.   1.   0.99 0.81 0.84 0.9  0.7  0.76 1.   1.   0.03 0.15]\n",
      "[0.97 0.85 0.99 0.81 0.84 0.9  0.7  0.76 1.   1.   0.09 0.19]\n",
      "[0.97 0.85 0.99 0.81 0.84 0.9  0.7  0.76 0.91 0.81 0.29 0.12]\n",
      "[0.97 0.85 0.99 0.81 0.55 0.78 0.7  0.76 0.91 0.81 0.24 0.16]\n",
      "[0.73 0.69 0.99 0.81 0.55 0.78 0.7  0.76 0.91 0.81 0.05 0.07]\n",
      "[0.73 0.69 0.99 0.81 0.55 0.78 0.7  0.76 0.86 0.74 0.   0.  ]\n",
      "[0.73 0.69 0.99 0.81 0.55 0.78 0.7  0.76 0.86 0.74 0.2  0.08]\n",
      "[0.73 0.69 0.79 0.73 0.55 0.78 0.7  0.76 0.86 0.74 0.18 0.11]\n",
      "[0.73 0.69 0.61 0.62 0.55 0.78 0.7  0.76 0.86 0.74 0.03 0.21]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 48\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[39mprint\u001b[39m(state)\n\u001b[1;32m     47\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtime\u001b[39;00m \u001b[39mimport\u001b[39;00m sleep\n\u001b[0;32m---> 48\u001b[0m sleep(\u001b[39m3\u001b[39;49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import numpy as np\n",
    "\n",
    "# epochs = 100\n",
    "data_size = 1000000\n",
    "data_path = os.path.join(base_path, \"dataset\", \"data_expert_selective.csv\")\n",
    "\n",
    "def cos_sim(a, b, thres):\n",
    "    state_a = np.array(a[:12])\n",
    "    action_a = np.array(a[12])\n",
    "    state_b = np.array(b[:12])\n",
    "    action_b = np.array(b[12])\n",
    "\n",
    "    state_cos_sim = np.dot(state_a, state_b) / (np.linalg.norm(state_a) * np.linalg.norm(state_b))\n",
    "\n",
    "    if abs(state_cos_sim - 1) < thres * 0.01 and action_a == action_b:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "with open(data_path, 'a') as f:\n",
    "    data_count = 0\n",
    "    while data_count < data_size:\n",
    "        done = False\n",
    "        state = env.reset()\n",
    "        prev = np.zeros(26)\n",
    "        while not done:\n",
    "            if data_count % 1000 == 0:\n",
    "                print(f\"Data Count: {data_count}\")\n",
    "            action = scheduler.decision(env)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            line = state.tolist() + [action] + next_state.tolist() + [int(done)]\n",
    "\n",
    "            line = list(map(lambda x: round(float(x), 2), line))\n",
    "            # Change action data type to int\n",
    "            line[12] = int(line[12])\n",
    "\n",
    "            if len(line) == 26 and not cos_sim(prev, line, 5):\n",
    "                f.write(','.join(list(map(str, line))) + '\\n')\n",
    "                data_count += 1\n",
    "                prev = line\n",
    "\n",
    "            state = next_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2])\n",
    "b = np.array([1,2])\n",
    "\n",
    "np.dot(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kube-gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
