{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ShofuYy-8dLs"
      },
      "source": [
        "## Stable Baselines3 - Pretraining with Behavior Cloning\n",
        "\n",
        "\n",
        "Github repo: https://github.com/araffin/rl-tutorial-jnrr19\n",
        "\n",
        "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
        "\n",
        "Documentation: https://stable-baselines.readthedocs.io/en/master/\n",
        "\n",
        "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
        "\n",
        "\n",
        "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
        "\n",
        "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
        "\n",
        "## Introduction\n",
        "\n",
        "In this notebook, you will learn how to record expert data, then pre-train an agent using this data\n",
        "and finally continue training with Stable-Baselines3.\n",
        "\n",
        "\n",
        "## Install Dependencies and Stable Baselines3 Using Pip\n",
        "\n",
        "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
        "\n",
        "Notebook originally created by [skervim](https://github.com/skervim)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MuDKkQkjua_O"
      },
      "outputs": [],
      "source": [
        "# for autoformatting\n",
        "# %load_ext jupyter_black"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QwJ1LHlN9E33"
      },
      "outputs": [],
      "source": [
        "# For Box2D env\n",
        "!apt-get update && apt-get install swig\n",
        "!pip install gymnasium[box2d]\n",
        "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tgx4AMZo8anP",
        "outputId": "90a4b60a-3bac-4834-8382-b3681ed6163f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.28.1\n"
          ]
        }
      ],
      "source": [
        "import gymnasium as gym\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "print(f\"{gym.__version__}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "U_1ZNBum8ane"
      },
      "outputs": [],
      "source": [
        "import torch as th\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import StepLR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "iaz2Szrl8anx"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC, TD3, DQN\n",
        "from stable_baselines3.common.evaluation import evaluate_policy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "dS9lr70R9eU9"
      },
      "outputs": [],
      "source": [
        "# Example for continuous actions\n",
        "# env_id = \"LunarLanderContinuous-v2\"\n",
        "\n",
        "# Example for discrete actions\n",
        "env_id = \"CartPole-v1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "eh88d4oR8an6"
      },
      "outputs": [],
      "source": [
        "env = gym.make(env_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKCgHCc_8aoB"
      },
      "source": [
        "## Train Expert Model\n",
        "\n",
        "We create an expert RL agent and let it learn to solve a task by interacting with the evironment.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ppo_expert = PPO(\"MlpPolicy\", env_id, verbose=1)\n",
        "rew, std = evaluate_policy(ppo_expert, env)\n",
        "print(f\"Teacher's initial performance: {rew}(+/- {std})\")"
      ],
      "metadata": {
        "id": "uZwolW_Zz3bB",
        "outputId": "9c5cd0f6-69cd-4c54-d6dc-bd5984396a1f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Teacher's initial performance: 9.1(+/- 0.7000000000000001)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkmIST0r8aoC",
        "outputId": "578a2b61-03ed-4a5d-cc2c-f3eefe68b315"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 18.9     |\n",
            "|    ep_rew_mean     | 18.9     |\n",
            "| time/              |          |\n",
            "|    fps             | 1474     |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 1        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 27.2       |\n",
            "|    ep_rew_mean          | 27.2       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 1028       |\n",
            "|    iterations           | 2          |\n",
            "|    time_elapsed         | 3          |\n",
            "|    total_timesteps      | 4096       |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00896654 |\n",
            "|    clip_fraction        | 0.104      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.686     |\n",
            "|    explained_variance   | 0.0002     |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.93       |\n",
            "|    n_updates            | 10         |\n",
            "|    policy_gradient_loss | -0.0151    |\n",
            "|    value_loss           | 44         |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 36.9        |\n",
            "|    ep_rew_mean          | 36.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 882         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 6           |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008371557 |\n",
            "|    clip_fraction        | 0.0422      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.667      |\n",
            "|    explained_variance   | 0.0948      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14          |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0149     |\n",
            "|    value_loss           | 38.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 48.7        |\n",
            "|    ep_rew_mean          | 48.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 849         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 9           |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011075552 |\n",
            "|    clip_fraction        | 0.122       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.631      |\n",
            "|    explained_variance   | 0.274       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 19.1        |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0236     |\n",
            "|    value_loss           | 52.9        |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 62.3       |\n",
            "|    ep_rew_mean          | 62.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 848        |\n",
            "|    iterations           | 5          |\n",
            "|    time_elapsed         | 12         |\n",
            "|    total_timesteps      | 10240      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00979734 |\n",
            "|    clip_fraction        | 0.0703     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.604     |\n",
            "|    explained_variance   | 0.202      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 18.5       |\n",
            "|    n_updates            | 40         |\n",
            "|    policy_gradient_loss | -0.0171    |\n",
            "|    value_loss           | 68.6       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 78.4        |\n",
            "|    ep_rew_mean          | 78.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 842         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 14          |\n",
            "|    total_timesteps      | 12288       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008616605 |\n",
            "|    clip_fraction        | 0.0958      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.585      |\n",
            "|    explained_variance   | 0.465       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21          |\n",
            "|    n_updates            | 50          |\n",
            "|    policy_gradient_loss | -0.0164     |\n",
            "|    value_loss           | 60.3        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 94.7         |\n",
            "|    ep_rew_mean          | 94.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 832          |\n",
            "|    iterations           | 7            |\n",
            "|    time_elapsed         | 17           |\n",
            "|    total_timesteps      | 14336        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0050750505 |\n",
            "|    clip_fraction        | 0.0279       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.584       |\n",
            "|    explained_variance   | 0.561        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.92         |\n",
            "|    n_updates            | 60           |\n",
            "|    policy_gradient_loss | -0.00589     |\n",
            "|    value_loss           | 49.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 109          |\n",
            "|    ep_rew_mean          | 109          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 823          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 19           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023568128 |\n",
            "|    clip_fraction        | 0.0186       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.576       |\n",
            "|    explained_variance   | 0.75         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.92         |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.0042      |\n",
            "|    value_loss           | 38.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 128         |\n",
            "|    ep_rew_mean          | 128         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 829         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 22          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008492313 |\n",
            "|    clip_fraction        | 0.0771      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.58       |\n",
            "|    explained_variance   | 0.902       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.97        |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.0107     |\n",
            "|    value_loss           | 23.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 146          |\n",
            "|    ep_rew_mean          | 146          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 833          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 24           |\n",
            "|    total_timesteps      | 20480        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029102932 |\n",
            "|    clip_fraction        | 0.0147       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.564       |\n",
            "|    explained_variance   | 0.0687       |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 19.8         |\n",
            "|    n_updates            | 90           |\n",
            "|    policy_gradient_loss | -0.00423     |\n",
            "|    value_loss           | 57.8         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 164         |\n",
            "|    ep_rew_mean          | 164         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 835         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 26          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006421673 |\n",
            "|    clip_fraction        | 0.069       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.57       |\n",
            "|    explained_variance   | 0.733       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13.8        |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.011      |\n",
            "|    value_loss           | 44.8        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 182         |\n",
            "|    ep_rew_mean          | 182         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 831         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 29          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010112917 |\n",
            "|    clip_fraction        | 0.0877      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.554      |\n",
            "|    explained_variance   | 0.814       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.55        |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00696    |\n",
            "|    value_loss           | 15.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 198          |\n",
            "|    ep_rew_mean          | 198          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 828          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 32           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054419357 |\n",
            "|    clip_fraction        | 0.0293       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.546       |\n",
            "|    explained_variance   | 0.726        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.92         |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -0.00225     |\n",
            "|    value_loss           | 38           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 216          |\n",
            "|    ep_rew_mean          | 216          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 825          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 34           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053168237 |\n",
            "|    clip_fraction        | 0.0183       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.53        |\n",
            "|    explained_variance   | 0.448        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.6          |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.00252     |\n",
            "|    value_loss           | 11.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 233          |\n",
            "|    ep_rew_mean          | 233          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 822          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 37           |\n",
            "|    total_timesteps      | 30720        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0058961352 |\n",
            "|    clip_fraction        | 0.0438       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.514       |\n",
            "|    explained_variance   | 0.429        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.254        |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00367     |\n",
            "|    value_loss           | 1.88         |\n",
            "------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "ppo_expert.learn(total_timesteps=3e4)\n",
        "ppo_expert.save(\"ppo_expert\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hlyTfGAQ_Az1"
      },
      "source": [
        "check the performance of the trained agent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-_rVEjC0_AQa",
        "outputId": "4ab5bae0-aa90-4717-db35-7842b5561c6b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward = 500.0 +/- 0.0\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(ppo_expert, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-Kv6v_V8aoJ"
      },
      "source": [
        "## Create Student\n",
        "\n",
        "We also create a student RL agent, which will later be trained with the expert dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLdLPUeC8aoL",
        "outputId": "62bd03c2-a337-4bf1-f7e2-fcdd52212f06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cpu device\n",
            "Creating environment from the given name 'CartPole-v1'\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        }
      ],
      "source": [
        "a2c_student = DQN(\"MlpPolicy\", env_id, verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "sdW8_41-OcXn"
      },
      "outputs": [],
      "source": [
        "# only valid for continuous actions\n",
        "# sac_student = SAC('MlpPolicy', env_id, verbose=1, policy_kwargs=dict(net_arch=[64, 64]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3GuNxcU8aoT"
      },
      "source": [
        "\n",
        "We now let our expert interact with the environment (except we already have expert data) and store resultant expert observations and actions to build an expert dataset.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "emodyZDW8aoU"
      },
      "outputs": [],
      "source": [
        "num_interactions = int(4e4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F3I_2s808aoZ",
        "outputId": "6cd4a67c-f9bf-46b3-c15b-05116e9ee0e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 40000/40000 [00:18<00:00, 2115.10it/s]\n"
          ]
        }
      ],
      "source": [
        "if isinstance(env.action_space, gym.spaces.Box):\n",
        "    expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "    expert_actions = np.empty((num_interactions,) + (env.action_space.shape[0],))\n",
        "\n",
        "else:\n",
        "    expert_observations = np.empty((num_interactions,) + env.observation_space.shape)\n",
        "    expert_actions = np.empty((num_interactions,) + env.action_space.shape)\n",
        "\n",
        "obs, _ = env.reset()\n",
        "\n",
        "for i in tqdm(range(num_interactions)):\n",
        "    action, _ = ppo_expert.predict(obs, deterministic=True)\n",
        "    expert_observations[i] = obs\n",
        "    expert_actions[i] = action\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "    if done:\n",
        "        obs, _ = env.reset()\n",
        "\n",
        "np.savez_compressed(\n",
        "    \"expert_data\",\n",
        "    expert_actions=expert_actions,\n",
        "    expert_observations=expert_observations,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toKEQE9i8aof"
      },
      "source": [
        "\n",
        "\n",
        "- To seamlessly use PyTorch in the training process, we subclass an `ExpertDataset` from PyTorch's base `Dataset`.\n",
        "- Note that we initialize the dataset with the previously generated expert observations and actions.\n",
        "- We further implement Python's `__getitem__` and `__len__` magic functions to allow PyTorch's dataset-handling to access arbitrary rows in the dataset and inform it about the length of the dataset.\n",
        "- For more information about PyTorch's datasets, you can read: https://pytorch.org/docs/stable/data.html.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "qT72bR1i8aog"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data.dataset import Dataset, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "xUetr5vl8aom"
      },
      "outputs": [],
      "source": [
        "class ExpertDataSet(Dataset):\n",
        "    def __init__(self, expert_observations, expert_actions):\n",
        "        self.observations = expert_observations\n",
        "        self.actions = expert_actions\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return (self.observations[index], self.actions[index])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.observations)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9bNAhXp8aor"
      },
      "source": [
        "\n",
        "\n",
        "We now instantiate the `ExpertDataSet` and split it into training and test datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "lIdT-zMV8aot"
      },
      "outputs": [],
      "source": [
        "expert_dataset = ExpertDataSet(expert_observations, expert_actions)\n",
        "\n",
        "train_size = int(0.8 * len(expert_dataset))\n",
        "\n",
        "test_size = len(expert_dataset) - train_size\n",
        "\n",
        "train_expert_dataset, test_expert_dataset = random_split(\n",
        "    expert_dataset, [train_size, test_size]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LgmtFFq8aox",
        "outputId": "0487ce0b-159c-4863-c987-16f727eec90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test_expert_dataset:  8000\n",
            "train_expert_dataset:  32000\n"
          ]
        }
      ],
      "source": [
        "print(\"test_expert_dataset: \", len(test_expert_dataset))\n",
        "print(\"train_expert_dataset: \", len(train_expert_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v8PhG2r8ao4"
      },
      "source": [
        "\n",
        "\n",
        "NOTE: The supervised learning section of this code is adapted from: https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
        "1. We extract the policy network of our RL student agent.\n",
        "2. We load the (labeled) expert dataset containing expert observations as inputs and expert actions as targets.\n",
        "3. We perform supervised learning, that is, we adjust the policy network's parameters such that given expert observations as inputs to the network, its outputs match the targets (expert actions).\n",
        "By training the policy network in this way the corresponding RL student agent is taught to behave like the expert agent that was used to created the expert dataset (Behavior Cloning).\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a2c_student.policy"
      ],
      "metadata": {
        "id": "b4s29AAqx9PB",
        "outputId": "29ea109f-41b0-4f16-a903-3e6bef90b3ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DQNPolicy(\n",
              "  (q_net): QNetwork(\n",
              "    (features_extractor): FlattenExtractor(\n",
              "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (q_net): Sequential(\n",
              "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=64, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              "  (q_net_target): QNetwork(\n",
              "    (features_extractor): FlattenExtractor(\n",
              "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "    )\n",
              "    (q_net): Sequential(\n",
              "      (0): Linear(in_features=4, out_features=64, bias=True)\n",
              "      (1): ReLU()\n",
              "      (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "      (3): ReLU()\n",
              "      (4): Linear(in_features=64, out_features=2, bias=True)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nn.Sequential(a2c_student.policy)"
      ],
      "metadata": {
        "id": "WBc9qMUrxojI",
        "outputId": "465dbc30-fa45-4e20-edd6-2ce65e0fc593",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sequential(\n",
              "  (0): DQNPolicy(\n",
              "    (q_net): QNetwork(\n",
              "      (features_extractor): FlattenExtractor(\n",
              "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "      )\n",
              "      (q_net): Sequential(\n",
              "        (0): Linear(in_features=4, out_features=64, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (3): ReLU()\n",
              "        (4): Linear(in_features=64, out_features=2, bias=True)\n",
              "      )\n",
              "    )\n",
              "    (q_net_target): QNetwork(\n",
              "      (features_extractor): FlattenExtractor(\n",
              "        (flatten): Flatten(start_dim=1, end_dim=-1)\n",
              "      )\n",
              "      (q_net): Sequential(\n",
              "        (0): Linear(in_features=4, out_features=64, bias=True)\n",
              "        (1): ReLU()\n",
              "        (2): Linear(in_features=64, out_features=64, bias=True)\n",
              "        (3): ReLU()\n",
              "        (4): Linear(in_features=64, out_features=2, bias=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "input = torch.tensor([[-0.004942  , -0.03194539,  0.02045883, -0.02706348]])\n",
        "a2c_student.policy(input)\n",
        "\n",
        "a2c_student.policy.q_net(input)"
      ],
      "metadata": {
        "id": "NZINwAEbxkYA",
        "outputId": "0abce9cf-07a9-4d75-bd75-129fcd47362d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.0054,  0.1156]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "VwUhCTGU8ao5"
      },
      "outputs": [],
      "source": [
        "def pretrain_agent(\n",
        "    student,\n",
        "    batch_size=64,\n",
        "    epochs=1000,\n",
        "    scheduler_gamma=0.7,\n",
        "    learning_rate=1.0,\n",
        "    log_interval=100,\n",
        "    no_cuda=True,\n",
        "    seed=1,\n",
        "    test_batch_size=64,\n",
        "):\n",
        "    use_cuda = not no_cuda and th.cuda.is_available()\n",
        "    th.manual_seed(seed)\n",
        "    device = th.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "    kwargs = {\"num_workers\": 1, \"pin_memory\": True} if use_cuda else {}\n",
        "\n",
        "    if isinstance(env.action_space, gym.spaces.Box):\n",
        "        criterion = nn.MSELoss()\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Extract initial policy\n",
        "    model = student.policy.to(device)\n",
        "\n",
        "    def train(model, device, train_loader, optimizer):\n",
        "        model.train()\n",
        "\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if isinstance(env.action_space, gym.spaces.Box):\n",
        "                # A2C/PPO policy outputs actions, values, log_prob\n",
        "                # SAC/TD3 policy outputs actions only\n",
        "                if isinstance(student, (A2C, PPO)):\n",
        "                    action, _, _ = model(data)\n",
        "                else:\n",
        "                    # SAC/TD3:\n",
        "                    action = model(data)\n",
        "                action_prediction = action.double()\n",
        "            else:\n",
        "                if isinstance(student, (DQN)):\n",
        "                    action_prediction = model.q_net(data)\n",
        "                    # target = model.q_net_target(data)\n",
        "                    target = target.long()\n",
        "                else:\n",
        "                  # Retrieve the logits for A2C/PPO when using discrete actions\n",
        "                  dist = model.get_distribution(data)\n",
        "                  action_prediction = dist.distribution.logits\n",
        "                  target = target.long()\n",
        "\n",
        "            loss = criterion(action_prediction, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            if batch_idx % log_interval == 0:\n",
        "                print(\n",
        "                    \"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                        epoch,\n",
        "                        batch_idx * len(data),\n",
        "                        len(train_loader.dataset),\n",
        "                        100.0 * batch_idx / len(train_loader),\n",
        "                        loss.item(),\n",
        "                    )\n",
        "                )\n",
        "\n",
        "    def test(model, device, test_loader):\n",
        "        model.eval()\n",
        "        test_loss = 0\n",
        "        with th.no_grad():\n",
        "            for data, target in test_loader:\n",
        "                data, target = data.to(device), target.to(device)\n",
        "\n",
        "                if isinstance(env.action_space, gym.spaces.Box):\n",
        "                    # A2C/PPO policy outputs actions, values, log_prob\n",
        "                    # SAC/TD3 policy outputs actions only\n",
        "                    if isinstance(student, (A2C, PPO)):\n",
        "                        action, _, _ = model(data)\n",
        "                    else:\n",
        "                        # SAC/TD3:\n",
        "                        action = model(data)\n",
        "                    action_prediction = action.double()\n",
        "                else:\n",
        "                    if isinstance(student, (DQN)):\n",
        "                        action_prediction = model.q_net(data)\n",
        "                        target = model.q_net_target(data)\n",
        "                    else:\n",
        "                        # Retrieve the logits for A2C/PPO when using discrete actions\n",
        "                        dist = model.get_distribution(data)\n",
        "                        action_prediction = dist.distribution.logits\n",
        "                        target = target.long()\n",
        "\n",
        "                test_loss = criterion(action_prediction, target)\n",
        "        test_loss /= len(test_loader.dataset)\n",
        "        print(f\"Test set: Average loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Here, we use PyTorch `DataLoader` to our load previously created `ExpertDataset` for training\n",
        "    # and testing\n",
        "    train_loader = th.utils.data.DataLoader(\n",
        "        dataset=train_expert_dataset, batch_size=batch_size, shuffle=True, **kwargs\n",
        "    )\n",
        "    test_loader = th.utils.data.DataLoader(\n",
        "        dataset=test_expert_dataset,\n",
        "        batch_size=test_batch_size,\n",
        "        shuffle=True,\n",
        "        **kwargs,\n",
        "    )\n",
        "\n",
        "    # Define an Optimizer and a learning rate schedule.\n",
        "    optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
        "    scheduler = StepLR(optimizer, step_size=1, gamma=scheduler_gamma)\n",
        "\n",
        "    # Now we are finally ready to train the policy model.\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, optimizer)\n",
        "        test(model, device, test_loader)\n",
        "        scheduler.step()\n",
        "\n",
        "    # Implant the trained policy network back into the RL student agent\n",
        "    a2c_student.policy = model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkEP6i0hEu_R"
      },
      "source": [
        "Evaluate the agent before pretraining, it should be random"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7kvYIneEui8",
        "outputId": "f78aa9b6-de9b-4c3d-cb58-fb8e614f219d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward = 9.2 +/- 0.8717797887081348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SgduZAbF8ao9"
      },
      "source": [
        "\n",
        "\n",
        "Having defined the training procedure we can now run the pretraining!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eI1EFFnW8ao-",
        "outputId": "b77ef874-c104-4223-8451-a7743ae74a54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 1 [0/32000 (0%)]\tLoss: 0.690298\n",
            "Train Epoch: 1 [6400/32000 (20%)]\tLoss: 0.345963\n",
            "Train Epoch: 1 [12800/32000 (40%)]\tLoss: 0.255350\n",
            "Train Epoch: 1 [19200/32000 (60%)]\tLoss: 0.298256\n",
            "Train Epoch: 1 [25600/32000 (80%)]\tLoss: 0.279253\n",
            "Test set: Average loss: 0.0000\n",
            "Train Epoch: 2 [0/32000 (0%)]\tLoss: 0.204465\n",
            "Train Epoch: 2 [6400/32000 (20%)]\tLoss: 0.364819\n",
            "Train Epoch: 2 [12800/32000 (40%)]\tLoss: 0.172510\n",
            "Train Epoch: 2 [19200/32000 (60%)]\tLoss: 0.278065\n",
            "Train Epoch: 2 [25600/32000 (80%)]\tLoss: 0.256919\n",
            "Test set: Average loss: 0.0000\n",
            "Train Epoch: 3 [0/32000 (0%)]\tLoss: 0.283690\n",
            "Train Epoch: 3 [6400/32000 (20%)]\tLoss: 0.211797\n",
            "Train Epoch: 3 [12800/32000 (40%)]\tLoss: 0.374600\n",
            "Train Epoch: 3 [19200/32000 (60%)]\tLoss: 0.228372\n",
            "Train Epoch: 3 [25600/32000 (80%)]\tLoss: 0.195449\n",
            "Test set: Average loss: 0.0000\n"
          ]
        }
      ],
      "source": [
        "pretrain_agent(\n",
        "    a2c_student,\n",
        "    epochs=3,\n",
        "    scheduler_gamma=0.7,\n",
        "    learning_rate=1.0,\n",
        "    log_interval=100,\n",
        "    no_cuda=True,\n",
        "    seed=1,\n",
        "    batch_size=64,\n",
        "    test_batch_size=1000,\n",
        ")\n",
        "a2c_student.save(\"a2c_student\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RK3Q5Jm58apE"
      },
      "source": [
        "\n",
        "\n",
        "Finally, let us test how well our RL agent student learned to mimic the behavior of the expert\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKZ8O--m8apF",
        "outputId": "dc2e23a5-377b-4f13-9c85-e6ac0a22ad31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward = 500.0 +/- 0.0\n"
          ]
        }
      ],
      "source": [
        "mean_reward, std_reward = evaluate_policy(a2c_student, env, n_eval_episodes=10)\n",
        "\n",
        "print(f\"Mean reward = {mean_reward} +/- {std_reward}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BPruhxvXua_V"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pretraining.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "3201c96db5836b171d01fee72ea1be894646622d4b41771abf25c98b548a611d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}